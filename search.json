[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Every day millions gigabytes data shared across internet. standard way internet users download data manually clicking downloading files formats Excel files, CSV files, Word files. However, process downloading files clicking works well downloading one two files. needed download 500 CSV files? happens need download data refreshed every 20 seconds? process manually clicking file simply feasible solution downloading data scale frequent intervals. ’s increasing amount data internet, ’s also increase tools allow access data programmatically. course focus exploring technologies.Let give example. COVID pandemic struck world 2020, paramount understand mortality rates causing general decomposition mortality (affecting males versus females well different age groups). team researchers Max Planck Institute Demographic Research set collect administrative data reported deaths country, regions, gender age. Now might sound like fun ’s actually awfully difficult countries uploaded data refreshed every day others weekly basis. regions given country might update data different schedule. website might different process getting know website can take time. Scale process collect data hundreds countries process collecting mortality data can become incredibly cumbersome: might spend hours web simply clicking links download data (see sample countries dates collect data ). Aside long tedious task boringly clicking links hours, chances ’ll also introduce errors along way. might confuse one region another assign wrong name CSV file, might skip particular country mistake might simply misspell country’s name, messing order files saving.example typical task want automate: wouldn’t great type robot automatically collect data different websites save files correct, explicity names ? ’s exactly team . created COVerAGE-DB dataset. help dozens collaborators, created web scraping scripts automatically collected data automated process run frequently needed. ’s managed created hundreds small robots work brought back data analyze. perfect motivation automated data acquisition extremely important tool every person works data benefit lot knowing tools.Throughout course, ’ll describe create upon basic formats data transfer JSON, HTML XML. learn subset, manipulate transform formats suitable data analysis tasks. data formats can accessed scraping website: programmatically accessing data website asking program download much data need frequently needed. integral part course focus perform ethical web scrapping, scale web scrapping download massive amounts data well program scraper access data frequent intervals.course also focus emerging technology websites share data: Application Programming Interfaces (API). touch upon basic formats data sharing APIs well make thousands data requests just seconds. Special emphasis made security ethical guidelines speaking APIs.big part course emphasize automation, way student create robust scalable data acquisition pipelines. step process, focus practical applications techniques exercise active participation students hands-challenges data acquisition. course make heavy use Github students need share homework well explore immense repository data acquisition technology already available open source software.goal course empower students right toolset ideas able create data acquisition programs, automate data extraction pipeline quickly transform data formats suitable analysis. course place contents light legal ethical consequences data acquisition can entail, always informing students best practices grabbing data internet.course assumes students familiar R programming language, transforming manipulating datasets well saving work Git Github. prior knowledge software development data acquisition techniques needed.","code":""},{"path":"index.html","id":"curriculum","chapter":"1 Introduction","heading":"1.1 Curriculum","text":"introduction Web Scraping\nWeb Scraping?\nTypes Web Scraping\nData formats: XML HTML\nPractical access XML HTML\nAutomation Web Scraping programs\nSelenium JavaScript based scraping\nEthical issues Web Scraping\nPractical exercises\nWeb Scraping?Types Web ScrapingData formats: XML HTMLPractical access XML HTMLAutomation Web Scraping programsSelenium JavaScript based scrapingEthical issues Web ScrapingPractical exercisesData APIs\nAPI\nFundamentals API communication\nintroduction JSON format\nCreate API (share )\nREST architecture\nAPIs way share obtain data (kind)\nAutomation API requests\nTalking Databases\nAuthentication ethical access APIs\nPractical exercises\nAPIFundamentals API communicationAn introduction JSON formatCreate API (share )REST architectureAPIs way share obtain data (kind)Automation API requestsTalking DatabasesAuthentication ethical access APIsPractical exercisesAutomation Data Acquisition\nneed automation?\nAccessing servers\nTechnologies automating programs\nAutomating cron jobs\nLogging tasks\nPractical exercises\nneed automation?Accessing serversTechnologies automating programsAutomating cron jobsLogging tasksPractical exercises","code":""},{"path":"introduction-to-webscraping.html","id":"introduction-to-webscraping","chapter":"2 Introduction to Webscraping","heading":"2 Introduction to Webscraping","text":"TODO","code":""},{"path":"a-primer-on-webscraping.html","id":"a-primer-on-webscraping","chapter":"3 A primer on Webscraping","heading":"3 A primer on Webscraping","text":"TODOHere quick example using rvest scrape Wikipedia tables. need add Wikipedia page scrapex.quick example using rvest scrape IMDB (? better example ?). need add Wikipedia page scrapex.","code":"\nlibrary(rvest)\nlibrary(dplyr)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nlink <- \"https://en.wikipedia.org/wiki/Elections_in_Spain\"\n\nhtml_website <-\n  link %>%\n  read_html()\n\nhtml_website## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nall_tables <-\n  html_website %>%\n  html_table()\n\nelections_data <- all_tables[[5]]\n\nelections_data %>%\n  select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]`    `IU[c]` `EHB[g]`    `CDS[h]`    UPyD        Cs    Com. \n##    <chr>     <chr>       <chr>   <chr>       <chr>       <chr>       <chr> <chr>\n##  1 Election  \"\"          \"\"      \"\"          \"\"          \"\"          \"\"    \"\"   \n##  2 1977      \"34.4\"      \"9.3\"   \"0.2\"       \"\"          \"\"          \"\"    \"\"   \n##  3 1979      \"34.8\"      \"10.8\"  \"1.0\"       \"\"          \"\"          \"\"    \"\"   \n##  4 1982      \"6.8\"       \"4.0\"   \"1.0\"       \"2.9\"       \"\"          \"\"    \"\"   \n##  5 1986      \"Dissolved\" \"4.6\"   \"1.1\"       \"9.2\"       \"\"          \"\"    \"\"   \n##  6 1989      \"Dissolved\" \"9.1\"   \"1.1\"       \"7.9\"       \"\"          \"\"    \"\"   \n##  7 1993      \"Dissolved\" \"9.6\"   \"0.9\"       \"1.8\"       \"\"          \"\"    \"\"   \n##  8 1996      \"Dissolved\" \"10.5\"  \"0.7\"       \"0.2\"       \"\"          \"\"    \"\"   \n##  9 2000      \"Dissolved\" \"5.4\"   \"Boycotted\" \"0.1\"       \"\"          \"\"    \"\"   \n## 10 2004      \"Dissolved\" \"5.0\"   \"Banned\"    \"0.1\"       \"\"          \"\"    \"\"   \n## 11 2008      \"Dissolved\" \"3.8\"   \"Banned\"    \"0.0\"       \"1.2\"       \"0.2\" \"\"   \n## 12 2011      \"Dissolved\" \"6.9\"   \"1.4\"       \"Dissolved\" \"4.7\"       \"Did… \"0.5\"\n## 13 2015      \"Dissolved\" \"3.7\"   \"0.9\"       \"Dissolved\" \"0.6\"       \"13.… \"[k]\"\n## 14 2016      \"Dissolved\" \"[k]\"   \"0.8\"       \"Dissolved\" \"0.2\"       \"13.… \"[k]\"\n## 15 Apr. 2019 \"Dissolved\" \"[l]\"   \"1.0\"       \"Dissolved\" \"Did not r… \"15.… \"0.7\"\n## 16 Nov. 2019 \"Dissolved\" \"[l]\"   \"1.2\"       \"Dissolved\" \"[m]\"       \"6.8\" \"[n]\"\nwrong_labels <- c(\n  \"Dissolved\",\n  \"[k]\",\n  \"[l]\",\n  \"[m]\",\n  \"n\",\n  \"Banned\",\n  \"Boycotted\",\n  \"Did not run\"\n)\n\nwrong_labels <- paste0(wrong_labels, collapse = \"|\")\n\nsemi_cleaned_data <-\n  elections_data %>%\n  mutate_if(\n    is.character,\n    .funs = str_replace_all,\n    pattern = wrong_labels,\n    replacement = NA_character_\n  ) %>%\n  mutate(Election = str_replace_all(Election, \"Apr. |Nov. \", \"\")) %>%\n  mutate_all(as.numeric) %>%\n  filter(!is.na(Election)) %>%\n  rename_all(~ str_replace_all(.x, \"\\\\[.+\\\\]\", \"\"))\n\ncleaned_data <-\n  semi_cleaned_data %>%\n  pivot_longer(-Election, names_to = \"parties\")\n\ncleaned_data## # A tibble: 255 × 3\n##    Election parties value\n##       <dbl> <chr>   <dbl>\n##  1     1977 UCD      34.4\n##  2     1977 PSOE     29.3\n##  3     1977 PP        8.3\n##  4     1977 IU        9.3\n##  5     1977 CDC       2.8\n##  6     1977 PNV       1.7\n##  7     1977 ERC       0.8\n##  8     1977 BNG       0.1\n##  9     1977 EHB       0.2\n## 10     1977 CDS      NA  \n## # … with 245 more rows\ntail(cleaned_data)## # A tibble: 6 × 3\n##   Election parties value\n##      <dbl> <chr>   <dbl>\n## 1     2019 UPyD     NA  \n## 2     2019 Cs        6.8\n## 3     2019 Com.     NA  \n## 4     2019 Pod.     12.8\n## 5     2019 Vox      15.1\n## 6     2019 MP        2.4\ncleaned_data %>%\n  ggplot(aes(Election, value, color = parties)) +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  scale_color_viridis_d() +\n  theme_minimal()## Warning: Removed 92 row(s) containing missing values (geom_path).\nhtml_website %>%\n  html_nodes(xpath = \"//table[@style='text-align:center; font-size:90%;']\") %>%\n  html_nodes(xpath = \"//th\")## {xml_nodeset (105)}\n##  [1] <th class=\"sidebar-title\"><div class=\"adr\"><a href=\"/wiki/Politics_of_Sp ...\n##  [2] <th><a href=\"/wiki/File:Escudo_de_Espa%C3%B1a_(mazonado).svg\" class=\"ima ...\n##  [3] <th style=\"border-bottom: #aaa 1px solid\"><\/th>\n##  [4] <th>Division<\/th>\n##  [5] <th>PSOE<\/th>\n##  [6] <th>PP<\/th>\n##  [7] <th>PAR<\/th>\n##  [8] <th>IU\\n<\/th>\n##  [9] <th>Seats<\/th>\n## [10] <th>3<\/th>\n## [11] <th>2<\/th>\n## [12] <th>1<\/th>\n## [13] <th>1\\n<\/th>\n## [14] <th>Division<\/th>\n## [15] <th>PSOE<\/th>\n## [16] <th>PP<\/th>\n## [17] <th>PAR<\/th>\n## [18] <th>IU\\n<\/th>\n## [19] <th>Seats<\/th>\n## [20] <th>2<\/th>\n## ...\nall_style_attr <-\n  html_website %>%\n  html_nodes(xpath = \"//table[@style='text-align:center; font-size:90%;']\") %>%\n  html_nodes(xpath = \"//tbody//th[@style]\") %>%\n  html_attr(\"style\")\n\nwhere_background <- all_style_attr %>% str_detect(\"background\")\n\nparty_colors <- all_style_attr[where_background] %>% str_replace_all(\"background:\", \"\")\n\nparties <- names(semi_cleaned_data)[-1]\ncolors_lookup <- tibble(parties = parties, party_colors = party_colors)\n\ncleaned_colors_data <-\n  cleaned_data %>%\n  left_join(colors_lookup)## Joining, by = \"parties\"\ntmp_colors <- cleaned_colors_data %>% distinct(parties, party_colors)\nvector_colors <- tmp_colors$party_colors\nnames(vector_colors) <- tmp_colors$parties\n\ncleaned_colors_data %>%\n  ggplot(aes(Election, value, color = parties)) +\n  geom_line() +\n  scale_y_continuous(name = \"% of votes\", labels = function(x) paste0(x, \"%\")) +\n  xlab(\"Election years\") +\n  scale_colour_manual(values = vector_colors) +\n  ggtitle(label = \"Parties and share of votes in all Spanish Elections\") +\n  theme_minimal()## Warning: Removed 92 row(s) containing missing values (geom_path)."},{"path":"data-formats-for-webscraping.html","id":"data-formats-for-webscraping","chapter":"4 Data Formats for Webscraping","heading":"4 Data Formats for Webscraping","text":"’ll want copy XML / HTML examples Spanish school case study elaborate . Inspiration: https://rvest.tidyverse.org/articles/harvesting--web.htmlYou want two things: explain clearly possible HTML done school example dive elaborate examples extract . Think example web scrapex simple one. Repeat XML. Difference XML HTML: https://www.guru99.com/xml-vs-html-difference.html","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"case-study-scraping-spanish-school-locations-from-the-web","chapter":"5 Case study: scraping Spanish school locations from the web","heading":"5 Case study: scraping Spanish school locations from the web","text":"case study ’ll working basics web scraping using R xml2 package. ’ll begin simple example using fake data elaborate trying scrape location sample schools Spain.","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"basic-steps","chapter":"5 Case study: scraping Spanish school locations from the web","heading":"5.1 Basic steps","text":"web scraping R, can fulfill almost needs xml2 package. wander web, ’ll see many examples using rvest package. xml2 rvest similar don’t feel ’re lacking behind learning one . addition two packages, ’ll need libraries plotting locations map (ggplot2, sf, rnaturalearth), identifying scrape (httr) wrangling data (tidyverse).Additionally, ’ll also need package scrapex. real-world example ’ll , ’ll scraping data website www.buscocolegio.com locate sample schools Spain. However, throughout tutorial won’t scraping data directly real-website. happen tutorial 6 months now www.buscocolegio.com updates design website? Everything real-world example lost.Web scraping tutorials usually unstable precisely . circumvent problem, ’ve saved random sample websites schools www.buscocolegio.com R package called scrapex. Although links ’ll working hosted locally machine, HTML website similar one hosted website (exception images/icons deleted purpose make package lightweight).can install package :Now, let’s move fake data example load packages :Let’s begin simple example. define XML string look structure:XML HTML basic building blocks something called tags. example, first tag structure shown <people>. tag matched <\/people> end string:pay close attention, ’ll see tag XML structure beginning (signaled <>) end (signaled <\/>). example, next tag <people> <jason> right tag <carol> end jason tag <\/jason>.Similarly, ’ll find <carol> tag also matched <\/carol> finishing tag.theory, tags can whatever meaning attach (<people> <occupation>). However, practice hundreds tags standard websites (example, ). ’re just getting started, ’s need learn progress web scraping, ’ll start recognize (one brief example <strong> simply bolds text website).xml2 package designed read XML strings navigate tree structure extract information. example, let’s read XML data fake example look general structure:can see structure tree-based, meaning tags <jason> <carol> nested within <people> tag. XML jargon, <people> root node, whereas <jason> <carol> child nodes <people>.detail, structure follows:root node <people>child nodes <jason> <carol>child node nodes <first_name>, <married>, <last_name> <occupation> nested within .Put another way, something nested within node, nested node child upper-level node. example, root node <people> can check children:Tags can also different attributes usually specified <fake_tag attribute='fake'> ended usual <\/fake_tag>. look XML structure example, ’ll notice <person> tag attribute called type. ’ll see real-world example, extracting attributes often aim scraping adventure. Using xml2, can extract attributes match specific name xml_attrs.Wait, didn’t work? Well, look output child_xml, two nodes <jason> <carol>.tags attribute? , , something like <jason type='fake_tag'>. need look <person> tag within <jason> <carol> extract attribute <person>.sound familiar? <jason> <carol> associated <person> tag , making children. can just go one level running xml_children tags extract .Using xml_path function can even find ‘address’ nodes retrieve specific tags without write xml_children many times. example:‘address’ specific tags tree extract automatically? extract specific ‘addresses’ XML tree, main function ’ll use xml_find_all. function accepts XML tree ‘address’ string. can use simple strings, one given xml_path:expression asking node \"/people/jason/person\". return saying xml_raw %>% xml_child(search = 1). deeply nested trees, xml_find_all many times much cleaner calling xml_child recursively many times.However, cases ‘addresses’ used xml_find_all come separate language called XPath (fact, ‘address’ ’ve looking XPath). XPath complex language (regular expressions strings) beyond brief tutorial. However, examples ’ve seen far, can use basic XPath ’ll need later .extract tags document, can use //name_of_tag.previous XPath, ’re searching married tags within complete XML tree. result returns married nodes (use words tags nodes interchangeably) complete tree structure. Another example finding <occupation> tags:want find tag can replace \"//occupation\" tag interest xml_find_all find .wanted find tags current node, need add . beginning: \".//occupation\". example, dived <jason> tag wanted <occupation> tag, \"//occupation\" returns <occupation> tags. Instead, \".//occupation\" return found tags current tag. example:first example returns <jason>’s occupation whereas second returned occupations, regardless tree.XPath also allows identify tags contain one specific attribute, one’s saw earlier. example, filter <person> tags attribute filter set fictional, :wanted tags current nodes, trick learned earlier work: \".//person[@type='fictional']\". just primers can help jump easily using XPath, encourage look examples web, complex websites often require complex XPath expressions.begin real-word example, might asking can actually extract text/numeric data nodes. Well, ’s easy: xml_text.’ve narrowed tree-based search one single piece text numbers, xml_text() extract (’s also xml_double xml_integer extracting numbers). said, XPath really huge language. ’re interested, XPath cheat sheets helped lot learn tricks easy scraping.","code":"\n# install.packages(\"devtools\")\ndevtools::install_github(\"cimentadaj/scrapex\")\nlibrary(xml2)\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)\nlibrary(scrapex)\nxml_test <- \"<people>\n<jason>\n  <person type='fictional'>\n    <first_name>\n      <married>\n        Jason\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Bourne\n    <\/last_name>\n    <occupation>\n      Spy\n    <\/occupation>\n  <\/person>\n<\/jason>\n<carol>\n  <person type='real'>\n    <first_name>\n      <married>\n        Carol\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Kalp\n    <\/last_name>\n    <occupation>\n      Scientist\n    <\/occupation>\n  <\/person>\n<\/carol>\n<\/people>\n\"\n\ncat(xml_test)## <people>\n## <jason>\n##   <person type='fictional'>\n##     <first_name>\n##       <married>\n##         Jason\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Bourne\n##     <\/last_name>\n##     <occupation>\n##       Spy\n##     <\/occupation>\n##   <\/person>\n## <\/jason>\n## <carol>\n##   <person type='real'>\n##     <first_name>\n##       <married>\n##         Carol\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Kalp\n##     <\/last_name>\n##     <occupation>\n##       Scientist\n##     <\/occupation>\n##   <\/person>\n## <\/carol>\n## <\/people>\nxml_raw <- read_xml(xml_test)\nxml_structure(xml_raw)## <people>\n##   <jason>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n##   <carol>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n# xml_child returns only one child (specified in search)\n# Here, jason is the first child\nxml_child(xml_raw, search = 1)## {xml_node}\n## <jason>\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n# Here, carol is the second child\nxml_child(xml_raw, search = 2)## {xml_node}\n## <carol>\n## [1] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Use xml_children to extract **all** children\nchild_xml <- xml_children(xml_raw)\n\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ...\n# Extract the attribute type from all nodes\nxml_attrs(child_xml, \"type\")## [[1]]\n## named character(0)\n## \n## [[2]]\n## named character(0)\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ...\n# We go down one level of children\nperson_nodes <- xml_children(child_xml)\n\n# <person> is now the main node, so we can extract attributes\nperson_nodes## {xml_nodeset (2)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n## [2] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Both type attributes\nxml_attrs(person_nodes, \"type\")## [[1]]\n##        type \n## \"fictional\" \n## \n## [[2]]\n##   type \n## \"real\"\n# Specific address of each person tag for the whole xml tree\n# only using the `person_nodes`\nxml_path(person_nodes)## [1] \"/people/jason/person\" \"/people/carol/person\"\n# You can use results from xml_path like directories\nxml_find_all(xml_raw, \"/people/jason/person\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n# Search for all 'married' nodes\nxml_find_all(xml_raw, \"//married\")## {xml_nodeset (2)}\n## [1] <married>\\n        Jason\\n      <\/married>\n## [2] <married>\\n        Carol\\n      <\/married>\nxml_find_all(xml_raw, \"//occupation\")## {xml_nodeset (2)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n## [2] <occupation>\\n      Scientist\\n    <\/occupation>\nxml_raw %>%\n  # Dive only into Jason's tag\n  xml_child(search = 1) %>%\n  xml_find_all(\".//occupation\")## {xml_nodeset (1)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n# Instead, the wrong way would have been:\nxml_raw %>%\n  # Dive only into Jason's tag\n  xml_child(search = 1) %>%\n  # Here we get both occupation tags\n  xml_find_all(\"//occupation\")## {xml_nodeset (2)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n## [2] <occupation>\\n      Scientist\\n    <\/occupation>\n# Give me all the tags 'person' that have an attribute type='fictional'\nxml_raw %>%\n  xml_find_all(\"//person[@type='fictional']\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\nxml_raw %>%\n  xml_find_all(\".//occupation\") %>%\n  xml_text()## [1] \"\\n      Spy\\n    \"       \"\\n      Scientist\\n    \""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"real-world-example","chapter":"5 Case study: scraping Spanish school locations from the web","heading":"5.2 Real-world example","text":"’re interested making list many schools Spain visualizing location. can useful many things matching population density children across different regions school locations. website www.buscocolegio.com contains database schools similar ’re looking . described beginning, instead ’re going use scrapex function spanish_schools_ex() containing links sample websites different schools saved locally computer.Let’s look example one school.’re interested looking website interactively browser, can browseURL(prep_browser(school_url)). Let’s read HTML (XML HTML usually interchangeable, use read_html).Web scraping strategies specific website ’re . get familiar website ’re interested able match perfectly information ’re looking . many cases, scraping two websites require vastly different strategies. particular example, ’re interested figuring location school extract location.image ’ll find typical school’s website wwww.buscocolegio.com. website lot information, ’re interested button circled orange rectangle. can’t find easily, ’s Google Maps right says “Buscar colegio cercano”.click button, actually points towards coordinates school just find way figuring click button figure get information. browsers allow press CTRL + SHIFT + c time (Firefox Chrome support hotkey). window right popped full code, ’re right track:can search source code website. place mouse pointer lines code right-window, ’ll see sections website highlighted blue. indicates parts code refer parts website. Luckily us, don’t search complete source code find specific location. can approximate search typing text ’re looking search bar top right window:click enter, ’ll automatically directed tag information want.specifically, can see latitude longitude schools found attributed called href tag <>:Can see latitude longitude fields text highlighted blue? ’s hidden -words. precisely type information ’re . Extracting <> tags website (hint: XPath similar \"//\") yield hundreds matches <> common tag. Moreover, refining search <> tags href attribute also yield hundreds matches href standard attribute attach links within websites. need narrow search within website.One strategy find ‘father’ ‘grandfather’ node particular <> tag match node sequence grandfather -> father -> child node. looking structure small XML snippet right-window, see ‘grandfather’ <> tag <p class=\"d-flex align-items-baseline g-mt-5'> particularly long attribute named class.Don’t intimidated tag names long attributes. also don’t know attributes mean. know ‘grandfather’ <> tag ’m interested . using XPath skills, let’s search <p> tag see get one match.one match, good news. means can uniquely identify particular <p> tag. Let’s refine search say: Find <> tags children specific <p> tag. means ’ll add \"//\" previous expression. Since one <p> tag class, ’re interested checking whether one <> tag <p> tag.go! can see specific href contains latitude longitude data ’re interested . extract href attribute? Using xml_attr !Ok, now need regex skills get latitude longitude (regex expressions used search patterns inside string, example date. See examples):Ok, got information needed one single school. Let’s turn function can pass school’s link get coordinates back., set something called User-Agent. short, User-Agent . good practice identify person scraping website ’re causing trouble website, website can directly identify causing problems. can figure user agent paste string . addition, add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests.Ok, ’s working. thing left extract many schools. shown earlier, scrapex contains list 27 school links can automatically scrape. Let’s loop , get information coordinates collapse data frame.now locations schools, let’s plot :go! went literally information beginning tutorial interpretable summarized information using web data. can see schools Madrid (center) well regions Spain, including Catalonia Galicia.marks end scraping adventure finish, want mention ethical guidelines web scraping. Scraping extremely useful us can give headaches people maintaining website interest. ’s list ethical guidelines always follow:Read terms services: many websites prohibit web scraping breach privacy scraping data. One famous example.Read terms services: many websites prohibit web scraping breach privacy scraping data. One famous example.Check robots.txt file. file websites (www.buscocolegio.com ) tell specific paths inside website scrapable . See explanation robots.txt look like find .Check robots.txt file. file websites (www.buscocolegio.com ) tell specific paths inside website scrapable . See explanation robots.txt look like find .websites supported big servers, means can send 4-5 website requests per second. Others, www.buscocolegio.com . ’s good practice always put time sleep requests. example, set 5 seconds small website don’t want crash servers.websites supported big servers, means can send 4-5 website requests per second. Others, www.buscocolegio.com . ’s good practice always put time sleep requests. example, set 5 seconds small website don’t want crash servers.making requests, computational ways identifying . example, every request (one’s ) can something called User-Agent. good practice include User-Agent (code) admin server can directly identify someone’s causing problems due web scraping.making requests, computational ways identifying . example, every request (one’s ) can something called User-Agent. good practice include User-Agent (code) admin server can directly identify someone’s causing problems due web scraping.Limit scraping non-busy hours overnight. can help reduce chances collapsing website since fewer people visiting websites evening.Limit scraping non-busy hours overnight. can help reduce chances collapsing website since fewer people visiting websites evening.can read ethical issues .","code":"\nschool_links <- spanish_schools_ex()\n\n# Keep only the HTML file of one particular school.\nschool_url <- school_links[13]\n\nschool_url## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/5f52b0cca7211cbdd54fe40a58a02568/scrapex/extdata/spanish_schools_ex/school_3006839.html\"\n# Here we use `read_html` because `read_xml` is throwing an error\n# when attempting to read. However, everything we've discussed\n# should be the same.\nschool_raw <- read_html(school_url) %>% xml_child()\n\nschool_raw## {html_node}\n## <head>\n##  [1] <title>Aquí encontrarás toda la información necesaria sobre CEIP SANCHIS ...\n##  [2] <meta charset=\"utf-8\">\\n\n##  [3] <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shri ...\n##  [4] <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n\n##  [5] <meta name=\"author\" content=\"BuscoColegio\">\\n\n##  [6] <meta name=\"description\" content=\"Encuentra toda la información necesari ...\n##  [7] <meta name=\"keywords\" content=\"opiniones SANCHIS GUARNER, contacto SANCH ...\n##  [8] <link rel=\"shortcut icon\" href=\"/favicon.ico\">\\n\n##  [9] <link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Roboto+Sl ...\n## [10] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [11] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-awesome/css/font-awesom ...\n## [12] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line/css/simple-line-ic ...\n## [13] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line-pro/style.css\">\\n\n## [14] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-hs/style.css\">\\n\n## [15] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [16] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [17] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [18] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [19] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [20] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## ...\n# Search for all <p> tags with that class in the document\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']\")## {xml_nodeset (1)}\n## [1] <p class=\"d-flex align-items-baseline g-mt-5\">\\r\\n\\t                    < ...\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\")## {xml_nodeset (1)}\n## [1] <a href=\"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274 ...\nlocation_str <-\n  school_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n  xml_attr(attr = \"href\")\n\nlocation_str## [1] \"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274492&colegio.longitud=0.0221681\"\nlocation <-\n  location_str %>%\n  str_extract_all(\"=.+$\") %>%\n  str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n  str_split(\"&\") %>%\n  .[[1]]\n\nlocation## [1] \"38.8274492\" \"0.0221681\"\n# This sets your `User-Agent` globally so that all requests are\n# identified with this `User-Agent`\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:70.0) Gecko/20100101 Firefox/70.0\")\n)\n\n# Collapse all of the code from above into one function called\n# school grabber\n\nschool_grabber <- function(school_url) {\n  # We add a time sleep of 5 seconds to avoid\n  # sending too many quick requests to the website\n  Sys.sleep(5)\n\n  school_raw <- read_html(school_url) %>% xml_child()\n\n  location_str <-\n    school_raw %>%\n    xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n    xml_attr(attr = \"href\")\n\n  location <-\n    location_str %>%\n    str_extract_all(\"=.+$\") %>%\n    str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n    str_split(\"&\") %>%\n    .[[1]]\n\n  # Turn into a data frame\n  data.frame(\n    latitude = location[1],\n    longitude = location[2],\n    stringsAsFactors = FALSE\n  )\n}\n\n\nschool_grabber(school_url)##     latitude longitude\n## 1 38.8274492 0.0221681\nres <- map_dfr(school_links, school_grabber)\nres##    latitude  longitude\n## 1  42.72779 -8.6567935\n## 2  43.24439 -8.8921645\n## 3  38.95592 -1.2255769\n## 4  39.18657 -1.6225903\n## 5  40.38245 -3.6410388\n## 6  40.22929 -3.1106322\n## 7  40.43860 -3.6970366\n## 8  40.33514 -3.5155669\n## 9  40.50546 -3.3738441\n## 10 40.63826 -3.4537107\n## 11 40.38543 -3.6639500\n## 12 37.76485 -1.5030467\n## 13 38.82745  0.0221681\n## 14 40.99434 -5.6224391\n## 15 40.99434 -5.6224391\n## 16 40.56037 -5.6703725\n## 17 40.99434 -5.6224391\n## 18 40.99434 -5.6224391\n## 19 41.13593  0.9901905\n## 20 41.26155  1.1670507\n## 21 41.22851  0.5461471\n## 22 41.14580  0.8199749\n## 23 41.18341  0.5680564\n## 24 42.07820  1.8203155\n## 25 42.25245  1.8621546\n## 26 41.73767  1.8383666\n## 27 41.62345  2.0013628\nres <- mutate_all(res, as.numeric)\n\nsp_sf <-\n  ne_countries(scale = \"large\", country = \"Spain\", returnclass = \"sf\") %>%\n  st_transform(crs = 4326)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = res, aes(x = longitude, y = latitude)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"wrap-up","chapter":"5 Case study: scraping Spanish school locations from the web","heading":"5.3 Wrap up","text":"tutorial introduced basic concepts web scraping applied real-world setting. Web scraping vast field computer science (can find entire books subject ). covered basic techniques think can take long way ’s definitely learn. curious turn, ’m looking forward upcoming book “Field Guide Web Scraping Accessing APIs R” Bob Rudis, released near future. Now go scrape websites ethically!","code":""},{"path":"footnotes-and-citations.html","id":"footnotes-and-citations","chapter":"6 Footnotes and citations","heading":"6 Footnotes and citations","text":"","code":""},{"path":"footnotes-and-citations.html","id":"footnotes","chapter":"6 Footnotes and citations","heading":"6.1 Footnotes","text":"Footnotes put inside square brackets caret ^[]. Like one.1","code":""},{"path":"footnotes-and-citations.html","id":"citations","chapter":"6 Footnotes and citations","heading":"6.2 Citations","text":"Reference items bibliography file(s) using @key.example, using bookdown package (Xie 2022) (check last code chunk index.Rmd see citation key added) sample book, built top R Markdown knitr (Xie 2015) (citation added manually external file book.bib).\nNote .bib files need listed index.Rmd YAML bibliography key.RStudio Visual Markdown Editor can also make easier insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations","code":""},{"path":"sharing-your-book.html","id":"sharing-your-book","chapter":"7 Sharing your book","heading":"7 Sharing your book","text":"","code":""},{"path":"sharing-your-book.html","id":"publishing","chapter":"7 Sharing your book","heading":"7.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"sharing-your-book.html","id":"pages","chapter":"7 Sharing your book","heading":"7.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"sharing-your-book.html","id":"metadata-for-sharing","chapter":"7 Sharing your book","heading":"7.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.gitbook uses social sharing data across chapters book- links shared look .Specify book’s source repository GitHub using edit key configuration options _output.yml file, allows users suggest edit linking chapter’s source file.Read features output format :https://pkgs.rstudio.com/bookdown/reference/gitbook.htmlOr use:","code":"\n?bookdown::gitbook"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"cross.html","id":"cross","chapter":"8 Cross-references","heading":"8 Cross-references","text":"Cross-references make easier readers find link elements book.","code":""},{"path":"cross.html","id":"chapters-and-sub-chapters","chapter":"8 Cross-references","heading":"8.1 Chapters and sub-chapters","text":"two steps cross-reference heading:Label heading: # Hello world {#nice-label}.\nLeave label like automated heading generated based heading title: example, # Hello world = # Hello world {#hello-world}.\nlabel un-numbered heading, use: # Hello world {-#nice-label} {# Hello world .unnumbered}.\nLeave label like automated heading generated based heading title: example, # Hello world = # Hello world {#hello-world}.label un-numbered heading, use: # Hello world {-#nice-label} {# Hello world .unnumbered}.Next, reference labeled heading anywhere text using \\@ref(nice-label); example, please see Chapter 8.\nprefer text link instead numbered reference use: text want can go .\nprefer text link instead numbered reference use: text want can go .","code":""},{"path":"cross.html","id":"captioned-figures-and-tables","chapter":"8 Cross-references","heading":"8.2 Captioned figures and tables","text":"Figures tables captions can also cross-referenced elsewhere book using \\@ref(fig:chunk-label) \\@ref(tab:chunk-label), respectively.See Figure 8.1.\nFigure 8.1: nice figure!\nDon’t miss Table 8.1.Table 8.1: nice table!","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(pressure, 10), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"parts.html","id":"parts","chapter":"9 Parts","heading":"9 Parts","text":"can add parts organize one book chapters together. Parts can inserted top .Rmd file, first-level chapter heading file.Add numbered part: # (PART) Act one {-} (followed # chapter)Add unnumbered part: # (PART\\*) Act one {-} (followed # chapter)Add appendix special kind un-numbered part: # (APPENDIX) stuff {-} (followed # chapter). Chapters appendix prepended letters instead numbers.Xie, Yihui. 2015. Dynamic Documents R Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/.———. 2022. Bookdown: Authoring Books Technical Documents R Markdown. https://CRAN.R-project.org/package=bookdown.","code":""}]
