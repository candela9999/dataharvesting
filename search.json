[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Every day millions gigabytes data shared across internet. standard way internet users download data manually clicking downloading files formats Excel files, CSV files, Word files. However, process downloading files clicking works well downloading one two files. needed download 500 CSV files? happens need download data refreshed every 20 seconds? process manually clicking file simply feasible solution downloading data scale frequent intervals. ’s increasing amount data internet, ’s also increase tools allow access data programmatically. course focus exploring technologies.Let give example. COVID pandemic struck world 2020, paramount understand mortality rates causing general decomposition mortality (affecting males versus females well different age groups). team researchers Max Planck Institute Demographic Research set collect administrative data reported deaths country, regions, gender age. Now might sound like fun ’s actually awfully difficult countries uploaded data refreshed every day others weekly basis. regions given country might update data different schedule. website might different process getting know website can take time. Scale process collect data hundreds countries process collecting mortality data can become incredibly cumbersome: might spend hours web simply clicking links download data (see sample countries dates collect data ). Aside long tedious task boringly clicking links hours, chances ’ll also introduce errors along way. might confuse one region another assign wrong name CSV file, might skip particular country mistake might simply misspell country’s name, messing order files saving.example typical task want automate: wouldn’t great type robot automatically collect data different websites save files correct, explicity names ? ’s exactly team . created COVerAGE-DB dataset. help dozens collaborators, created web scraping scripts automatically collected data automated process run frequently needed. ’s managed created hundreds small robots work brought back data analyze. perfect motivation automated data acquisition extremely important tool every person works data benefit lot knowing tools.Throughout course, ’ll describe create upon basic formats data transfer JSON, HTML XML. learn subset, manipulate transform formats suitable data analysis tasks. data formats can accessed scraping website: programmatically accessing data website asking program download much data need frequently needed. integral part course focus perform ethical web scrapping, scale web scrapping download massive amounts data well program scraper access data frequent intervals.course also focus emerging technology websites share data: Application Programming Interfaces (API). touch upon basic formats data sharing APIs well make thousands data requests just seconds. Special emphasis made security ethical guidelines speaking APIs.big part course emphasize automation, way student create robust scalable data acquisition pipelines. step process, focus practical applications techniques exercise active participation students hands-challenges data acquisition. course make heavy use Github students need share homework well explore immense repository data acquisition technology already available open source software.goal course empower students right toolset ideas able create data acquisition programs, automate data extraction pipeline quickly transform data formats suitable analysis. course place contents light legal ethical consequences data acquisition can entail, always informing students best practices grabbing data internet.course assumes students familiar R programming language, transforming manipulating datasets well saving work Git Github. prior knowledge software development data acquisition techniques needed.","code":""},{"path":"index.html","id":"curriculum","chapter":"1 Introduction","heading":"1.1 Curriculum","text":"introduction Web Scraping\nWeb Scraping?\nTypes Web Scraping\nData formats: XML HTML\nPractical access XML HTML\nAutomation Web Scraping programs\nSelenium JavaScript based scraping\nEthical issues Web Scraping\nPractical exercises\nWeb Scraping?Types Web ScrapingData formats: XML HTMLPractical access XML HTMLAutomation Web Scraping programsSelenium JavaScript based scrapingEthical issues Web ScrapingPractical exercisesData APIs\nAPI\nFundamentals API communication\nintroduction JSON format\nCreate API (share )\nREST architecture\nAPIs way share obtain data (kind)\nAutomation API requests\nTalking Databases\nAuthentication ethical access APIs\nPractical exercises\nAPIFundamentals API communicationAn introduction JSON formatCreate API (share )REST architectureAPIs way share obtain data (kind)Automation API requestsTalking DatabasesAuthentication ethical access APIsPractical exercisesAutomation Data Acquisition\nneed automation?\nAccessing servers\nTechnologies automating programs\nAutomating cron jobs\nLogging tasks\nPractical exercises\nneed automation?Accessing serversTechnologies automating programsAutomating cron jobsLogging tasksPractical exercises","code":""},{"path":"introduction-to-webscraping.html","id":"introduction-to-webscraping","chapter":"2 Introduction to Webscraping","heading":"2 Introduction to Webscraping","text":"Welcome world collecting data internet. Although \nTODOInstall scrapex explain problem decribed scrapex README web scraping tutorial doomed obsolete etc..\nWebscraping creative: clear way get want. Need come creative solutions.need mad data cleaning skills string / regex intermediatte. Come general regex encompasses need know read book can’t figure , study . Talk string heavy lifter book.TODO: need write entire webscraping part can done without internet\nTODO: Add general script save plots folder chapter automatically (look R4DS initial scripts)","code":""},{"path":"a-primer-on-webscraping.html","id":"a-primer-on-webscraping","chapter":"3 A primer on Webscraping","heading":"3 A primer on Webscraping","text":"Webscraping subtle art ninja. need expert many things swiss army knife skills comes data cleaning. String manipulations, subetting, clever tricks approximations rather exact solutions companion day day. book skip usual start basics sections directly giving results efforts. aim primer create plot like one:plot shows election results political parties Spain since 1978. data local computer ’ll need find online scrape . scrape specifically mean write little R script go website manually select data points tell . Wikipedia data remember websites change. make book persistent time possible, history_elections_spain_ex() function scrapex already website saved locally (original website https://en.wikipedia.org/wiki/Elections_in_Spain). Let’s load packages take look website.website saved locally computer. can directly visualize browser like :bottom right can see plot ’d like generate. plots political parties since 1978 2020 elections. first step webscraping ‘read’ website R. can read_html function. pass website link (typical https://en.wikipedia.org string) since already website locally, just pass path local website:can’t understand much output read_html HTML code behind website long. read_html shows top level details. case, don’t need understand details first. Now website already R need figure actual data elections website. scroll , near end website ’ll see table like one:precisely data need. contains election results parties since 1978. rvest package (already loaded also read_html function) handy function called html_table() automatically extracts tables website R. However, html_table() needs know website ’re working ned pass html_website read previous step:html_table() reads tables ’s lot information all_tables (list 10 tables precise). won’t print entire R object ’s verbose encourage reader write all_tables R console explore tables scraped automatically. understand information ’re looking . carefully inspecting tables, figure table ’re looking slot 5:can see ’s table saw website :first column year column name political party. However, table problems need fix. Let’s outline everything:first row table emptyThe Election column character column row 15 16 contains months Apr. Nov..political party column values numbers (usually values representing foot notes [k] others Dissolved parties dissolved years). force columns character columns fact want columns class numeric visualize plot.Column names also footnote values names. probably remove .recalled first paragraphs primer, able webscraping, need data ninja. ’ll need become familiar basics regular expressions (fancy name manipulating strings) also cleaning data. ’ll use basics string manipulation, ’s fine feel completely lost. Just work hard little little ’ll learn tricks along way.first thing ’d want keep columns character. problems related columns:can see different string values columns. ’d want replace non-numeric values NA’s. way convert columns numbers won’t loose information. can remove values? went columns wrote character values need remove:Now just need apply basic regular expression (regex now ) skills remove . Let’s explain want . regex world | stands . means want find words Banned Boycotted replace NA' write Banned|Boycotted. literally means Banned Boycotted. can take previous wrong_labels vector insert | wrong labels:effectively says: Dissolved [k] [l], …string can use function str_replace_all replace wrong labels NA’s. ’s ’d :Alright, don’t get stressed, ’ll explain line line. second line references data (elections_data, one ’ve working now). third line uses mutate_if function works applying function columns subset based criteria. Let’s break explanation even . can actually read code like :elections_dataFor columns character columns (mutate_if(character, ...))Apply transformation (mutate_if(character, ~ str_replace_all(...))example means character columns, function str_replace_all function applied. function replaces wrong_labels NA’s. can see right away:columns still characters, don’t wrong labels identified . problem Election column months Apr. Nov. won’t able convert numeric. can apply regex trick saying “replace Apr. Nov. empty string”. Let’s :Let’s check everything worked expected:go, don’t strings se columns anymore. Let’s transform columns numeric remove first row empty:go, columns class numeric look nice tidy plotting. Last step need take remove footnote values party column names. ’ll need advanced regex patterns ’ll explain briefly (separate chapter ). pattern ’ll use [.+] means: detect character (.) repeated one times (+) enclosed within brackets ([] part). example, string Election won’t find match bracket values repeated one times. However, column name UCD[] pattern: contains two brackets [] value repeated one time (). ’s last quick need take account brackets ([]) special meaning regex world. signal regex want match brackets literally, need append backslash (\\\\). final regex pattern want match : \\\\[.+\\\\]. Let’s use rename columns:dataset ready plot. ’s tidy clean. Let’s plot :","code":"\nlibrary(scrapex)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nlink <- history_elections_spain_ex()\nlink## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/8bd8b482443a462d0c5a1f9fd4e932dc/scrapex/extdata/history_elections_spain//Elections_Spain.html\"\nbrowseURL(prep_browser(link))\nhtml_website <-\n  link %>%\n  read_html()\n\nhtml_website## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nall_tables <-\n  html_website %>%\n  html_table()\nelections_data <- all_tables[[5]]\nelections_data## # A tibble: 16 × 18\n##    Election  `UCD[a]`     PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##    <chr>     <chr>       <dbl>   <dbl> <chr>      <dbl> <dbl>    <dbl>    <dbl>\n##  1 Election  \"\"           NA      NA   \"\"          NA    NA       NA       NA  \n##  2 1977      \"34.4\"       29.3     8.3 \"9.3\"        2.8   1.7      0.8      0.1\n##  3 1979      \"34.8\"       30.4     6.1 \"10.8\"       1.7   1.6      0.7      0.3\n##  4 1982      \"6.8\"        48.1    26.4 \"4.0\"        3.7   1.9      0.7      0.2\n##  5 1986      \"Dissolved\"  44.1    26   \"4.6\"        5     1.5      0.4      0.1\n##  6 1989      \"Dissolved\"  39.6    25.8 \"9.1\"        5     1.2      0.4      0.2\n##  7 1993      \"Dissolved\"  38.8    34.8 \"9.6\"        4.9   1.2      0.8      0.5\n##  8 1996      \"Dissolved\"  37.6    38.8 \"10.5\"       4.6   1.3      0.7      0.9\n##  9 2000      \"Dissolved\"  34.2    44.5 \"5.4\"        4.2   1.5      0.8      1.3\n## 10 2004      \"Dissolved\"  42.6    37.7 \"5.0\"        3.2   1.6      2.5      0.8\n## 11 2008      \"Dissolved\"  43.9    39.9 \"3.8\"        3     1.2      1.2      0.8\n## 12 2011      \"Dissolved\"  28.8    44.6 \"6.9\"        4.2   1.3      1.1      0.8\n## 13 2015      \"Dissolved\"  22      28.7 \"3.7\"        2.2   1.2      2.4      0.3\n## 14 2016      \"Dissolved\"  22.6    33   \"[k]\"        2     1.2      2.6      0.2\n## 15 Apr. 2019 \"Dissolved\"  28.7    16.7 \"[l]\"        1.9   1.5      3.9      0.4\n## 16 Nov. 2019 \"Dissolved\"  28      20.8 \"[l]\"        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <chr>, `CDS[h]` <chr>, `CC[i]` <dbl>,\n## #   UPyD <chr>, Cs <chr>, Com. <chr>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>\nelections_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]`    `IU[c]` `EHB[g]`    `CDS[h]`    UPyD        Cs    Com. \n##    <chr>     <chr>       <chr>   <chr>       <chr>       <chr>       <chr> <chr>\n##  1 Election  \"\"          \"\"      \"\"          \"\"          \"\"          \"\"    \"\"   \n##  2 1977      \"34.4\"      \"9.3\"   \"0.2\"       \"\"          \"\"          \"\"    \"\"   \n##  3 1979      \"34.8\"      \"10.8\"  \"1.0\"       \"\"          \"\"          \"\"    \"\"   \n##  4 1982      \"6.8\"       \"4.0\"   \"1.0\"       \"2.9\"       \"\"          \"\"    \"\"   \n##  5 1986      \"Dissolved\" \"4.6\"   \"1.1\"       \"9.2\"       \"\"          \"\"    \"\"   \n##  6 1989      \"Dissolved\" \"9.1\"   \"1.1\"       \"7.9\"       \"\"          \"\"    \"\"   \n##  7 1993      \"Dissolved\" \"9.6\"   \"0.9\"       \"1.8\"       \"\"          \"\"    \"\"   \n##  8 1996      \"Dissolved\" \"10.5\"  \"0.7\"       \"0.2\"       \"\"          \"\"    \"\"   \n##  9 2000      \"Dissolved\" \"5.4\"   \"Boycotted\" \"0.1\"       \"\"          \"\"    \"\"   \n## 10 2004      \"Dissolved\" \"5.0\"   \"Banned\"    \"0.1\"       \"\"          \"\"    \"\"   \n## 11 2008      \"Dissolved\" \"3.8\"   \"Banned\"    \"0.0\"       \"1.2\"       \"0.2\" \"\"   \n## 12 2011      \"Dissolved\" \"6.9\"   \"1.4\"       \"Dissolved\" \"4.7\"       \"Did… \"0.5\"\n## 13 2015      \"Dissolved\" \"3.7\"   \"0.9\"       \"Dissolved\" \"0.6\"       \"13.… \"[k]\"\n## 14 2016      \"Dissolved\" \"[k]\"   \"0.8\"       \"Dissolved\" \"0.2\"       \"13.… \"[k]\"\n## 15 Apr. 2019 \"Dissolved\" \"[l]\"   \"1.0\"       \"Dissolved\" \"Did not r… \"15.… \"0.7\"\n## 16 Nov. 2019 \"Dissolved\" \"[l]\"   \"1.2\"       \"Dissolved\" \"[m]\"       \"6.8\" \"[n]\"\nwrong_labels <- c(\n  \"Dissolved\",\n  \"[k]\",\n  \"[l]\",\n  \"[m]\",\n  \"n\",\n  \"Banned\",\n  \"Boycotted\",\n  \"Did not run\"\n)\nwrong_labels <- paste0(wrong_labels, collapse = \"|\")\nwrong_labels## [1] \"Dissolved|[k]|[l]|[m]|n|Banned|Boycotted|Did not run\"\nsemi_cleaned_data <-\n  elections_data %>%\n  mutate_if(\n    is.character,\n    ~ str_replace_all(string = .x, pattern = wrong_labels, replacement = NA_character_)\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>     <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>      \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977      \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979      \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982      \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986       <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989       <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993       <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996       <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000       <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004       <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008       <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011       <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015       <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016       <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 Apr. 2019  <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 Nov. 2019  <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate(\n    Election = str_replace_all(string = Election, pattern = \"Apr. |Nov. \", replacement = \"\")\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>    <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>     \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977     \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979     \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982     \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986      <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989      <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993      <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996      <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000      <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004      <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008      <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011      <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015      <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016      <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 2019      <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 2019      <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate_all(as.numeric) %>%\n  filter(!is.na(Election))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election `UCD[a]`  PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##       <dbl>    <dbl> <dbl>   <dbl>   <dbl>    <dbl> <dbl>    <dbl>    <dbl>\n##  1     1977     34.4  29.3     8.3     9.3      2.8   1.7      0.8      0.1\n##  2     1979     34.8  30.4     6.1    10.8      1.7   1.6      0.7      0.3\n##  3     1982      6.8  48.1    26.4     4        3.7   1.9      0.7      0.2\n##  4     1986     NA    44.1    26       4.6      5     1.5      0.4      0.1\n##  5     1989     NA    39.6    25.8     9.1      5     1.2      0.4      0.2\n##  6     1993     NA    38.8    34.8     9.6      4.9   1.2      0.8      0.5\n##  7     1996     NA    37.6    38.8    10.5      4.6   1.3      0.7      0.9\n##  8     2000     NA    34.2    44.5     5.4      4.2   1.5      0.8      1.3\n##  9     2004     NA    42.6    37.7     5        3.2   1.6      2.5      0.8\n## 10     2008     NA    43.9    39.9     3.8      3     1.2      1.2      0.8\n## 11     2011     NA    28.8    44.6     6.9      4.2   1.3      1.1      0.8\n## 12     2015     NA    22      28.7     3.7      2.2   1.2      2.4      0.3\n## 13     2016     NA    22.6    33      NA        2     1.2      2.6      0.2\n## 14     2019     NA    28.7    16.7    NA        1.9   1.5      3.9      0.4\n## 15     2019     NA    28      20.8    NA        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <dbl>, `CDS[h]` <dbl>, `CC[i]` <dbl>,\n## #   UPyD <dbl>, Cs <dbl>, Com. <dbl>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  rename_all(~ str_replace_all(.x, \"\\\\[.+\\\\]\", \"\"))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election   UCD  PSOE    PP    IU   CDC   PNV   ERC   BNG   EHB   CDS    CC\n##       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1     1977  34.4  29.3   8.3   9.3   2.8   1.7   0.8   0.1   0.2  NA    NA  \n##  2     1979  34.8  30.4   6.1  10.8   1.7   1.6   0.7   0.3   1    NA    NA  \n##  3     1982   6.8  48.1  26.4   4     3.7   1.9   0.7   0.2   1     2.9  NA  \n##  4     1986  NA    44.1  26     4.6   5     1.5   0.4   0.1   1.1   9.2   0.3\n##  5     1989  NA    39.6  25.8   9.1   5     1.2   0.4   0.2   1.1   7.9   0.3\n##  6     1993  NA    38.8  34.8   9.6   4.9   1.2   0.8   0.5   0.9   1.8   0.9\n##  7     1996  NA    37.6  38.8  10.5   4.6   1.3   0.7   0.9   0.7   0.2   0.9\n##  8     2000  NA    34.2  44.5   5.4   4.2   1.5   0.8   1.3  NA     0.1   1.1\n##  9     2004  NA    42.6  37.7   5     3.2   1.6   2.5   0.8  NA     0.1   0.9\n## 10     2008  NA    43.9  39.9   3.8   3     1.2   1.2   0.8  NA     0     0.7\n## 11     2011  NA    28.8  44.6   6.9   4.2   1.3   1.1   0.8   1.4  NA     0.6\n## 12     2015  NA    22    28.7   3.7   2.2   1.2   2.4   0.3   0.9  NA     0.3\n## 13     2016  NA    22.6  33    NA     2     1.2   2.6   0.2   0.8  NA     0.3\n## 14     2019  NA    28.7  16.7  NA     1.9   1.5   3.9   0.4   1    NA     0.5\n## 15     2019  NA    28    20.8  NA     2.2   1.6   3.6   0.5   1.2  NA     0.5\n## # … with 6 more variables: UPyD <dbl>, Cs <dbl>, Com. <dbl>, Pod. <dbl>,\n## #   Vox <dbl>, MP <dbl>\n# Pivot from wide to long to plot it in ggplot\ncleaned_data <-\n  semi_cleaned_data %>%\n  pivot_longer(-Election, names_to = \"parties\")\n\n# Plot it\ncleaned_data %>%\n  ggplot(aes(Election, value, color = parties)) +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  scale_color_viridis_d() +\n  theme_minimal()"},{"path":"data-formats-for-webscraping.html","id":"data-formats-for-webscraping","chapter":"4 Data Formats for Webscraping","heading":"4 Data Formats for Webscraping","text":"webscraping ’ll involve parsing either XML HTML. two formats much alike fact many examples ’ll notice almost indistinguishable. web ’ll find formal definitions languages ’s definition layman person: series tags formats website structured (HTML) store transfer data (XML). Still rather vague eh? Let’s go concrete examples.XML abbreviation Extensible Markup Language whereas HTML stands Hypertext Markup Language. might’ve guessed, ’re ‘Markup’ languages, share lot common. R can read formats xml2 package. Let’s load package getting started:","code":"\nlibrary(xml2)"},{"path":"data-formats-for-webscraping.html","id":"a-primer-on-xml-and-html","chapter":"4 Data Formats for Webscraping","heading":"4.1 A primer on XML and HTML","text":"Let’s begin simple example. define string look structure:XML HTML basic building blocks called tags. example, first tag structure shown <people>. tag matched <\/people> end string:pay close attention, ’ll see tag XML structure beginning (signaled <>) end (signaled <\/>). example, next tag <people> <jason> right tag <carol> end jason tag <\/jason>.Similarly, ’ll find <carol> tag also matched <\/carol> finishing tag.XML world, tags can whatever meaning attach (<people> <occupation>). However, HTML hundreds tags standard structuring websites. want stop second highlight XML HTML many differences, conceptual visible users. Throughout rest chapter ’ll focus think important ones context webscraping.Let’s compare visibly previous XML example HTML:One key differences tags (<div>, <head>, <title>, etc..) specific properties structure website shown someone browser. Anything inside <head> tag header website. Anything within <body> tag body website, . tags standard across HTML language predetermined behavior format website. Let’s look another HTML also result structured website. ’s code:rendered website:heading contains hyperlink (<> tag href property), bigger bold comparison “paragraph”. specific tags together attributes interpreted give outline text ’ll start notice, trademark value HTML language.contrast, XML tags meaning creator meant . <occupation> tag simply means inside tag occupation related content. ’s say XML transfering data (tags inherent behavior) HTML structuring website.Another difference XML HTML HTML tags don’t need closed, meaning don’t need <\\ > tag (example <br> adds space content website). hand, XML strict ’ll find tags equivalent closing tag.Now, might asking , since standard tags XML, many standard HTML tags ? Well, many remember ’re getting started. ’s short set:comprehensive list see . don’t learn every single tag webscraping (fact know handful) ’s helpful hint able locate specific parts website ’re interested webscraping. theory way, let’s get hands dirty manipulating formats R.R can read XML HTML formats read_xml read_html functions. Let’s read XML string fake example look general structure:can see structure tree-based, meaning tags <jason> <carol> nested within <people> tag. XML jargon, <people> root node, whereas <jason> <carol> child nodes <people>.detail, structure follows:root node <people>child nodes <jason> <carol>child node nodes <first_name>, <married>, <last_name> <occupation> nested within .Put another way, something nested within node, nested node child upper-level node. example, root node <people> can check children:","code":"\nxml_test <- \"<people>\n<jason>\n  <person type='fictional'>\n    <first_name>\n      <married>\n        Jason\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Bourne\n    <\/last_name>\n    <occupation>\n      Spy\n    <\/occupation>\n  <\/person>\n<\/jason>\n<carol>\n  <person type='real'>\n    <first_name>\n      <married>\n        Carol\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Kalp\n    <\/last_name>\n    <occupation>\n      Scientist\n    <\/occupation>\n  <\/person>\n<\/carol>\n<\/people>\n\"\n\ncat(xml_test)## <people>\n## <jason>\n##   <person type='fictional'>\n##     <first_name>\n##       <married>\n##         Jason\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Bourne\n##     <\/last_name>\n##     <occupation>\n##       Spy\n##     <\/occupation>\n##   <\/person>\n## <\/jason>\n## <carol>\n##   <person type='real'>\n##     <first_name>\n##       <married>\n##         Carol\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Kalp\n##     <\/last_name>\n##     <occupation>\n##       Scientist\n##     <\/occupation>\n##   <\/person>\n## <\/carol>\n## <\/people>\nhtml_test <- \"<html>\n  <head>\n    <title>Div Align Attribbute<\/title>\n  <\/head>\n  <body>\n    <div align='left'>\n      First text\n    <\/div>\n    <div align='right'>\n      Second text\n    <\/div>\n    <div align='center'>\n      Third text\n    <\/div>\n    <div align='justify'>\n      Fourth text\n    <\/div>\n  <\/body>\n<\/html>\n\"<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title<\/title>\n<\/head>\n<body>\n\n<h1> <a href=\"www.google.com\">This is a Heading <\/a> <\/h1>\n<br>\n<p>This is a paragraph.<\/p>\n\n<\/body>\n<\/html>\nxml_raw <- read_xml(xml_test)\nxml_structure(xml_raw)## <people>\n##   <jason>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n##   <carol>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n# xml_child returns only one child (specified in search)\n# Here, jason is the first child\nxml_child(xml_raw, search = 1)## {xml_node}\n## <jason>\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n# Here, carol is the second child\nxml_child(xml_raw, search = 2)## {xml_node}\n## <carol>\n## [1] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Use xml_children to extract **all** children\nchild_xml <- xml_children(xml_raw)\n\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ..."},{"path":"data-formats-for-webscraping.html","id":"tag-attributes","chapter":"4 Data Formats for Webscraping","heading":"4.2 Tag attributes","text":"Tags can also different attributes usually specified <fake_tag attribute='fake'> ended usual <\/fake_tag>. look XML structure example, ’ll notice <person> tag attribute called type. ’ll see real-world example, extracting attributes often aim scraping adventure. Using xml_attrs function can extract attributes match specific name:Wait, didn’t work? Well, look output child_xml, two nodes <jason> <carol>.tags attribute? , , something like <jason type='fake_tag'>. need look <person> tag within <jason> <carol> extract attribute <person>.sound familiar? <jason> <carol> associated <person> tag , making children. can just go one level running xml_children tags extract .Using xml_path function can even find ‘address’ nodes retrieve specific tags without write xml_children many times. example:‘address’ specific tags tree extract automatically? extract specific ‘addresses’ XML tree, main function ’ll use xml_find_all. function accepts XML tree ‘address’ string. can use simple strings, one given xml_path:expression asking node \"/people/jason/person\". return saying xml_raw %>% xml_child(search = 1). deeply nested trees, xml_find_all many times much cleaner calling xml_child recursively many times.However, cases ‘addresses’ used xml_find_all come separate language called XPath (fact, ‘address’ ’ve looking XPath). XPath complex language (regular expressions strings) ’ll cover chapter # TODO.Attributes flexible XML tag can many attributes see fit. example:attributes can also repeated many times, example, might generic <person> tag used person database:usual HTML, tags also standard specific meaning. Let’s read previous HTML example visualize structure:structure shows <div> tag align attribute. might’ve guessed, attribute aligns text. common attributes HTML tags easy understand. ’s list :<> tag contains text hyperlink (href). <img> tag (abbreviation image) contains src tag points image , together width height image. Finally, <p> tag short paragraph contains style attribute declaring styling properties text. just examples common HTML tags common attributes. case, ’ve outlined , ’s fine know tags; intuition behind enough locate specific parts website.finalize, whenever ’ll scraping something, ’ll want know whether ’s XML HTML based. manage receive .html .xml ’s just simple looking extension file. chance access source code file, can also look tags quickly see many standard HTML tags deduce actual format. Another solution just look root node ’ll see hint right away. ’ll see xml signalled right beginning:html:","code":"\n# Extract the attribute type from all nodes\nxml_attrs(child_xml, \"type\")## [[1]]\n## named character(0)\n## \n## [[2]]\n## named character(0)\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ...\n# We go down one level of children\nperson_nodes <- xml_children(child_xml)\n\n# <person> is now the main node, so we can extract attributes\nperson_nodes## {xml_nodeset (2)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n## [2] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Both type attributes\nxml_attrs(person_nodes, \"type\")## [[1]]\n##        type \n## \"fictional\" \n## \n## [[2]]\n##   type \n## \"real\"\n# Specific address of each person tag for the whole xml tree\n# only using the `person_nodes`\nxml_path(person_nodes)## [1] \"/people/jason/person\" \"/people/carol/person\"\n# You can use results from xml_path like directories\nxml_find_all(xml_raw, \"/people/jason/person\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...<name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<\/name><name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<person age=\"25\" status=\"single\" occupation=\"doctor\"> Jane Doe <\/person>\n<\/name>\nhtml_raw <- read_html(html_test)\nhtml_structure(html_raw)## <html>\n##   <head>\n##     <title>\n##       {text}\n##   <body>\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}<a href=\"https://www.w3schools.com\">Visit W3Schools<\/a>\n<img src=\"img_girl.jpg\" width=\"500\" height=\"600\">\n<p style=\"color:red;\">This is a red paragraph.<\/p><?xml version=\"1.0>\n<company>\n    <name> John Doe <\/name>\n    <email> johndoe@gmail.com <\/email>\n<\/address> <!DOCTYPE html>\n<html>\n<body>\n\n<h1>My First Heading<\/h1>\n<p>My first paragraph.<\/p>\n\n<\/body>\n<\/html>"},{"path":"data-formats-for-webscraping.html","id":"conclusion","chapter":"4 Data Formats for Webscraping","heading":"4.3 Conclusion","text":"frank . Although XML HTML important differences respect technology philosophy, difference pretty much us: webscraping needs, don’t care data formatted website (HTML) whether tags special meanings (XML), care formatted tags can extract information.Let’s brief recap key take aways. read XML HTML can use equivalent read_* functions xml2 package. navigate nodes data forms can use xml_child recursively find ’re looking . extract children given node, can use xml_children extract attributes given tag, can resort xml_attr. Finally, want extract text tag, xml_text extracts . functions give handy toolset explore small scale tree nodes oboth XML HTML documents.finish, ’s important highlight HTML much widely used webscraping. webcsraping websites HTML specifically designed show website formatted. However, difference indistinguishable webscraping R.","code":""},{"path":"data-formats-for-webscraping.html","id":"exercises","chapter":"4 Data Formats for Webscraping","heading":"4.4 Exercises","text":"Extract values align attributes html_raw (Hint, look function xml_children).Extract occupation Carol Kalp xml_rawExtract text <div> tags html_raw. result look specifically like :Manually create XML string contains root node, two children nested within two grandchildren nested within child. first child second grandchild second child attribute called family set ‘1’. Read string find two attributes function xml_find_all xml_attrs.output previous exercises either xml_nodeset html_document (can read top print results):Can extract text last name Carol scientist using R subsetting rules object? example some_object$people$person$... (Hint: xml2 function called as_list).","code":"\ndiv_nodes <- xml_child(html_raw, search = 2)\nxml_attrs(xml_children(div_nodes), \"align\")\ncarol_node <- xml_child(xml_raw, search = 2)\nperson_node <- xml_child(carol_node, search = 1)\noccupation <- xml_child(person_node, search = 3)\nxml_text(occupation)[1] \"\\n      First text\\n    \"  \"\\n      Second text\\n    \"\n[3] \"\\n      Third text\\n    \"  \"\\n      Fourth text\\n    \"\ndiv_nodes <- xml_child(html_raw, search = 2)\nxml_text(xml_children(div_nodes), \"align\")\ncustom_xml <- \"<root>\n<child1 family='1'>\n<granchild1>\n<\/granchild1>\n<granchild2>\n<\/granchild2>\n<\/child1>\n\n\n<child2>\n<granchild1>\n<\/granchild1>\n<granchild2 family='1'>\n<\/granchild2>\n<\/child2>\n<\/root>\"\n\ncustom_raw <- read_xml(custom_xml)\n\n# First attribute\nxml_attrs(xml_find_all(custom_raw, \"/root/child1\"))\n\n# Second attribute\nxml_attrs(xml_find_all(custom_raw, \"/root/child2/granchild2\")){html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n    <div align=\"left\">\\n      First text\\n    <\/div>\\n    <div al ...\nxml_list <- as_list(xml_raw)\nxml_list$people$carol$person$last_name[[1]]## [1] \"\\n        Kalp\\n    \""},{},{"path":"what-they-never-taught-you-about-regular-expressions.html","id":"what-they-never-taught-you-about-regular-expressions","chapter":"5 What they never taught you about regular expressions","heading":"5 What they never taught you about regular expressions","text":"Primer regular expressions","code":""},{"path":"what-they-never-taught-you-about-regular-expressions.html","id":"exercises-1","chapter":"5 What they never taught you about regular expressions","heading":"5.1 Exercises","text":"","code":""},{"path":"what-they-never-taught-you-about-xpath.html","id":"what-they-never-taught-you-about-xpath","chapter":"6 What they never taught you about XPath","heading":"6 What they never taught you about XPath","text":"Primer XPathTo extract tags document, can use //name_of_tag.previous XPath, ’re searching married tags within complete XML tree. result returns married nodes (use words tags nodes interchangeably) complete tree structure. Another example finding <occupation> tags:want find tag can replace \"//occupation\" tag interest xml_find_all find .wanted find tags current node, need add . beginning: \".//occupation\". example, dived <jason> tag wanted <occupation> tag, \"//occupation\" returns <occupation> tags. Instead, \".//occupation\" return found tags current tag. example:first example returns <jason>’s occupation whereas second returned occupations, regardless tree.XPath also allows identify tags contain one specific attribute, one’s saw earlier. example, filter <person> tags attribute filter set fictional, :wanted tags current nodes, trick learned earlier work: \".//person[@type='fictional']\". just primers can help jump easily using XPath, encourage look examples web, complex websites often require complex XPath expressions.begin real-word example, might asking can actually extract text/numeric data nodes. Well, ’s easy: xml_text.’ve narrowed tree-based search one single piece text numbers, xml_text() extract (’s also xml_double xml_integer extracting numbers). said, XPath really huge language. ’re interested, XPath cheat sheets helped lot learn tricks easy scraping.","code":"\nxml_test <- \"<people>\n<jason>\n  <person type='fictional'>\n    <first_name>\n      <married>\n        Jason\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Bourne\n    <\/last_name>\n    <occupation>\n      Spy\n    <\/occupation>\n  <\/person>\n<\/jason>\n<carol>\n  <person type='real'>\n    <first_name>\n      <married>\n        Carol\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Kalp\n    <\/last_name>\n    <occupation>\n      Scientist\n    <\/occupation>\n  <\/person>\n<\/carol>\n<\/people>\n\"\n\nxml_raw <- read_xml(xml_test)\n# Search for all 'married' nodes\nxml_find_all(xml_raw, \"//married\")## {xml_nodeset (2)}\n## [1] <married>\\n        Jason\\n      <\/married>\n## [2] <married>\\n        Carol\\n      <\/married>\nxml_find_all(xml_raw, \"//occupation\")## {xml_nodeset (2)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n## [2] <occupation>\\n      Scientist\\n    <\/occupation>\nxml_raw %>%\n  # Dive only into Jason's tag\n  xml_child(search = 1) %>%\n  xml_find_all(\".//occupation\")## {xml_nodeset (1)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n# Instead, the wrong way would have been:\nxml_raw %>%\n  # Dive only into Jason's tag\n  xml_child(search = 1) %>%\n  # Here we get both occupation tags\n  xml_find_all(\"//occupation\")## {xml_nodeset (2)}\n## [1] <occupation>\\n      Spy\\n    <\/occupation>\n## [2] <occupation>\\n      Scientist\\n    <\/occupation>\n# Give me all the tags 'person' that have an attribute type='fictional'\nxml_raw %>%\n  xml_find_all(\"//person[@type='fictional']\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\nxml_raw %>%\n  xml_find_all(\".//occupation\") %>%\n  xml_text()## [1] \"\\n      Spy\\n    \"       \"\\n      Scientist\\n    \""},{"path":"what-they-never-taught-you-about-xpath.html","id":"exercises-2","chapter":"6 What they never taught you about XPath","heading":"6.1 Exercises","text":"","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"case-study-scraping-spanish-school-locations-from-the-web","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7 Case study: scraping Spanish school locations from the web","text":"case study ’ll working basics web scraping using R xml2 package. ’ll begin simple example using fake data elaborate trying scrape location sample schools Spain.","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"basic-steps","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.1 Basic steps","text":"web scraping R, can fulfill almost needs xml2 package. wander web, ’ll see many examples using rvest package. xml2 rvest similar don’t feel ’re lacking behind learning one . addition two packages, ’ll need libraries plotting locations map (ggplot2, sf, rnaturalearth), identifying scrape (httr) wrangling data (tidyverse).Additionally, ’ll also need package scrapex. real-world example ’ll , ’ll scraping data website www.buscocolegio.com locate sample schools Spain. However, throughout tutorial won’t scraping data directly real-website. happen tutorial 6 months now www.buscocolegio.com updates design website? Everything real-world example lost.Web scraping tutorials usually unstable precisely . circumvent problem, ’ve saved random sample websites schools www.buscocolegio.com R package called scrapex. Although links ’ll working hosted locally machine, HTML website similar one hosted website (exception images/icons deleted purpose make package lightweight).can install package :","code":"\n# install.packages(\"devtools\")\ndevtools::install_github(\"cimentadaj/scrapex\")\nlibrary(xml2)\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)\nlibrary(scrapex)"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"real-world-example","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.2 Real-world example","text":"’re interested making list many schools Spain visualizing location. can useful many things matching population density children across different regions school locations. website www.buscocolegio.com contains database schools similar ’re looking . described beginning, instead ’re going use scrapex function spanish_schools_ex() containing links sample websites different schools saved locally computer.Let’s look example one school.’re interested looking website interactively browser, can browseURL(prep_browser(school_url)). Let’s read HTML (XML HTML usually interchangeable, use read_html).Web scraping strategies specific website ’re . get familiar website ’re interested able match perfectly information ’re looking . many cases, scraping two websites require vastly different strategies. particular example, ’re interested figuring location school extract location.image ’ll find typical school’s website wwww.buscocolegio.com. website lot information, ’re interested button circled orange rectangle. can’t find easily, ’s Google Maps right says “Buscar colegio cercano”.click button, actually points towards coordinates school just find way figuring click button figure get information. browsers allow press CTRL + SHIFT + c time (Firefox Chrome support hotkey). window right popped full code, ’re right track:can search source code website. place mouse pointer lines code right-window, ’ll see sections website highlighted blue. indicates parts code refer parts website. Luckily us, don’t search complete source code find specific location. can approximate search typing text ’re looking search bar top right window:click enter, ’ll automatically directed tag information want.specifically, can see latitude longitude schools found attributed called href tag <>:Can see latitude longitude fields text highlighted blue? ’s hidden -words. precisely type information ’re . Extracting <> tags website (hint: XPath similar \"//\") yield hundreds matches <> common tag. Moreover, refining search <> tags href attribute also yield hundreds matches href standard attribute attach links within websites. need narrow search within website.One strategy find ‘father’ ‘grandfather’ node particular <> tag match node sequence grandfather -> father -> child node. looking structure small XML snippet right-window, see ‘grandfather’ <> tag <p class=\"d-flex align-items-baseline g-mt-5'> particularly long attribute named class.Don’t intimidated tag names long attributes. also don’t know attributes mean. know ‘grandfather’ <> tag ’m interested . using XPath skills, let’s search <p> tag see get one match.one match, good news. means can uniquely identify particular <p> tag. Let’s refine search say: Find <> tags children specific <p> tag. means ’ll add \"//\" previous expression. Since one <p> tag class, ’re interested checking whether one <> tag <p> tag.go! can see specific href contains latitude longitude data ’re interested . extract href attribute? Using xml_attr !Ok, now need regex skills get latitude longitude (regex expressions used search patterns inside string, example date. See examples):Ok, got information needed one single school. Let’s turn function can pass school’s link get coordinates back., set something called User-Agent. short, User-Agent . good practice identify person scraping website ’re causing trouble website, website can directly identify causing problems. can figure user agent paste string . addition, add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests.Ok, ’s working. thing left extract many schools. shown earlier, scrapex contains list 27 school links can automatically scrape. Let’s loop , get information coordinates collapse data frame.now locations schools, let’s plot :go! went literally information beginning tutorial interpretable summarized information using web data. can see schools Madrid (center) well regions Spain, including Catalonia Galicia.marks end scraping adventure finish, want mention ethical guidelines web scraping. Scraping extremely useful us can give headaches people maintaining website interest. ’s list ethical guidelines always follow:Read terms services: many websites prohibit web scraping breach privacy scraping data. One famous example.Read terms services: many websites prohibit web scraping breach privacy scraping data. One famous example.Check robots.txt file. file websites (www.buscocolegio.com ) tell specific paths inside website scrapable . See explanation robots.txt look like find .Check robots.txt file. file websites (www.buscocolegio.com ) tell specific paths inside website scrapable . See explanation robots.txt look like find .websites supported big servers, means can send 4-5 website requests per second. Others, www.buscocolegio.com . ’s good practice always put time sleep requests. example, set 5 seconds small website don’t want crash servers.websites supported big servers, means can send 4-5 website requests per second. Others, www.buscocolegio.com . ’s good practice always put time sleep requests. example, set 5 seconds small website don’t want crash servers.making requests, computational ways identifying . example, every request (one’s ) can something called User-Agent. good practice include User-Agent (code) admin server can directly identify someone’s causing problems due web scraping.making requests, computational ways identifying . example, every request (one’s ) can something called User-Agent. good practice include User-Agent (code) admin server can directly identify someone’s causing problems due web scraping.Limit scraping non-busy hours overnight. can help reduce chances collapsing website since fewer people visiting websites evening.Limit scraping non-busy hours overnight. can help reduce chances collapsing website since fewer people visiting websites evening.can read ethical issues .","code":"\nschool_links <- spanish_schools_ex()\n\n# Keep only the HTML file of one particular school.\nschool_url <- school_links[13]\n\nschool_url## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/8bd8b482443a462d0c5a1f9fd4e932dc/scrapex/extdata/spanish_schools_ex/school_3006839.html\"\n# Here we use `read_html` because `read_xml` is throwing an error\n# when attempting to read. However, everything we've discussed\n# should be the same.\nschool_raw <- read_html(school_url) %>% xml_child()\n\nschool_raw## {html_node}\n## <head>\n##  [1] <title>Aquí encontrarás toda la información necesaria sobre CEIP SANCHIS ...\n##  [2] <meta charset=\"utf-8\">\\n\n##  [3] <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shri ...\n##  [4] <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n\n##  [5] <meta name=\"author\" content=\"BuscoColegio\">\\n\n##  [6] <meta name=\"description\" content=\"Encuentra toda la información necesari ...\n##  [7] <meta name=\"keywords\" content=\"opiniones SANCHIS GUARNER, contacto SANCH ...\n##  [8] <link rel=\"shortcut icon\" href=\"/favicon.ico\">\\n\n##  [9] <link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Roboto+Sl ...\n## [10] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [11] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-awesome/css/font-awesom ...\n## [12] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line/css/simple-line-ic ...\n## [13] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line-pro/style.css\">\\n\n## [14] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-hs/style.css\">\\n\n## [15] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [16] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [17] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [18] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [19] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [20] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## ...\n# Search for all <p> tags with that class in the document\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']\")## {xml_nodeset (1)}\n## [1] <p class=\"d-flex align-items-baseline g-mt-5\">\\r\\n\\t                    < ...\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\")## {xml_nodeset (1)}\n## [1] <a href=\"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274 ...\nlocation_str <-\n  school_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n  xml_attr(attr = \"href\")\n\nlocation_str## [1] \"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274492&colegio.longitud=0.0221681\"\nlocation <-\n  location_str %>%\n  str_extract_all(\"=.+$\") %>%\n  str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n  str_split(\"&\") %>%\n  .[[1]]\n\nlocation## [1] \"38.8274492\" \"0.0221681\"\n# This sets your `User-Agent` globally so that all requests are\n# identified with this `User-Agent`\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:70.0) Gecko/20100101 Firefox/70.0\")\n)\n\n# Collapse all of the code from above into one function called\n# school grabber\n\nschool_grabber <- function(school_url) {\n  # We add a time sleep of 5 seconds to avoid\n  # sending too many quick requests to the website\n  Sys.sleep(5)\n\n  school_raw <- read_html(school_url) %>% xml_child()\n\n  location_str <-\n    school_raw %>%\n    xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n    xml_attr(attr = \"href\")\n\n  location <-\n    location_str %>%\n    str_extract_all(\"=.+$\") %>%\n    str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n    str_split(\"&\") %>%\n    .[[1]]\n\n  # Turn into a data frame\n  data.frame(\n    latitude = location[1],\n    longitude = location[2],\n    stringsAsFactors = FALSE\n  )\n}\n\n\nschool_grabber(school_url)##     latitude longitude\n## 1 38.8274492 0.0221681\nres <- map_dfr(school_links, school_grabber)\nres##    latitude  longitude\n## 1  42.72779 -8.6567935\n## 2  43.24439 -8.8921645\n## 3  38.95592 -1.2255769\n## 4  39.18657 -1.6225903\n## 5  40.38245 -3.6410388\n## 6  40.22929 -3.1106322\n## 7  40.43860 -3.6970366\n## 8  40.33514 -3.5155669\n## 9  40.50546 -3.3738441\n## 10 40.63826 -3.4537107\n## 11 40.38543 -3.6639500\n## 12 37.76485 -1.5030467\n## 13 38.82745  0.0221681\n## 14 40.99434 -5.6224391\n## 15 40.99434 -5.6224391\n## 16 40.56037 -5.6703725\n## 17 40.99434 -5.6224391\n## 18 40.99434 -5.6224391\n## 19 41.13593  0.9901905\n## 20 41.26155  1.1670507\n## 21 41.22851  0.5461471\n## 22 41.14580  0.8199749\n## 23 41.18341  0.5680564\n## 24 42.07820  1.8203155\n## 25 42.25245  1.8621546\n## 26 41.73767  1.8383666\n## 27 41.62345  2.0013628\nres <- mutate_all(res, as.numeric)\n\nsp_sf <-\n  ne_countries(scale = \"large\", country = \"Spain\", returnclass = \"sf\") %>%\n  st_transform(crs = 4326)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = res, aes(x = longitude, y = latitude)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"wrap-up","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.3 Wrap up","text":"tutorial introduced basic concepts web scraping applied real-world setting. Web scraping vast field computer science (can find entire books subject ). covered basic techniques think can take long way ’s definitely learn. curious turn, ’m looking forward upcoming book “Field Guide Web Scraping Accessing APIs R” Bob Rudis, released near future. Now go scrape websites ethically!","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"exercises-3","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.4 Exercises","text":"","code":""},{"path":"automating-web-scraping-scripts.html","id":"automating-web-scraping-scripts","chapter":"8 Automating Web scraping scripts","heading":"8 Automating Web scraping scripts","text":"’re still unsure whether much info students. involves renting server stuff like . Moreover, assumes ’re using Linux using cron stuff might using mac windows. also don’t think might benefit start since probably gonna scrape simpler things.","code":""},{"path":"scraping-javascript-based-website.html","id":"scraping-javascript-based-website","chapter":"9 Scraping JavaScript based website","heading":"9 Scraping JavaScript based website","text":"","code":"\n## sudo docker pull selenium/standalone-firefox:2.53.0\n## docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.0\n\nlibrary(RSelenium)\nlibrary(rvest)\nlibrary(glue)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\n\nremDr <- remoteDriver(port = 4445L)\nremDr$open()\n\nclickForm <- function(driver, id_val, type_val) {\n  driver$findElement(value = glue(\"//*[@id='{id_val}']\"))$clickElement()\n  Sys.sleep(2)\n  driver$findElement(value = glue(\"//*[@id='{id_val}']\"))$sendKeysToElement(list(type_val))\n  Sys.sleep(2)\n  driver$findElement(value = glue(\"//*[@id='{id_val}']\"))$clickElement()\n  driver\n}\n\nremDr$navigate(\"https://www.educacion.gob.es/centros/selectaut.do;jsessionid=D1599759162212D625E129FAB8584742\")\n\nsleep <- 3\nval <- \"/html/body/div[2]/div[2]/form/div/fieldset/div[2]/ul/li[1]/a\"\nremDr$findElement(value = val)$clickElement()\n\n# Fill all forms\nSys.sleep(3)\nremDr <- clickForm(remDr, \"comboniv\", \"Formación Profesional\")\nSys.sleep(1)\nremDr <- clickForm(remDr, \"ssel_natur\", \"Centro privado\")\nSys.sleep(3)\nremDr <- clickForm(remDr, \"tipocentro\", \"Centro Privado de Formación Profesional Especifica\")\nSys.sleep(1)\n\n# Click enter to see all schools\nremDr$findElement(value = \"//*[@id='idGhost']\")$clickElement()\nSys.sleep(sleep)\n\nhtml_all <- remDr$getPageSource()[[1]] %>% read_html()\n\nall_properties <-\n  html_all %>%\n  html_elements(xpath = \"//ul\") %>%\n  html_elements(xpath = \".//li\") %>%\n  html_text()\n\nschools <- all_properties[str_detect(all_properties, \"Centros seleccionados\")]\nschools <- as.numeric(str_replace(schools, \"Centros seleccionados: \", \"\"))\nnumber_schools <- glue(\"/html/body/div[2]/div[2]/div/form/table/tbody/tr[{seq_len(schools)}]/td[5]/a\")\n\nall_dfs <- list()\n\nfor (i in seq_along(number_schools)) {\n  print(glue(\"School {i}\"))\n  remDr$findElement(value = number_schools[i])$clickElement()\n\n  ## Sys.sleep(sleep)\n  html <- remDr$getPageSource()[[1]] %>% read_html()\n\n  ensenanzas_table <-\n    html %>%\n    html_table() %>%\n    .[[1]]\n\n  all_properties <-\n    html %>%\n    html_elements(xpath = \"//div[@id='formulario']\") %>%\n    html_elements(xpath = \".//li\") %>%\n    html_text()\n\n  all_properties <- gsub(\"http://\", \"\", all_properties)\n\n  all_fields <- gsub(\"\\\\n|\\\\t\", \"\", all_properties)\n  matrix_fields <- str_split(all_fields, pattern = \":\", simplify = TRUE)\n\n  column_names <- str_trim(matrix_fields[, 1])\n  values <- str_trim(matrix_fields[, 2])\n\n  names(values) <- column_names\n\n  property_df <-\n    values %>%\n    enframe() %>%\n    pivot_wider()\n\n  final_df <- bind_cols(property_df, ensenanzas_table)\n\n  all_dfs[[i]] <- final_df\n\n  remDr$goBack()\n}\n\n\ncomplete_dfs <- bind_rows(all_dfs)"},{"path":"scraping-javascript-based-website.html","id":"exercises-4","chapter":"9 Scraping JavaScript based website","heading":"9.1 Exercises","text":"","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"ethical-issues-in-web-scraping","chapter":"10 Ethical issues in Web Scraping","heading":"10 Ethical issues in Web Scraping","text":"","code":""},{"path":"introduction-to-rest-apis.html","id":"introduction-to-rest-apis","chapter":"11 Introduction to REST APIs","heading":"11 Introduction to REST APIs","text":"API\nFundamentals API communication","code":""},{"path":"introduction-to-json.html","id":"introduction-to-json","chapter":"12 Introduction to JSON","heading":"12 Introduction to JSON","text":"Examples JSON\nExamples nested JSON\n’re parsed RHow handle nested data frames R","code":""},{"path":"a-primer-on-apis.html","id":"a-primer-on-apis","chapter":"13 A primer on APIs","heading":"13 A primer on APIs","text":"","code":""},{"path":"what-is-a-restful-api.html","id":"what-is-a-restful-api","chapter":"14 What is a RESTful API?","heading":"14 What is a RESTful API?","text":"","code":""},{"path":"authentication-in-apis.html","id":"authentication-in-apis","chapter":"15 Authentication in APIs","heading":"15 Authentication in APIs","text":"","code":""},{"path":"case-study-grabbing-data-from-a-public-api.html","id":"case-study-grabbing-data-from-a-public-api","chapter":"16 Case Study: grabbing data from a public API","heading":"16 Case Study: grabbing data from a public API","text":"","code":""},{"path":"why-automation-is-important.html","id":"why-automation-is-important","chapter":"17 Why automation is important","heading":"17 Why automation is important","text":"Examples data lost (bycicle)\nLack time scrape times","code":""},{"path":"automating-data-collectiong-programs.html","id":"automating-data-collectiong-programs","chapter":"18 Automating data collectiong programs","heading":"18 Automating data collectiong programs","text":"","code":""},{"path":"insightful-and-robust-data-collection-progams.html","id":"insightful-and-robust-data-collection-progams","chapter":"19 Insightful and robust data collection progams","heading":"19 Insightful and robust data collection progams","text":"Adding logger\nChecking inputs\nSaving empty rows avoid breaking programs","code":""}]
