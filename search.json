[{"path":"index.html","id":"welcome","chapter":"1 Welcome","heading":"1 Welcome","text":"Every day millions gigabytes data shared across internet. standard way internet users download data manually clicking downloading files formats Excel files, CSV files, Word files. However, process downloading files clicking works well downloading one two files. needed download 500 CSV files? happens need download data refreshed every 20 seconds? process manually clicking file simply feasible solution downloading data scale frequent intervals. ’s increasing amount data internet, ’s also increase tools allow access data programmatically. course focus exploring technologies.Let give example. COVID pandemic struck world 2020, paramount understand mortality rates causing general decomposition mortality (affecting males versus females well different age groups). team researchers Max Planck Institute Demographic Research set collect administrative data reported deaths country, regions, gender age. Now might sound like fun ’s actually awfully difficult countries uploaded data refreshed every day others weekly basis. regions given country might update data different schedule. website might different process getting know website can take time. Scale process collect data hundreds countries process collecting mortality data can become incredibly cumbersome: might spend hours web simply clicking links download data (see sample countries dates collect data ). Aside long tedious task boringly clicking links hours, chances ’ll also introduce errors along way. might confuse one region another assign wrong name CSV file, might skip particular country mistake might simply misspell country’s name, messing order files saving.example typical task want automate: wouldn’t great type robot automatically collect data different websites save files correct, explicity names ? ’s exactly team . created COVerAGE-DB dataset. help dozens collaborators, created web scraping scripts automatically collected data automated process run frequently needed. ’s managed created hundreds small robots work brought back data analyze. perfect motivation automated data acquisition extremely important tool every person works data benefit lot knowing tools.Throughout course, ’ll describe create upon basic formats data transfer JSON, HTML XML. learn subset, manipulate transform formats suitable data analysis tasks. data formats can accessed scraping website: programmatically accessing data website asking program download much data need frequently needed. integral part course focus perform ethical web scrapping, scale web scrapping download massive amounts data well program scraper access data frequent intervals.course also focus emerging technology websites share data: Application Programming Interfaces (API). touch upon basic formats data sharing APIs well make thousands data requests just seconds. Special emphasis made security ethical guidelines speaking APIs.big part course emphasize automation, way student create robust scalable data acquisition pipelines. step process, focus practical applications techniques exercise active participation students hands-challenges data acquisition. course make heavy use Github students need share homework well explore immense repository data acquisition technology already available open source software.goal course empower students right toolset ideas able create data acquisition programs, automate data extraction pipeline quickly transform data formats suitable analysis. course place contents light legal ethical consequences data acquisition can entail, always informing students best practices grabbing data internet.course assumes students familiar R programming language, transforming manipulating datasets well saving work Git Github. prior knowledge software development data acquisition techniques needed.","code":""},{"path":"index.html","id":"curriculum","chapter":"1 Welcome","heading":"1.1 Curriculum","text":"","code":""},{"path":"index.html","id":"todo-update-once-the-course-is-ready","chapter":"1 Welcome","heading":"1.2 TODO: Update once the course is ready","text":"introduction Web Scraping\nWeb Scraping?\nTypes Web Scraping\nData formats: XML HTML\nPractical access XML HTML\nAutomation Web Scraping programs\nSelenium JavaScript based scraping\nEthical issues Web Scraping\nPractical exercises\nWeb Scraping?Types Web ScrapingData formats: XML HTMLPractical access XML HTMLAutomation Web Scraping programsSelenium JavaScript based scrapingEthical issues Web ScrapingPractical exercisesData APIs\nAPI\nFundamentals API communication\nintroduction JSON format\nCreate API (share )\nREST architecture\nAPIs way share obtain data (kind)\nAutomation API requests\nTalking Databases\nAuthentication ethical access APIs\nPractical exercises\nAPIFundamentals API communicationAn introduction JSON formatCreate API (share )REST architectureAPIs way share obtain data (kind)Automation API requestsTalking DatabasesAuthentication ethical access APIsPractical exercisesAutomation Data Acquisition\nneed automation?\nAccessing servers\nTechnologies automating programs\nAutomating cron jobs\nLogging tasks\nPractical exercises\nneed automation?Accessing serversTechnologies automating programsAutomating cron jobsLogging tasksPractical exercises","code":""},{"path":"introduction-to-webscraping.html","id":"introduction-to-webscraping","chapter":"2 Introduction to Webscraping","heading":"2 Introduction to Webscraping","text":"Welcome world collecting data internet. Although \nTODOInstall scrapex explain problem decribed scrapex README web scraping tutorial doomed obsolete etc..\nWebscraping creative: clear way get want. Need come creative solutions.Install scrapex explain problem decribed scrapex README web scraping tutorial doomed obsolete etc..\nWebscraping creative: clear way get want. Need come creative solutions.Install packages used book.Install packages used book.TODO: Make website wider plots render nicelyYou need mad data cleaning skills string / regex intermediatte. Come general regex encompasses need know read book can’t figure , study . Talk string heavy lifter book.Highlight ’ll using xml2 package rvest package interchangeably. Explain rvest just wrapper around xml2 slightly high level. ’ll use along course.TODO: need write entire webscraping part can done without internet\nTODO: Add general script save plots folder chapter automatically (look R4DS initial scripts)","code":""},{"path":"a-primer-on-webscraping.html","id":"a-primer-on-webscraping","chapter":"3 A primer on Webscraping","heading":"3 A primer on Webscraping","text":"Webscraping subtle art ninja. need expert many things swiss army knife skills comes data cleaning data manipulation. String manipulations, data subsetting clever tricks rather exact solutions companion day day scraping needs. Throughout chapter ’ll give one--one tour expect webscraping wild. reason, book skip usual ‘start basics’ sections directly webscrape website see results efforts right away. Ready? Let’s get going .aim primer create plot like one:plot shows election results political parties Spain since 1978. data local computer ’ll need find online scrape . scrape specifically mean write little R script go website manually select data points tell . Wikipedia data throughout book work mostly local copies websites outlined # TODO introduction chapter.","code":""},{"path":"a-primer-on-webscraping.html","id":"getting-website-data-into-r","chapter":"3 A primer on Webscraping","heading":"3.1 Getting website data into R","text":"scrapex package function calledhistory_elections_spain_ex() points locally saved copy Wikipedia website persist time(original online link https://en.wikipedia.org/wiki/Elections_in_Spain). Let’s load packages take look website.website saved locally computer. can directly visualize browser like :bottom right can see plot ’d like generate. plots political parties since 1978 2020 elections. first step webscraping ‘read’ website R. can read_html function. pass website link (typical https://en.wikipedia.org string) since already website locally, just pass path local website:TODO: explain set user agent. Rewrite paragraph mix chapter.\n, set something called User-Agent. short, User-Agent . good practice identify person scraping website ’re causing trouble website, website can directly identify causing problems. can figure user agent paste string . addition, add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests.can’t understand much output read_html HTML code behind website long. read_html shows top level details. case, don’t need understand details first. Now website already R need figure actual data elections website. scroll , near end website ’ll see table like one:precisely data need. contains election results parties since 1978. rvest package (already loaded) handy function called html_table() automatically extracts tables website R. However, html_table() needs know website ’re working need pass html_website read previous step:html_table() reads tables ’s lot information all_tables (list 10 tables precise). won’t print entire R object ’s verbose encourage reader write all_tables R console explore tables scraped automatically. understand information ’re looking . carefully inspecting tables, figure table ’re looking slot 5:can see ’s table saw website :","code":"\nlibrary(scrapex)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nlink <- history_elections_spain_ex()\nlink## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/6361e4136f22ccc75aefb83dbfada352/scrapex/extdata/history_elections_spain//Elections_Spain.html\"\nbrowseURL(prep_browser(link))\nhtml_website <- link %>% read_html()\nhtml_website## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nall_tables <-\n  html_website %>%\n  html_table()\nelections_data <- all_tables[[5]]\nelections_data## # A tibble: 16 × 18\n##    Election  `UCD[a]`     PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##    <chr>     <chr>       <dbl>   <dbl> <chr>      <dbl> <dbl>    <dbl>    <dbl>\n##  1 Election  \"\"           NA      NA   \"\"          NA    NA       NA       NA  \n##  2 1977      \"34.4\"       29.3     8.3 \"9.3\"        2.8   1.7      0.8      0.1\n##  3 1979      \"34.8\"       30.4     6.1 \"10.8\"       1.7   1.6      0.7      0.3\n##  4 1982      \"6.8\"        48.1    26.4 \"4.0\"        3.7   1.9      0.7      0.2\n##  5 1986      \"Dissolved\"  44.1    26   \"4.6\"        5     1.5      0.4      0.1\n##  6 1989      \"Dissolved\"  39.6    25.8 \"9.1\"        5     1.2      0.4      0.2\n##  7 1993      \"Dissolved\"  38.8    34.8 \"9.6\"        4.9   1.2      0.8      0.5\n##  8 1996      \"Dissolved\"  37.6    38.8 \"10.5\"       4.6   1.3      0.7      0.9\n##  9 2000      \"Dissolved\"  34.2    44.5 \"5.4\"        4.2   1.5      0.8      1.3\n## 10 2004      \"Dissolved\"  42.6    37.7 \"5.0\"        3.2   1.6      2.5      0.8\n## 11 2008      \"Dissolved\"  43.9    39.9 \"3.8\"        3     1.2      1.2      0.8\n## 12 2011      \"Dissolved\"  28.8    44.6 \"6.9\"        4.2   1.3      1.1      0.8\n## 13 2015      \"Dissolved\"  22      28.7 \"3.7\"        2.2   1.2      2.4      0.3\n## 14 2016      \"Dissolved\"  22.6    33   \"[k]\"        2     1.2      2.6      0.2\n## 15 Apr. 2019 \"Dissolved\"  28.7    16.7 \"[l]\"        1.9   1.5      3.9      0.4\n## 16 Nov. 2019 \"Dissolved\"  28      20.8 \"[l]\"        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <chr>, `CDS[h]` <chr>, `CC[i]` <dbl>,\n## #   UPyD <chr>, Cs <chr>, Com. <chr>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>"},{"path":"a-primer-on-webscraping.html","id":"data-cleaning","chapter":"3 A primer on Webscraping","heading":"3.2 Data cleaning","text":"first column year column name political party. However, table problems need fix. Let’s outline things need fix:first row table emptyThe Election column character column row 15 16 contains months Apr. Nov..political party column values numbers (usually values representing foot notes [k] others Dissolved parties dissolved years). forces columns character columns fact want class numeric able visualize plot.Column names also footnote values names. probably remove .recalled first paragraphs primer, able webscraping, need data ninja. ’ll need become familiar basics regular expressions (fancy name manipulating strings) also cleaning data. ’ll use basics string manipulation, ’s fine feel completely lost. Just work hard little little ’ll learn tricks along way.first thing ’d want keep columns character. problems related columns:areWe can see different string values columns. ’d want replace non-numeric values NA’s. way convert columns numbers won’t loose information. can remove values? went columns wrote character values need remove:Now just need apply regular expression (regex now ) skills remove . Let’s explain want . regex world | stands . means want find words Banned Boycotted replace NA' write Banned|Boycotted. literally means Banned Boycotted. can take previous wrong_labels vector insert | wrong labels:effectively says: Dissolved [k] [l], …string can use function str_replace_all replace wrong labels NA’s. ’s ’d :Alright, don’t get stressed, ’ll explain line line. second line references data (elections_data, one ’ve working now). third line uses mutate_if function works applying function columns subset based criteria. Let’s break explanation even . can actually read code like :elections_dataFor columns character columns (mutate_if(character, ...))Apply transformation (mutate_if(character, ~ str_replace_all(...))example means character columns, function str_replace_all function applied. function replaces wrong_labels NA’s. can see right away:columns still characters, don’t wrong labels identified . problem Election column months Apr. Nov. won’t able convert numeric. can apply regex trick saying replace Apr. Nov. empty string. Let’s :Let’s check everything worked expected:go, don’t strings columns anymore. Let’s transform columns numeric remove first row empty:go, columns class numeric look nice tidy plotting. Last step need take remove footnote values party column names. ’ll need advanced regex patterns ’ll explain briefly (# TODO separate chapter ). pattern ’ll use [.+] means: detect character (.) repeated one times (+) enclosed within brackets ([] part). example, string Election won’t find match bracket values repeated one times. However, column name UCD[] pattern: contains two brackets [] value repeated one time ().’s also last trick need take account brackets ([]) special meaning regex world. signal regex want match brackets literally, need append backslash (\\\\). final regex pattern want match : \\\\[.+\\\\]. Let’s use rename columns:data set ready plot. ’s tidy clean. Let’s plot :. primer gave direct experience webscraping involves. involves read data website R, manually automatically finding chunks data want scrape, extracting cleaning enough able something useful . next chapter ’ll see depth work HTML XML data, ’ll need intuition find stuff within HTML XML document.","code":"\nelections_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]`    `IU[c]` `EHB[g]`    `CDS[h]`    UPyD        Cs    Com. \n##    <chr>     <chr>       <chr>   <chr>       <chr>       <chr>       <chr> <chr>\n##  1 Election  \"\"          \"\"      \"\"          \"\"          \"\"          \"\"    \"\"   \n##  2 1977      \"34.4\"      \"9.3\"   \"0.2\"       \"\"          \"\"          \"\"    \"\"   \n##  3 1979      \"34.8\"      \"10.8\"  \"1.0\"       \"\"          \"\"          \"\"    \"\"   \n##  4 1982      \"6.8\"       \"4.0\"   \"1.0\"       \"2.9\"       \"\"          \"\"    \"\"   \n##  5 1986      \"Dissolved\" \"4.6\"   \"1.1\"       \"9.2\"       \"\"          \"\"    \"\"   \n##  6 1989      \"Dissolved\" \"9.1\"   \"1.1\"       \"7.9\"       \"\"          \"\"    \"\"   \n##  7 1993      \"Dissolved\" \"9.6\"   \"0.9\"       \"1.8\"       \"\"          \"\"    \"\"   \n##  8 1996      \"Dissolved\" \"10.5\"  \"0.7\"       \"0.2\"       \"\"          \"\"    \"\"   \n##  9 2000      \"Dissolved\" \"5.4\"   \"Boycotted\" \"0.1\"       \"\"          \"\"    \"\"   \n## 10 2004      \"Dissolved\" \"5.0\"   \"Banned\"    \"0.1\"       \"\"          \"\"    \"\"   \n## 11 2008      \"Dissolved\" \"3.8\"   \"Banned\"    \"0.0\"       \"1.2\"       \"0.2\" \"\"   \n## 12 2011      \"Dissolved\" \"6.9\"   \"1.4\"       \"Dissolved\" \"4.7\"       \"Did… \"0.5\"\n## 13 2015      \"Dissolved\" \"3.7\"   \"0.9\"       \"Dissolved\" \"0.6\"       \"13.… \"[k]\"\n## 14 2016      \"Dissolved\" \"[k]\"   \"0.8\"       \"Dissolved\" \"0.2\"       \"13.… \"[k]\"\n## 15 Apr. 2019 \"Dissolved\" \"[l]\"   \"1.0\"       \"Dissolved\" \"Did not r… \"15.… \"0.7\"\n## 16 Nov. 2019 \"Dissolved\" \"[l]\"   \"1.2\"       \"Dissolved\" \"[m]\"       \"6.8\" \"[n]\"\nwrong_labels <- c(\n  \"Dissolved\",\n  \"[k]\",\n  \"[l]\",\n  \"[m]\",\n  \"n\",\n  \"Banned\",\n  \"Boycotted\",\n  \"Did not run\"\n)\nwrong_labels <- paste0(wrong_labels, collapse = \"|\")\nwrong_labels## [1] \"Dissolved|[k]|[l]|[m]|n|Banned|Boycotted|Did not run\"\nsemi_cleaned_data <-\n  elections_data %>%\n  mutate_if(\n    is.character,\n    ~ str_replace_all(string = .x, pattern = wrong_labels, replacement = NA_character_)\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>     <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>      \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977      \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979      \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982      \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986       <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989       <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993       <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996       <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000       <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004       <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008       <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011       <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015       <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016       <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 Apr. 2019  <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 Nov. 2019  <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate(\n    Election = str_replace_all(string = Election, pattern = \"Apr. |Nov. \", replacement = \"\")\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>    <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>     \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977     \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979     \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982     \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986      <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989      <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993      <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996      <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000      <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004      <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008      <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011      <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015      <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016      <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 2019      <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 2019      <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate_all(as.numeric) %>%\n  filter(!is.na(Election))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election `UCD[a]`  PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##       <dbl>    <dbl> <dbl>   <dbl>   <dbl>    <dbl> <dbl>    <dbl>    <dbl>\n##  1     1977     34.4  29.3     8.3     9.3      2.8   1.7      0.8      0.1\n##  2     1979     34.8  30.4     6.1    10.8      1.7   1.6      0.7      0.3\n##  3     1982      6.8  48.1    26.4     4        3.7   1.9      0.7      0.2\n##  4     1986     NA    44.1    26       4.6      5     1.5      0.4      0.1\n##  5     1989     NA    39.6    25.8     9.1      5     1.2      0.4      0.2\n##  6     1993     NA    38.8    34.8     9.6      4.9   1.2      0.8      0.5\n##  7     1996     NA    37.6    38.8    10.5      4.6   1.3      0.7      0.9\n##  8     2000     NA    34.2    44.5     5.4      4.2   1.5      0.8      1.3\n##  9     2004     NA    42.6    37.7     5        3.2   1.6      2.5      0.8\n## 10     2008     NA    43.9    39.9     3.8      3     1.2      1.2      0.8\n## 11     2011     NA    28.8    44.6     6.9      4.2   1.3      1.1      0.8\n## 12     2015     NA    22      28.7     3.7      2.2   1.2      2.4      0.3\n## 13     2016     NA    22.6    33      NA        2     1.2      2.6      0.2\n## 14     2019     NA    28.7    16.7    NA        1.9   1.5      3.9      0.4\n## 15     2019     NA    28      20.8    NA        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <dbl>, `CDS[h]` <dbl>, `CC[i]` <dbl>,\n## #   UPyD <dbl>, Cs <dbl>, Com. <dbl>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  rename_all(~ str_replace_all(.x, \"\\\\[.+\\\\]\", \"\"))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election   UCD  PSOE    PP    IU   CDC   PNV   ERC   BNG   EHB   CDS    CC\n##       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1     1977  34.4  29.3   8.3   9.3   2.8   1.7   0.8   0.1   0.2  NA    NA  \n##  2     1979  34.8  30.4   6.1  10.8   1.7   1.6   0.7   0.3   1    NA    NA  \n##  3     1982   6.8  48.1  26.4   4     3.7   1.9   0.7   0.2   1     2.9  NA  \n##  4     1986  NA    44.1  26     4.6   5     1.5   0.4   0.1   1.1   9.2   0.3\n##  5     1989  NA    39.6  25.8   9.1   5     1.2   0.4   0.2   1.1   7.9   0.3\n##  6     1993  NA    38.8  34.8   9.6   4.9   1.2   0.8   0.5   0.9   1.8   0.9\n##  7     1996  NA    37.6  38.8  10.5   4.6   1.3   0.7   0.9   0.7   0.2   0.9\n##  8     2000  NA    34.2  44.5   5.4   4.2   1.5   0.8   1.3  NA     0.1   1.1\n##  9     2004  NA    42.6  37.7   5     3.2   1.6   2.5   0.8  NA     0.1   0.9\n## 10     2008  NA    43.9  39.9   3.8   3     1.2   1.2   0.8  NA     0     0.7\n## 11     2011  NA    28.8  44.6   6.9   4.2   1.3   1.1   0.8   1.4  NA     0.6\n## 12     2015  NA    22    28.7   3.7   2.2   1.2   2.4   0.3   0.9  NA     0.3\n## 13     2016  NA    22.6  33    NA     2     1.2   2.6   0.2   0.8  NA     0.3\n## 14     2019  NA    28.7  16.7  NA     1.9   1.5   3.9   0.4   1    NA     0.5\n## 15     2019  NA    28    20.8  NA     2.2   1.6   3.6   0.5   1.2  NA     0.5\n## # … with 6 more variables: UPyD <dbl>, Cs <dbl>, Com. <dbl>, Pod. <dbl>,\n## #   Vox <dbl>, MP <dbl>\n# Pivot from wide to long to plot it in ggplot\ncleaned_data <-\n  semi_cleaned_data %>%\n  pivot_longer(-Election, names_to = \"parties\")\n\n# Plot it\ncleaned_data %>%\n  ggplot(aes(Election, value, color = parties)) +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  scale_color_viridis_d() +\n  theme_minimal()"},{"path":"a-primer-on-webscraping.html","id":"exercises","chapter":"3 A primer on Webscraping","heading":"3.3 Exercises","text":"Europe ageing problem mandatory retirement age constantly revised. scrapex package ’s copy “Retirement Europe” wikipedia website https://en.wikipedia.org/wiki/Retirement_in_Europe. can find local link function retirement_age_europe_ex(). Can inspect website, parse table replicate plot ? (Hint, might function str_sub stringr).parsing elections table, parsed tables Wikipedia table all_tables. Among tables, ’s one table documents years general elections, presidential elections, european elections, local elections, regional elections referendums Spain. Can extract numeric vector years general elections Spain? (Hint: might need str_split stringr functions resulting vector start 1810 end 2019).parsing elections table, parsed tables Wikipedia table all_tables. Among tables, ’s one table documents years general elections, presidential elections, european elections, local elections, regional elections referendums Spain. Can extract numeric vector years general elections Spain? (Hint: might need str_split stringr functions resulting vector start 1810 end 2019).Building previous code, can tell years local elections, european elections general elections overlapped?Building previous code, can tell years local elections, european elections general elections overlapped?","code":""},{"path":"data-formats-for-webscraping.html","id":"data-formats-for-webscraping","chapter":"4 Data Formats for Webscraping","heading":"4 Data Formats for Webscraping","text":"webscraping ’ll involve parsing either XML HTML. two formats much alike fact many examples ’ll notice almost indistinguishable. web ’ll find formal definitions languages ’s definition layman person: series tags formats website structured (HTML) store transfer data (XML). Still rather vague eh? Let’s go concrete examples.XML abbreviation Extensible Markup Language whereas HTML stands Hypertext Markup Language. might’ve guessed, ’re ‘Markup’ languages, share lot common. R can read formats xml2 package. Let’s load package getting started:","code":"\nlibrary(xml2)"},{"path":"data-formats-for-webscraping.html","id":"a-primer-on-xml-and-html","chapter":"4 Data Formats for Webscraping","heading":"4.1 A primer on XML and HTML","text":"Let’s begin simple example. define string look structure:XML HTML basic building blocks called tags. example, first tag structure shown <people>. tag matched <\/people> end string:pay close attention, ’ll see tag XML structure beginning (signaled <>) end (signaled <\/>). example, next tag <people> <jason> right tag <carol> end jason tag <\/jason>.Similarly, ’ll find <carol> tag also matched <\/carol> finishing tag.XML world, tags can whatever meaning attach (<people> <occupation>). However, HTML hundreds tags standard structuring websites. want stop second highlight XML HTML many differences, conceptual visible users. Throughout rest chapter ’ll focus think important ones context webscraping.Let’s compare visibly previous XML example HTML:One key differences tags (<div>, <head>, <title>, etc..) specific properties structure website shown someone browser. Anything inside <head> tag header website. Anything within <body> tag body website, . tags standard across HTML language predetermined behavior format website. Let’s look another HTML also result structured website. ’s code:rendered website:heading contains hyperlink (<> tag href property), bigger bold comparison “paragraph”. specific tags together attributes interpreted give outline text ’ll start notice, trademark value HTML language.contrast, XML tags meaning creator meant . <occupation> tag simply means inside tag occupation related content. ’s say XML transferring data (tags inherent behavior) HTML structuring website.Another difference XML HTML HTML tags don’t need closed, meaning don’t need <\\ > tag (example <br> adds space content website). hand, XML strict ’ll find tags equivalent closing tag.Now, might asking , since standard tags XML, many standard HTML tags ? Well, many remember ’re getting started. ’s short set:comprehensive list see . don’t learn every single tag webscraping (fact know handful) ’s helpful hint able locate specific parts website ’re interested webscraping. theory way, let’s get hands dirty manipulating formats R.R can read XML HTML formats read_xml read_html functions. Let’s read XML string fake example look general structure:can see structure tree-based, meaning tags <jason> <carol> nested within <people> tag. XML jargon, <people> root node, whereas <jason> <carol> child nodes <people>.detail, structure follows:root node <people>child nodes <jason> <carol>child node nodes <first_name>, <married>, <last_name> <occupation> nested within .Put another way, something nested within node, nested node child upper-level node. example, root node <people> can check children:","code":"\nxml_test <- \"<people>\n<jason>\n  <person type='fictional'>\n    <first_name>\n      <married>\n        Jason\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Bourne\n    <\/last_name>\n    <occupation>\n      Spy\n    <\/occupation>\n  <\/person>\n<\/jason>\n<carol>\n  <person type='real'>\n    <first_name>\n      <married>\n        Carol\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Kalp\n    <\/last_name>\n    <occupation>\n      Scientist\n    <\/occupation>\n  <\/person>\n<\/carol>\n<\/people>\n\"\n\ncat(xml_test)## <people>\n## <jason>\n##   <person type='fictional'>\n##     <first_name>\n##       <married>\n##         Jason\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Bourne\n##     <\/last_name>\n##     <occupation>\n##       Spy\n##     <\/occupation>\n##   <\/person>\n## <\/jason>\n## <carol>\n##   <person type='real'>\n##     <first_name>\n##       <married>\n##         Carol\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Kalp\n##     <\/last_name>\n##     <occupation>\n##       Scientist\n##     <\/occupation>\n##   <\/person>\n## <\/carol>\n## <\/people>\nhtml_test <- \"<html>\n  <head>\n    <title>Div Align Attribbute<\/title>\n  <\/head>\n  <body>\n    <div align='left'>\n      First text\n    <\/div>\n    <div align='right'>\n      Second text\n    <\/div>\n    <div align='center'>\n      Third text\n    <\/div>\n    <div align='justify'>\n      Fourth text\n    <\/div>\n  <\/body>\n<\/html>\n\"<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title<\/title>\n<\/head>\n<body>\n\n<h1> <a href=\"www.google.com\">This is a Heading <\/a> <\/h1>\n<br>\n<p>This is a paragraph.<\/p>\n\n<\/body>\n<\/html>\nxml_raw <- read_xml(xml_test)\nxml_structure(xml_raw)## <people>\n##   <jason>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n##   <carol>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n# xml_child returns only one child (specified in search)\n# Here, jason is the first child\nxml_child(xml_raw, search = 1)## {xml_node}\n## <jason>\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n# Here, carol is the second child\nxml_child(xml_raw, search = 2)## {xml_node}\n## <carol>\n## [1] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Use xml_children to extract **all** children\nchild_xml <- xml_children(xml_raw)\n\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ..."},{"path":"data-formats-for-webscraping.html","id":"tag-attributes","chapter":"4 Data Formats for Webscraping","heading":"4.2 Tag attributes","text":"Tags can also different attributes usually specified <fake_tag attribute='fake'> ended usual <\/fake_tag>. look XML structure example, ’ll notice <person> tag attribute called type. ’ll see real-world example, extracting attributes often aim scraping adventure. Using xml_attrs function can extract attributes match specific name:Wait, didn’t work? Well, look output child_xml, two nodes <jason> <carol>.tags attribute? , , something like <jason type='fake_tag'>. need look <person> tag within <jason> <carol> extract attribute <person>.sound familiar? <jason> <carol> associated <person> tag , making children. can just go one level running xml_children tags extract .Using xml_path function can even find ‘address’ nodes retrieve specific tags without write xml_children many times. example:‘address’ specific tags tree extract automatically? extract specific ‘addresses’ XML tree, main function ’ll use xml_find_all. function accepts XML tree ‘address’ string. can use simple strings, one given xml_path:expression asking node \"/people/jason/person\". return saying xml_raw %>% xml_child(search = 1). deeply nested trees, xml_find_all many times much cleaner calling xml_child recursively many times.However, cases ‘addresses’ used xml_find_all come separate language called XPath (fact, ‘address’ ’ve looking XPath). XPath complex language (regular expressions strings) ’ll cover chapter # TODO.Attributes flexible XML tag can many attributes see fit. example:attributes can also repeated many times, example, might generic <person> tag used person database:usual HTML, tags also standard specific meaning. Let’s read previous HTML example visualize structure:structure shows <div> tag align attribute. might’ve guessed, attribute aligns text. common attributes HTML tags easy understand. ’s list :<> tag contains text hyperlink (href). <img> tag (abbreviation image) contains src tag points image , together width height image. Finally, <p> tag short paragraph contains style attribute declaring styling properties text. just examples common HTML tags common attributes. case, ’ve outlined , ’s fine know tags; intuition behind enough locate specific parts website.finalize, whenever ’ll scraping something, ’ll want know whether ’s XML HTML based. manage receive .html .xml ’s just simple looking extension file. chance access source code file, can also look tags quickly see many standard HTML tags deduce actual format. Another solution just look root node ’ll see hint right away. ’ll see xml signaled right beginning:html:","code":"\n# Extract the attribute type from all nodes\nxml_attrs(child_xml, \"type\")## [[1]]\n## named character(0)\n## \n## [[2]]\n## named character(0)\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ...\n# We go down one level of children\nperson_nodes <- xml_children(child_xml)\n\n# <person> is now the main node, so we can extract attributes\nperson_nodes## {xml_nodeset (2)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n## [2] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Both type attributes\nxml_attrs(person_nodes, \"type\")## [[1]]\n##        type \n## \"fictional\" \n## \n## [[2]]\n##   type \n## \"real\"\n# Specific address of each person tag for the whole xml tree\n# only using the `person_nodes`\nxml_path(person_nodes)## [1] \"/people/jason/person\" \"/people/carol/person\"\n# You can use results from xml_path like directories\nxml_find_all(xml_raw, \"/people/jason/person\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...<name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<\/name><name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<person age=\"25\" status=\"single\" occupation=\"doctor\"> Jane Doe <\/person>\n<\/name>\nhtml_raw <- read_html(html_test)\nhtml_structure(html_raw)## <html>\n##   <head>\n##     <title>\n##       {text}\n##   <body>\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}<a href=\"https://www.w3schools.com\">Visit W3Schools<\/a>\n<img src=\"img_girl.jpg\" width=\"500\" height=\"600\">\n<p style=\"color:red;\">This is a red paragraph.<\/p><?xml version=\"1.0>\n<company>\n    <name> John Doe <\/name>\n    <email> johndoe@gmail.com <\/email>\n<\/address> <!DOCTYPE html>\n<html>\n<body>\n\n<h1>My First Heading<\/h1>\n<p>My first paragraph.<\/p>\n\n<\/body>\n<\/html>"},{"path":"data-formats-for-webscraping.html","id":"conclusion","chapter":"4 Data Formats for Webscraping","heading":"4.3 Conclusion","text":"frank . Although XML HTML important differences respect technology philosophy, difference pretty much us: webscraping needs, don’t care data formatted website (HTML) whether tags special meanings (XML), care formatted tags can extract information.Let’s brief recap summary. XML HTML code tag based, built opening closing tags like : <tag> <\/tag>. languages hierarchical, meaning nodes within nodes parent-child relationships. tags can attributes signal either behavior (HTML) data (XML) tags.read XML HTML R can use equivalent read_* functions xml2 package. navigate nodes data forms can use xml_child recursively find ’re looking . extract children given node, can use xml_children extract attributes given tag, can resort xml_attr. Finally, want extract text tag, xml_text extracts . functions give handy tool set explore small scale tree nodes XML HTML documents.finish, ’s important highlight HTML much widely used webscraping. webscraping websites HTML specifically designed show website formatted. However, difference indistinguishable webscraping R.","code":""},{"path":"data-formats-for-webscraping.html","id":"exercises-1","chapter":"4 Data Formats for Webscraping","heading":"4.4 Exercises","text":"Extract values align attributes html_raw (Hint, look function xml_children).Extract values align attributes html_raw (Hint, look function xml_children).Extract occupation Carol Kalp xml_rawExtract occupation Carol Kalp xml_rawExtract text <div> tags html_raw. result look specifically like :Extract text <div> tags html_raw. result look specifically like :Manually create XML string contains root node, two children nested within two grandchildren nested within child. first child second grandchild second child attribute called family set ‘1’. Read string find two attributes function xml_find_all xml_attrs.Manually create XML string contains root node, two children nested within two grandchildren nested within child. first child second grandchild second child attribute called family set ‘1’. Read string find two attributes function xml_find_all xml_attrs.output previous exercises either xml_nodeset html_document (can read top print results):output previous exercises either xml_nodeset html_document (can read top print results):Can extract text last name Carol scientist using R subsetting rules object? example some_object$people$person$... (Hint: xml2 function called as_list).","code":"[1] \"\\n      First text\\n    \"  \"\\n      Second text\\n    \"\n[3] \"\\n      Third text\\n    \"  \"\\n      Fourth text\\n    \"{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n    <div align=\"left\">\\n      First text\\n    <\/div>\\n    <div al ..."},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"what-you-need-to-know-about-regular-expressions","chapter":"5 What you need to know about regular expressions","heading":"5 What you need to know about regular expressions","text":"set write chapter hesitant . don’t consider expert regular expressions think able take beginner expert complicated topic. top , dozens excelent tutorial regular expressions topic justice able . said, web scraping without knowing regular expression techniques. data ’ll extract web need type massaging. times ’ll find data need string, joined stuff. ’ll need know tools clean string extract just need.reason, decided wanted write chapter focusing think key things know regex respect web scraping. means ’ll give incomplete picture regex can enough get running short amount time. reader interested topic (), refer reader chapter strings book R Data Science. chapter give thorough introduction topic general applications. Without adue, let’s begin.","code":""},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"matching-strings","chapter":"5 What you need to know about regular expressions","heading":"5.1 Matching strings","text":"Throughout chapter ’ll use exclusively stringr package. package cover string manipulation needs. Let’s load :Regular expressions (regex now ) way find patterns within strings. find pattern, can extract replace original string. Let’s take famous quote Jorge Luis Borges find “” letters quote:’s one simplest regex can think . ’s regular expression right : ’re matching word “eighteenth”. stringr two functions backbone using regex R: str_replace_all str_extract_all. first one replace string another string. example:webscraping ’ll need replace lot thingThe ., +, ^ $\\\\. characters [.","code":"\nlibrary(stringr)\nlibrary(scrapex)\nlibrary(rvest)\nborges <- \"I like hourglasses, maps, eighteenth century  typography.\"\nstr_view_all(borges, \"eighteenth\", match = TRUE)\nstr_replace_all(borges, \"eighteenth\", \"[DATE]\")## [1] \"I like hourglasses, maps, [DATE] century  typography.\"\nstr_view_all(borges, \".\")\nstr_view_all(borges, \"\\\\.\")\nstr_view_all(borges, \"maps, . century\")\nstr_view_all(borges, \"maps, .+ century\")\nres <- str_extract_all(borges, \"maps, .+ century\")[[1]]\nres %>% str_replace_all(\"maps, | century\", \"\")## [1] \"eighteenth\"\nstr_view_all(borges, \" \")\nstr_view_all(borges, \",\")\nstr_view_all(borges, \"a|e|i\")\nstr_view_all(str_to_lower(borges), \"i\")\nstr_view_all(borges, \"I[ ]\")\nretirement <- read_html(retirement_age_europe_ex()) %>% html_table() %>% .[[2]]\nstr_view_all(retirement$Men, \"[:digit:]\")\nstr_view_all(retirement$Men, \"\\\\s$\")\nstr_view_all(retirement$Men, \"\\\\s.+$\")\nas.numeric(str_replace_all(retirement$Men, \"\\\\s.+$\", \"\"))##  [1] 65.0 65.0 62.5 65.0 65.0 64.0 65.0 65.0 63.0 67.0 63.0 65.0 62.0 65.0 67.0\n## [16] 65.0 67.0 66.0 67.0 63.0 65.0 63.0 65.0 62.0 63.0 66.0 66.0 64.0 67.0 65.0\n## [31] 66.0 65.0 60.0 65.0 62.0 65.0 65.0 65.0 65.0 62.0 66.0\nstr_view_all(retirement$Notes, \"gradually.+[:digit:]{2,2} years\")\nregex <- gradually <- \"(gradually|reach).+[:digit:]{2,2} years\"\nstr_view_all(retirement$Notes, regex)"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"exercises-2","chapter":"5 What you need to know about regular expressions","heading":"5.2 Exercises","text":"","code":""},{"path":"what-you-need-to-know-about-xpath.html","id":"what-you-need-to-know-about-xpath","chapter":"6 What you need to know about XPath","heading":"6 What you need to know about XPath","text":"XPath (XML Path Language) language designed identify address one several tags within HTML XML document. address, XPath allows us extract data tags. example, take look XML :extract book ‘Hyperion Cantos’ Dan Simmons, simplest XPath can use /bookshelf/dansimmons/book. Let’s break understand better:first node bookshelf start /bookshelf.child bookshelf <dansimmons> XPath becomes /bookshelf/dansimmons/child <dansimmons> <book> just add XPath: /bookshelf/dansimmons/bookThat doesn’t look hard, right? problem web scraping needs, exact address, node node, generalizable.","code":"<bookshelf>\n  <dansimmons>\n    <book>\n      Hyperion Cantos\n    <\/book>\n  <\/dansimmons>\n<\/bookshelf>"},{"path":"what-you-need-to-know-about-xpath.html","id":"finding-tags-with-xpath","chapter":"6 What you need to know about XPath","heading":"6.1 Finding tags with XPath","text":"read previous XPath let’s load libraries ’ll need chapter.Let’s read XML R test initial XPath:works expected. Now remember told specific address generalizable? someone added authors tag bookshelf?can’t find using previous XPath expression. know , ’s instead use XPath /bookshelf/authors/dansimmons/book. someone (, developer behind website ’re trying scrape) continually changes XML? Can’t build general expression? XPath handy tricks can use . example, ’s one thing know book Hyperion Cantos: written Dan Simmons. Instead, can extract <dansimmons> tag directly //dansimmons. return <dansimmons> tags entire XML document. However, since know ’s one <dansimmons> tag, know ’ll grabbing one ’re :// handy, means: search entire document bring back <dansimmons> tags. doesn’t matter depth <dansimmons> tag, three twenty times deep, // return occurrences tag. Let’s extend example include another Dan Simmons book release date:Can predict XPath return running ? Let’s find :returns <dansimmons> tag, expected. Within <dansimmons> two <book> tags can’t see result two tags directly output. want extract names books. can reuse idea using / book tag. know book direct child dansimmons:go, get two book nodes. book direct child <dansimmons>, / wouldn’t work. example, instead book searched release_year (new tag added well), return empty node:’s expected: dansimmons doesn’t direct child called release_year (grandchild <release_year>. However, know // search tags entire XML tree can instead request release_year tags inside dansimmons:point, might useful know xml_path can return literal, direct address given node. Exactly like first XPath chapter:concrete cases, using literal path one might much quicker coming single XPath makes general. ’re trying scrape something one-thing, manually navigating HTML/XML tree (functions xml_child directly web developer tools browser) copying ’s exact XPath location might better choice.XPath also allows hand pick nodes position. Within dansimmons tag two book tags. XPath expression look like subset 2nd <book> tag dansimmons? can tell XPath position tag want using [number], number replaced position:can supply position node. expected, return empty position doesn’t exist:However, throughout examples specific supplying exact address child node respect ’s parent. //dansimmons return dansimmons tags won’t able see children. need know specific book tags children dansimmons, . XPath introduces * wildcard pattern return children current parent tag. example:result dansimmons tag ’s children, regardless whether <book> tags tag. strategy useful ’re unsure nodes certain parent want extract : fact generalizable can extract children tag pick one ’re string manipulation.Similarly, * can used fill tag don’t know name . know author <book> tags don’t know name authors. extract book tags like :words, XPath saying: extract book tags three tags , doesn’t matter tags . ’ll see later chapter, quite nice trick complex HTML/XML structures.Let’s recap far:/ links two tags direct parent-child relationship// finds tags HTML/XML tree regardless depthUse [number] subset position node. example: //[8] return 8th <> tag.* wildcard allows signal nodes without specifying nodes.rules can take long way building XPath expressions real flexibility comes ’re available filter attributes given node.","code":"\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(scrapex)\nraw_xml <- \"\n<bookshelf>\n  <dansimmons>\n    <book>\n      Hyperion Cantos\n    <\/book>\n  <\/dansimmons>\n<\/bookshelf>\"\n\nbook_xml <- read_xml(raw_xml)\ndirect_address <- \"/bookshelf/dansimmons/book\"\n\nbook_xml %>%\n  xml_find_all(direct_address)## {xml_nodeset (1)}\n## [1] <book>\\n      Hyperion Cantos\\n    <\/book>\n# Note the new `<authors>` tag, a child of `<bookshelf>`.\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book>\n        Hyperion Cantos\n      <\/book>\n    <\/dansimmons>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\n\nbook_xml %>%\n  xml_find_all(direct_address)## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons\")## {xml_nodeset (1)}\n## [1] <dansimmons>\\n  <book>\\n        Hyperion Cantos\\n      <\/book>\\n<\/dansimm ...\n# Note the new `<release_year>` tag below the second (also new) `<book>` tag\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book>\n        Hyperion Cantos\n      <\/book>\n      <book>\n        <release_year>\n         1996\n        <\/release_year>\n        Endymion\n      <\/book>\n    <\/dansimmons>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\nbook_xml %>%\n  xml_find_all(\"//dansimmons\")## {xml_nodeset (1)}\n## [1] <dansimmons>\\n  <book>\\n        Hyperion Cantos\\n      <\/book>\\n  <book>< ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/release_year\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons//release_year\")## {xml_nodeset (1)}\n## [1] <release_year>\\n         1996\\n        <\/release_year>\nbook_xml %>%\n  xml_find_all(\"//dansimmons//release_year\") %>%\n  xml_path()## [1] \"/bookshelf/authors/dansimmons/book[2]/release_year\"\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book[2]\")## {xml_nodeset (1)}\n## [1] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book[8]\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons/*\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"/*/*/*/book\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ..."},{"path":"what-you-need-to-know-about-xpath.html","id":"filter-by-attributes","chapter":"6 What you need to know about XPath","heading":"6.2 Filter by attributes","text":"parsing complicated websites, ’ll need additional flexibility parse HTML/XML. XPath great property allows pick tags specific attributes. Let’s update XML example include new author tag <stephenking>, one ’s books additional attributes books:power XPath comes can filter tags attributes. Perhaps ’d like extract book tags price, regardless author. catch books certain topic. Whenever want tags match specific attribute can add two brackets end tag match attribute ’re . Say wanted know Dan Simmons book price, XPath look like?new XPath saying: find <book> tags attribute price set yes descendants (necessarily direct child, //) <dansimmons> tag. Quite interesting eh? approach allows us much flexible language parsing HTML/XML documents. Everything inside [] serves add additional filters/criteria matches XPath. help keyword, can alter previous XPath get books price topic horror:grab books price attribute (’s different price set yes ):find books price:correct attribute price set ‘’. can also use keyword match certain properties:XPath goodies perform basic filtering (, , =, !=) also additional functions useful filtering. common ones include:contains()starts-()text()()count()use ? always use functions within context filtering (everything used inside []). can reach level fine-grained filtering can save hours searching source code XML/HTML document. go cases functions useful, let’s load new example scrapex package.rest chapter exercises ’ll working main page newspaper “El País”. “El País” international daily newspaper. among circulated newspapers Spain rich website ’ll scraping. can load function elpais_newspaper_ex():Let’s look website web browser:website news organized along left, right center website. scroll ’ll see dozens news snippets scattered throughout website. news organized sections ‘Culture’, ‘Sports’ ‘Business’.Let’s say ’re interested figuring links sections newspaper able scrape news separately area. avoid complexity, ’ll start first grabbing ‘Science’ section link first step. section want explore :left can see section ‘Science, Tech & Health’ articles belong section. words ‘Science, Tech & Health’ bold contain hyperlink main page science articles. ’s want access. right, ’ll see opened web developer tools browser. clicking manually ‘Science, Tech & Health’ right, source code highlights blue hyperlink .concretely, can see source code want <> tag nested within <section> tag (two tags <> tag). <> tag attribute href contains link:Ok, information can creative build XPath expressions says: find <> tags href attribute containing word ‘Science’ also inherits <section> tag:Hmm, XPath seems right output returns many tags. expecting one link general science section (something like https://english.elpais.com/science-tech/). know <> tag <section> tag two additional <header> <div> tags:two exact tags might sections can try specifying two wild cards tags <section> <>. example:’s one looking . Let’s explain XPath expression://section means search sections throughout HTML tree//section/*/* means search two direct children <section> (regardless tags )[contains(@href, 'science')] finds <> tags @href attribute contains text ‘science’.final expression says: finds <> tags @href attribute contains text ‘science’ descendant <section> tag two tags .might become evident, function contains searches text attribute. matches supplied text contained attribute want. Kinda like regular expressions. However can also use function text() points actual text tag. rewrite previous XPath make even precise:Instead, XPath grabs <> tags contain text ‘Science, Tech & Health’. fact, make even shorter. Since probably <> tag contains text ‘Science, Tech & Health’, can remove wildcards * tags:final expression asks <> tags descendants <section> tags contains specific science text. functions (text, contains) make filtering much precise easy understand. functions start-() perform job contains() matching whether attribute/text starts provided text.function () also useful filtering. negates everything inside filter expression. previous example, using () return sections ones containing text ‘Science, Tech & Health’:see links sections economy--business international. Finally, function count() allows use conditionals based counting something. One interesting question many sections three articles. might interested scraping newspaper sites measure whether bias amount news published certain sections. XPath directly tackles might like :looking result see attribute data-dtm-region contains information name section (see word culture third node). Let’s extract :Five sections, mostly entertainment related except first one front page (‘aperatura’ something like ‘opening’). Although XPath short, contains things might now. Let’s explain ://section find section tags XML document[count(.//article])] counts articles articles current tag. ’s write .//article dot (.) signals search articles current position. instead wrote //article search articles entire HTML tree.[count(.//article])]>3 counts sections three articlesThese XPath filtering rules can take long way building precise expressions. chapter covers somewhat beginner/intermediate introduction XPath one can take far. Trust tell XPath rules can fulfill vast percentage webscraping needs, start easy. start building scraping programs supposed run frequent intervals work bigger team developers dependent scraped data, might need careful build XPath expressions avoid breaking scraper frequently. However, fairly good start achieving scraping needs beginner.","code":"\n# Note the new <stephenking> tag with it's book 'The Stand' and all <book> tags have some attributes\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book price='yes' topic='scifi'>\n        Hyperion Cantos\n      <\/book>\n      <book topic='scifi'>\n        <release_year>\n         1996\n        <\/release_year>\n        Endymion\n      <\/book>\n    <\/dansimmons>\n    <stephenking>\n    <book price='yes' topic='horror'>\n     The Stand\n    <\/book>\n    <\/stephenking>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\nbook_xml %>%\n  xml_find_all(\"//dansimmons//book[@price='yes']\") %>%\n  xml_text()## [1] \"\\n        Hyperion Cantos\\n      \"\nbook_xml %>%\n  xml_find_all(\"//book[@price='yes' and @topic='horror']\") %>%\n  xml_text()## [1] \"\\n     The Stand\\n    \"\nbook_xml %>%\n  xml_find_all(\"//book[@price]\")## {xml_nodeset (2)}\n## [1] <book price=\"yes\" topic=\"scifi\">\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book price=\"yes\" topic=\"horror\">\\n     The Stand\\n    <\/book>\nbook_xml %>%\n  xml_find_all(\"//book[@price!='yes']\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//book[@price='yes' or @topic='scifi']\") %>%\n  xml_text()## [1] \"\\n        Hyperion Cantos\\n      \"                  \n## [2] \"\\n         1996\\n        \\n        Endymion\\n      \"\n## [3] \"\\n     The Stand\\n    \"\nnewspaper_link <- elpais_newspaper_ex()\nnewspaper <- read_html(newspaper_link)\nbrowseURL(prep_browser(newspaper_link))\nnewspaper %>%\n  xml_find_all(\"//section//a[contains(@href, 'science')]\")## {xml_nodeset (20)}\n##  [1] <a href=\"https://english.elpais.com/science-tech/2022-10-07/is-climate-c ...\n##  [2] <a href=\"https://english.elpais.com/science-tech/2022-10-07/worlds-top-m ...\n##  [3] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Sci ...\n##  [4] <a href=\"https://english.elpais.com/science-tech/2022-10-07/a-new-drug-t ...\n##  [5] <a href=\"https://english.elpais.com/science-tech/2022-10-07/a-new-drug-t ...\n##  [6] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n##  [7] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n##  [8] <a href=\"https://english.elpais.com/science-tech/2022-10-06/ophiuchus-th ...\n##  [9] <a href=\"https://english.elpais.com/science-tech/2022-10-06/ophiuchus-th ...\n## [10] <a href=\"https://english.elpais.com/science-tech/2022-09-17/one-girls-ge ...\n## [11] <a href=\"https://english.elpais.com/science-tech/2022-09-17/one-girls-ge ...\n## [12] <a href=\"https://english.elpais.com/science-tech/2022-10-06/global-warmi ...\n## [13] <a href=\"https://english.elpais.com/science-tech/2022-10-06/global-warmi ...\n## [14] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Sci ...\n## [15] <a href=\"https://english.elpais.com/science-tech/2022-10-06/how-can-a-sm ...\n## [16] <a href=\"https://english.elpais.com/science-tech/2022-10-06/how-can-a-sm ...\n## [17] <a href=\"https://english.elpais.com/science-tech/2022-10-04/the-orbit-of ...\n## [18] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n## [19] <a href=\"https://english.elpais.com/science-tech/2022-10-01/rare-diamond ...\n## [20] <a href=\"https://english.elpais.com/science-tech/2022-09-30/carole-hoove ...\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(@href, 'science')]\")## {xml_nodeset (2)}\n## [1] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Scie ...\n## [2] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Scie ...\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(text(), 'Science, Tech & Health')]\") %>%\n  xml_attr(\"href\")## [1] \"https://english.elpais.com/science-tech/\"\nnewspaper %>%\n  xml_find_all(\"//section//a[contains(text(), 'Science, Tech & Health')]\") %>%\n  xml_attr(\"href\")## [1] \"https://english.elpais.com/science-tech/\"\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[not(contains(text(), 'Science, Tech & Health'))]\") %>%\n  xml_attr(\"href\")##  [1] \"https://english.elpais.com/economy-and-business/\"                                                                                               \n##  [2] \"https://english.elpais.com/opinion/the-global-observer/\"                                                                                        \n##  [3] \"https://english.elpais.com/international/\"                                                                                                      \n##  [4] \"https://english.elpais.com/culture/\"                                                                                                            \n##  [5] \"https://english.elpais.com/science-tech/\"                                                                                                       \n##  [6] \"https://english.elpais.com/society/\"                                                                                                            \n##  [7] \"https://elpais.com/archivo/#?prm=hemeroteca_pie_ep\"                                                                                             \n##  [8] \"https://elpais.com/archivo/#?prm=hemeroteca_pie_ep\"                                                                                             \n##  [9] \"https://play.google.com/store/apps/details?id=com.elpais.elpais&hl=es&gl=US\"                                                                    \n## [10] \"https://apps.apple.com/es/app/el-pa%C3%ADs/id301049096\"                                                                                         \n## [11] \"https://elpais.com/suscripciones/#/campaign#?prod=SUSDIGCRART&o=susdig_camp&prm=pw_suscrip_cta_pie_eng&backURL=https%3A%2F%2Fenglish.elpais.com\"\nnewspaper %>%\n  xml_find_all(\"//section[count(.//article)>3]\")## {xml_nodeset (5)}\n## [1] <section class=\"_g _g-md _g-o b b-d\" data-dtm-region=\"portada_apertura\">< ...\n## [2] <section class=\"b b-t b-t-ad _g-o \" data-dtm-region=\"portada_tematicos_sc ...\n## [3] <section class=\"b b-m _g-o\" data-dtm-region=\"portada_arrevistada_culture\" ...\n## [4] <section class=\"b b-t b-t-df b-t-1 _g-o \" data-dtm-region=\"portada_temati ...\n## [5] <section class=\"b b-t b-t-df b-t-1 _g-o \" data-dtm-region=\"portada_temati ...\nnewspaper %>%\n  xml_find_all(\"//section[count(.//article)>3]\") %>%\n  xml_attr(\"data-dtm-region\")## [1] \"portada_apertura\"                          \n## [2] \"portada_tematicos_science,-tech-&-health\"  \n## [3] \"portada_arrevistada_culture\"               \n## [4] \"portada_tematicos_celebrities,-movies-&-tv\"\n## [5] \"portada_tematicos_our-selection\""},{"path":"what-you-need-to-know-about-xpath.html","id":"xpath-cookbook","chapter":"6 What you need to know about XPath","heading":"6.3 XPath cookbook","text":"’ve written set cookbook commands might find useful webscraping using XPath:","code":"\n# Find all sections\nnewspaper %>%\n  xml_find_all(\"//section\")\n\n# Return all divs below all sections\nnewspaper %>%\n  xml_find_all(\"//section//div\")\n\n# Return all sections which a div as a child\nnewspaper %>%\n  xml_find_all(\"//section/div\")\n\n# Return the child (any, because of *) of all sections\nnewspaper %>%\n  xml_find_all(\"//section/*\")\n\n# Return all a tags of all section tags which have two nodes in between\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a\")\n\n# Return all a tags below all section tags without a class attribute\nnewspaper %>%\n  xml_find_all(\"//section//a[not(@class)]\")\n\n# Return all a tags below all section tags that contain a class attribute\nnewspaper %>%\n  xml_find_all(\"//section//a[@class]\")\n\n# Return all a tags of all section tags which have two nodes in between\n# and contain some text in the a tag.\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(text(), 'Science')]\")\n\n# Return all span tags in the document with a specific class\nnewspaper %>%\n  xml_find_all(\"//span[@class='c_a_l']\")\n\n# Return all span tags in the document that don't have a specific class\nnewspaper %>%\n  xml_find_all(\"//span[@class!='c_a_l']\")\n\n# Return all a tags where an attribute starts with something\nnewspaper %>%\n  xml_find_all(\"//a[starts-with(@href, 'https://')]\")\n\n# Return all a tags where an attribute contains some text\nnewspaper %>%\n  xml_find_all(\"//a[contains(@href, 'science-tech')]\")\n\n# Return all section tags which have tag *descendants (because of the .//)* that have a class attribute\nnewspaper %>%\n  xml_find_all(\"//section[.//a[@class]]\")\n\n# Return all section tags which have <td> children\nnewspaper %>%\n  xml_find_all(\"//section[td]\")\n\n# Return the first occurrence of a section tag\nnewspaper %>%\n  xml_find_all(\"(//section)[1]\")\n\n# Return the last occurrence of a section tag\nnewspaper %>%\n  xml_find_all(\"(//section)[last()]\")"},{"path":"what-you-need-to-know-about-xpath.html","id":"conclusion-1","chapter":"6 What you need to know about XPath","heading":"6.4 Conclusion","text":"XPath rich language 20 years development. ’ve covered basics well intermediate parts language ’s much learned. encourage look examples online check additional resources. leave best resources worked :XPath CheetsheetExtensive XPath CheetsheetXPath tutorial","code":""},{"path":"what-you-need-to-know-about-xpath.html","id":"exercises-3","chapter":"6 What you need to know about XPath","heading":"6.5 Exercises","text":"many jpg png images website? (Hint: look source code figure tag attribute contains links images).many jpg png images website? (Hint: look source code figure tag attribute contains links images).many articles entire website?many articles entire website?headlines (headlines mean bold text article begins ), many contain word ‘climate’?headlines (headlines mean bold text article begins ), many contain word ‘climate’?city reporters?city reporters?headline article words description? (Hint: remember .// searcher tags current tag. // search tags document, regardless whether ’s current selected node) text ’ll want measure amount letters bold headline news article:headline article words description? (Hint: remember .// searcher tags current tag. // search tags document, regardless whether ’s current selected node) text ’ll want measure amount letters bold headline news article:","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"case-study-scraping-spanish-school-locations-from-the-web","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7 Case study: scraping Spanish school locations from the web","text":"chapter ’ll scraping location sample schools Spain. case study involves using XPath find specific chunks code contain coordinates schools Spain, well inspecting source code website depth. final result chapter generate plot like one without previous knowledge schools :usual, website saved locally scrapex package changes made website don’t break code future. Althought links ’ll working hosted locally machine, HTML website similar one hosted online website (exception images/icons deleted purpose make package lightweight). said, local website fairly good representation ’ll find real website internet. website Spanish ’ll make sure point specifically type information ’re looking . Non-spanish speakers able work chapter without problem.begin, let’s load package ’ll use chapter:","code":"\nlibrary(xml2)\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(scrapex)"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"building-a-scraper-for-one-school","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.1 Building a scraper for one school","text":"Visualizing school locations can useful many things matching population density children across different regions school locations mapping patterns inequality geographical locations. website www.buscocolegio.com contains database schools Spain, containing plethora information school, together coordinates school located. function spanish_schools_ex() contains local links school.Let’s look example one school.Let’s look rendered website code :website shows standard details school “CEIP SANCHIS GUARNER”. can see ’s contact email contact details. Additionally, can see ’s location right. want extract precisely data visualize locations. can access coordinates?Let’s read website R start narrowing obtain coordinates:point, shouldn’t worry exploring actual website R. ’s always better idea look code browser. idea data ’re looking , can come back raw HTML search specific tags.Web scraping strategies specific website ’re . get familiar website ’re interested able match perfectly information ’re looking . many cases, scraping two websites require vastly different strategies.first thing want start looking source code maps section right can start looking hints coordinates somewhere code. popping web developer’s tools. browsers support tool can open press CTRL + SHIFT + c time (Firefox Chrome support hotkey). can also open searching settings menu browser looking ‘Web Developer Tools’. window right popped full code ’re right track:can search source code website. place mouse pointer lines code right-window, ’ll see sections website highlighted blue. indicates parts code refer parts website. never search complete source code find want rather approximate search typing text ’re looking search bar top right window.first intuition search text just Google Maps:Searching text best approximation ’ll just within maps section code can start looking coordinates instead entire website. Let’s search search bar developer tools:click enter, ’ll automatically directed tag information want.point, started browsing tags part code. surprise, actually landed specifically coordinates . can see latitude longitude schools found attributed called href <> tag:Can see latitude longitude fields text highlighted blue? ’s hidden -words. precisely type information ’re . href attribute <> tag contains coordinates.Extracting <> tags website yield hundreds matches <> common tag HTML. Refining search <> tags href attribute also yield hundreds matches href standard attribute attach links within websites. need narrow search within website.One strategy find ‘father’ ‘grandfather’ node particular <> tag match . strategy look ascendant tag particular property unique enough narrow search. example, father <> tag <p> tag, return dozens results. looking structure small HTML snippet right-window, see ‘father’ <> tag <p class=\"d-flex align-items-baseline g-mt-5'> particularly long attribute named class. seems like good candidate use:<p> tag father <> tag <> tag want extract coordinates :’s important intimidated tag names long attributes. also don’t know attributes mean. know ‘father’ <> tag ’m interested . using XPath skills, let’s search <p> tag class set 'd-flex align-items-baseline g-mt-5' see get one match. mean can identify tag precisely:one match, good news. means can uniquely identify particular <p> tag. Let’s refine search say: Find <> tags children specific <p> tag. means ’ll add \"//\" previous expression. Since one <p> tag class, ’re interested checking whether one <> tag <p> tag.go! can see specific href contains latitude longitude data ’re interested . extract href attribute? xml_attr standard function extract attributes tags:","code":"\nschool_links <- spanish_schools_ex()\n\n# Keep only the HTML file of one particular school.\nschool_url <- school_links[13]\n\nschool_url## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/6361e4136f22ccc75aefb83dbfada352/scrapex/extdata/spanish_schools_ex/school_3006839.html\"\nbrowseURL(prep_browser(school_url))\nschool_raw <- read_html(school_url) %>% xml_child()\nschool_raw## {html_node}\n## <head>\n##  [1] <title>Aquí encontrarás toda la información necesaria sobre CEIP SANCHIS ...\n##  [2] <meta charset=\"utf-8\">\\n\n##  [3] <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shri ...\n##  [4] <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n\n##  [5] <meta name=\"author\" content=\"BuscoColegio\">\\n\n##  [6] <meta name=\"description\" content=\"Encuentra toda la información necesari ...\n##  [7] <meta name=\"keywords\" content=\"opiniones SANCHIS GUARNER, contacto SANCH ...\n##  [8] <link rel=\"shortcut icon\" href=\"/favicon.ico\">\\n\n##  [9] <link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Roboto+Sl ...\n## [10] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [11] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-awesome/css/font-awesom ...\n## [12] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line/css/simple-line-ic ...\n## [13] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line-pro/style.css\">\\n\n## [14] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-hs/style.css\">\\n\n## [15] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [16] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [17] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [18] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [19] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [20] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## ...\n# Search for all <p> tags with that class in the document\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']\")## {xml_nodeset (1)}\n## [1] <p class=\"d-flex align-items-baseline g-mt-5\">\\r\\n\\t                    < ...\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\")## {xml_nodeset (1)}\n## [1] <a href=\"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274 ...\nlocation_str <-\n  school_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n  xml_attr(attr = \"href\")\n\nlocation_str## [1] \"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274492&colegio.longitud=0.0221681\""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"data-cleaning-1","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.1.1 Data cleaning","text":"Ok, now need regular expression skills get latitude longitude. string coordinates see first coordinate appears first =. Moreover, aside coordinates, ’s words like colegio (just spanish school) longitud middle two coordinates. mind, can write regex:\"=.+$\" captures = followed character (.) repeated 1 times (+) end string ($). layman terms: extract text = end string.Let’s apply :result exactly wanted. Now need replaced everything coordinates. regex look like:\"=|colegio\\\\.longitud\" matches = colegio.longitud (remember | stands regex). . preceded two \\\\? saw previous regex used, . regular expressions means character. signal want . matched literally write like \\\\.. layman terms: match either = colegio.longitud. Let’s apply :go. replaced everything coordinates except & . didn’t replace ? ’ll split two coordinates & separate:Alright, ’s end data cleaning efforts. managed locate specific HTML node contained school coordinates extracted using string manipulations. Now got information one single school, let’s turn function can pass school’s link get coordinates backTODO reference user agent chapter","code":"\nlocation <-\n  location_str %>%\n  str_extract_all(\"=.+$\")\n\nlocation## [[1]]\n## [1] \"=38.8274492&colegio.longitud=0.0221681\"\nlocation <-\n  location %>%\n  str_replace_all(\"=|colegio\\\\.longitud\", \"\")\n\nlocation## [1] \"38.8274492&0.0221681\"\nlocation <-\n  location %>%\n  str_split(\"&\") %>%\n  .[[1]]\n\nlocation## [1] \"38.8274492\" \"0.0221681\""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"scaling-the-scraper-to-all-schools","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.2 Scaling the scraper to all schools","text":"moving code scrape one single case many cases, ’ll want move code function make compact, organized easy read. Let’s move code function also make sure set user agent well add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests:can see worked single school, can try schools. thing left extract many schools. shown earlier, scrapex contains list 27 school links can automatically scrape. Let’s loop , get information coordinates collapse data frame.now locations schools, let’s plot :’s ! managed extract coordinates schools, clean plot .","code":"\n# This sets your `User-Agent` globally so that all requests are\n# identified with this `User-Agent`\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:105.0) Gecko/20100101 Firefox/105.0\")\n)\n\n# Collapse all of the code from above into one function called\n# school grabber\n\nschool_grabber <- function(school_url) {\n  # We add a time sleep of 5 seconds to avoid\n  # sending too many quick requests to the website\n  Sys.sleep(5)\n\n  school_raw <- read_html(school_url) %>% xml_child()\n\n  location_str <-\n    school_raw %>%\n    xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n    xml_attr(attr = \"href\")\n\n  location <-\n    location_str %>%\n    str_extract_all(\"=.+$\") %>%\n    str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n    str_split(\"&\") %>%\n    .[[1]]\n\n  # Turn into a data frame\n  data.frame(\n    latitude = location[1],\n    longitude = location[2],\n    stringsAsFactors = FALSE\n  )\n}\n\nschool_grabber(school_url)##     latitude longitude\n## 1 38.8274492 0.0221681\ncoordinates <- map_dfr(school_links, school_grabber)\ncoordinates##    latitude  longitude\n## 1  42.72779 -8.6567935\n## 2  43.24439 -8.8921645\n## 3  38.95592 -1.2255769\n## 4  39.18657 -1.6225903\n## 5  40.38245 -3.6410388\n## 6  40.22929 -3.1106322\n## 7  40.43860 -3.6970366\n## 8  40.33514 -3.5155669\n## 9  40.50546 -3.3738441\n## 10 40.63826 -3.4537107\n## 11 40.38543 -3.6639500\n## 12 37.76485 -1.5030467\n## 13 38.82745  0.0221681\n## 14 40.99434 -5.6224391\n## 15 40.99434 -5.6224391\n## 16 40.56037 -5.6703725\n## 17 40.99434 -5.6224391\n## 18 40.99434 -5.6224391\n## 19 41.13593  0.9901905\n## 20 41.26155  1.1670507\n## 21 41.22851  0.5461471\n## 22 41.14580  0.8199749\n## 23 41.18341  0.5680564\n## 24 42.07820  1.8203155\n## 25 42.25245  1.8621546\n## 26 41.73767  1.8383666\n## 27 41.62345  2.0013628\ncoordinates <- mutate_all(coordinates, as.numeric)\n\nsp_sf <-\n  ne_countries(scale = \"large\", country = \"Spain\", returnclass = \"sf\") %>%\n  st_transform(crs = 4326)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = coordinates, aes(x = longitude, y = latitude)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"scraping-publicprivate-school-information","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.3 Scraping public/private school information","text":"Suppose working consultant ministry economy, task understanding whether public/private schools correlated income inequality neighborhood level. ’s many public sources nowadays can tell average income given neighborhood can find data already cleaned ready use us internet. However, ’s difficult find list national schools country together information whether ’re public private.saw www.buscocolegio.com, school website also information whether school public private. ’s exactly :“Centro Público” means Public Center. Ig managed build scraper extract information know school either public private. Let’s open web developer tools look specifically tag highlighted clicking “Centro Público”:public/private information ’re interested within <strong> tag. tag HTML means text bold (althought don’t care tag , ’s handy know). Searching simply <strong> tag probably yield dozens matches since <strong> generic. need find unique tag identifies entire box. point, started looking father tags <strong> found 1st ascendant <strong> tag:<div> tag class 'col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25' seems unique enough give try. Let’s try:Ok, makes sense. returns 10 nodes <div> present details bigger details box:One strategy extract <strong> tags within tag extract text. find “Centro Público” text , can use regex extract :can see type school right slot 7 vector. Let’s finalize detecting string pattern extracting :go. scaffold code build function extract schools. Let’s move code function accepts one school url loop schools:info hand can merge coordinates school visualize public/private schools location:","code":"\nschool_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")## {xml_nodeset (10)}\n##  [1] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [2] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [3] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [4] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [5] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [6] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [7] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [8] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [9] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n## [10] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\ntext_boxes <-\n  school_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']//strong\") %>%\n  xml_text()\n\ntext_boxes##  [1] \"616522706\"                                                         \n##  [2] \"966428835\"                                                         \n##  [3] \"03006839@edu.gva.es\"                                               \n##  [4] \"-\"                                                                 \n##  [5] \"966428836\"                                                         \n##  [6] \"3006839\"                                                           \n##  [7] \"Centro Público\"                                                    \n##  [8] \"Colegio de Educación Infantil y Primaria\"                          \n##  [9] \"GENERALITAT VALENCIANA\"                                            \n## [10] \"Vuelta al cole, inicio y fin de clases, vacaciones y días festivos\"\nsingle_public_private <-\n  text_boxes %>%\n  str_detect(\"Centro\") %>%\n  text_boxes[.]\n\nsingle_public_private## [1] \"Centro Público\"\ngrab_public_private_school <- function(school_link) {\n  Sys.sleep(5)\n  school <- read_html(school_link)\n  text_boxes <-\n    school %>%\n    xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']//strong\") %>%\n    xml_text()\n\n  single_public_private <-\n    text_boxes %>%\n    str_detect(\"Centro\") %>%\n    text_boxes[.]\n\n\n  data.frame(\n    public_private = single_public_private,\n    stringsAsFactors = FALSE\n  )\n}\n\npublic_private_schools <- map_dfr(school_links, grab_public_private_school)\npublic_private_schools##    public_private\n## 1  Centro Privado\n## 2  Centro Público\n## 3  Centro Público\n## 4  Centro Público\n## 5  Centro Privado\n## 6  Centro Público\n## 7  Centro Público\n## 8  Centro Privado\n## 9  Centro Público\n## 10 Centro Público\n## 11 Centro Público\n## 12 Centro Público\n## 13 Centro Público\n## 14 Centro Público\n## 15 Centro Público\n## 16 Centro Público\n## 17 Centro Público\n## 18 Centro Privado\n## 19 Centro Público\n## 20 Centro Público\n## 21 Centro Privado\n## 22 Centro Público\n## 23 Centro Privado\n## 24 Centro Público\n## 25 Centro Público\n## 26 Centro Privado\n## 27 Centro Público\n# Let's translate the public/private names from Spanish to English\nlookup <- c(\"Centro Público\" = \"Public\", \"Centro Privado\" = \"Private\")\npublic_private_schools$public_private <- lookup[public_private_schools$public_private]\n\n# Merge it with the coordinates data\nall_schools <- cbind(coordinates, public_private_schools)\n\n# Plot private/public by coordinates\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = all_schools, aes(x = longitude, y = latitude, color = public_private)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain by private/public\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"extracting-type-of-school","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.4 Extracting type of school","text":"One additional information might also useful know type school: kindergarten, secondary school, primary school, etc. information just next whether school private public:Now, might think can just copy exact code previous extraction one slightly different. problem don’t know advance type school school might . , can’t use regex search (like public/private) something like “Primary” can “Primary school” many types school. search header box:recycle first XPath public/private scraper find exact <div> belongs type school:10 nodes saw . Let’s extract text within one match keyword header “Tipo Centro”:7th node one ’re looking . Let’s subset node can last time: extract strong tag. Note ’ll use XPath .//strong. .//strong means find strong tags . means search tags current selection. write instead //strong, search strong tags entire source code. . front tells XPath search downwards current selection. Let’s write confirmar get correct text:Perfect, ’s type school also website. “Colegio de Educación Infantil y Primaria” means “Pre-school Primary School”. code ready, can wrap function extract schools:Let’s merge previous data schools visualize points private/public well type school:might seen chapter, webscraping creative. Finding somewhat unique tags identify data ’re looking precise science requires tricks using regular expressions, string manipulations handy XPath knowledge. exercises ’ll need creative find clever ways extract additional data schools data.","code":"\ntext_boxes <-\n  school_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")\n\ntext_boxes## {xml_nodeset (10)}\n##  [1] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [2] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [3] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [4] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [5] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [6] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [7] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [8] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [9] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n## [10] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\nselected_node <- text_boxes %>% xml_text() %>% str_detect(\"Tipo Centro\")\nselected_node##  [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\nsingle_type_school <-\n  text_boxes[selected_node] %>%\n  xml_find_all(\".//strong\") %>%\n  xml_text()\n\nsingle_type_school## [1] \"Colegio de Educación Infantil y Primaria\"\ngrab_type_school <- function(school_link) {\n  Sys.sleep(5)\n  school <- read_html(school_link)\n\n  text_boxes <-\n    school %>%\n    xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")\n\n  selected_node <-\n    text_boxes %>%\n    xml_text() %>%\n    str_detect(\"Tipo Centro\")\n\n  single_type_school <-\n    text_boxes[selected_node] %>%\n    xml_find_all(\".//strong\") %>%\n    xml_text()\n\n\n  data.frame(\n    type_school = single_type_school,\n    stringsAsFactors = FALSE\n  )\n}\n\nall_type_schools <- map_dfr(school_links, grab_type_school)\nall_type_schools##                                     type_school\n## 1                              Escuela Infantil\n## 2                              Escuela Infantil\n## 3                              Escuela Infantil\n## 4                              Escuela Infantil\n## 5                              Escuela Infantil\n## 6  Escuela de Educación Infantil, Casa de Niños\n## 7                              Escuela Infantil\n## 8                              Escuela Infantil\n## 9                              Escuela Infantil\n## 10 Escuela de Educación Infantil, Casa de Niños\n## 11                             Escuela Infantil\n## 12      Escuela Municipal de Educación Infantil\n## 13     Colegio de Educación Infantil y Primaria\n## 14                             Escuela Infantil\n## 15                             Escuela Infantil\n## 16                             Escuela Infantil\n## 17                             Escuela Infantil\n## 18                             Escuela Infantil\n## 19                                       Escola\n## 20                                       Escola\n## 21                                       Escola\n## 22                                       Escola\n## 23                                       Escola\n## 24                                       Escola\n## 25                                       Escola\n## 26                                       Escola\n## 27                                       Escola\nall_schools <- cbind(all_schools, all_type_schools)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = all_schools, aes(x = longitude, y = latitude, color = public_private)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  facet_wrap(~ type_school) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"exercises-4","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.5 Exercises","text":"left menu click says “Enseñanzas” (teachings english). want extract unique modality school. whether school remote -presence classes. can find information :school, classes -presence “presencial” Spanish. result data frame one column 27 rows (one school). result like 27 schools:Hint: might handy pick tags XPath using indices //tag[2] get specific index want.left menu click says “Servicios” (services English). want extract services school provides. example, school provides “Comedor escolar” (school cafeteria) “Transporte” (transportation):result look something like 27 schools:main school page, extract address school. information :result like 27 schools:main page website, scroll ’ll find part website like :schools people visited visiting specific school. type network analysis, might want analyze whether people visiting certain type schools interested schools certain properties. , might need actually go school’s websites extract information. Right now won’t , task extract school’s links case wanted automatically scrape schools well. school, ’ll four links similar schools. result like 27 schools:Hint: ’ll want look hyperlink leads school href tag.Combine all_schools previous exercises final data frame school. Final result like 27 schools:","code":"##   mode_school\n## 1  Presencial\n## 2  Presencial\n## 3  Presencial\n## 4  Presencial\n## 5  Presencial\n## 6  Presencial##                      amenities\n## 11                        <NA>\n## 12                        <NA>\n## 13 Comedor Escolar, Transporte\n## 14                     Comedor\n## 15                     Comedor\n## 16                        <NA>##                                                                             addresses\n## 1                  CALZADA S/N-IRIA. 15917. Puente-Cesures, Padrón, A Coruña, GALICIA\n## 2                                 TRABE. 15110. A Trabe, Ponteceso, A Coruña, GALICIA\n## 3                      CL. MEJORADA, S/N. 02690. Alpera, Albacete, CASTILLA-LA MANCHA\n## 4 TR. CANTARRANAS, 3 B. 02610. El Bonillo, Bonillo (El), Albacete, CASTILLA-LA MANCHA\n## 5                                 C/ de la Fuente de Piedra 10. 28018. Madrid, MADRID\n## 6                                    C/ Mayor 29. 28596. Brea de Tajo, Madrid, MADRID## # A tibble: 27 × 4\n##    school_one                                school_two school_three school_four\n##    <chr>                                     <chr>      <chr>        <chr>      \n##  1 /Colegio/detalles-colegio.action?id=1500… /Colegio/… /Colegio/de… /Colegio/d…\n##  2 /Colegio/detalles-colegio.action?id=1502… /Colegio/… /Colegio/de… /Colegio/d…\n##  3 /Colegio/detalles-colegio.action?id=2009… /Colegio/… /Colegio/de… /Colegio/d…\n##  4 /Colegio/detalles-colegio.action?id=2009… /Colegio/… /Colegio/de… /Colegio/d…\n##  5 /Colegio/detalles-colegio.action?id=2804… /Colegio/… /Colegio/de… /Colegio/d…\n##  6 /Colegio/detalles-colegio.action?id=2804… /Colegio/… /Colegio/de… /Colegio/d…\n##  7 /Colegio/detalles-colegio.action?id=2806… /Colegio/… /Colegio/de… /Colegio/d…\n##  8 /Colegio/detalles-colegio.action?id=2805… /Colegio/… /Colegio/de… /Colegio/d…\n##  9 /Colegio/detalles-colegio.action?id=2806… /Colegio/… /Colegio/de… /Colegio/d…\n## 10 /Colegio/detalles-colegio.action?id=2805… /Colegio/… /Colegio/de… /Colegio/d…\n## # … with 17 more rows## # A tibble: 27 × 11\n##    latitude longitude public_private type_school mode_school amenities addresses\n##       <dbl>     <dbl> <chr>          <chr>       <chr>       <chr>     <chr>    \n##  1     42.7     -8.66 Private        Escuela In… Presencial  <NA>      CALZADA …\n##  2     43.2     -8.89 Public         Escuela In… Presencial  <NA>      TRABE. 1…\n##  3     39.0     -1.23 Public         Escuela In… Presencial  <NA>      CL. MEJO…\n##  4     39.2     -1.62 Public         Escuela In… Presencial  <NA>      TR. CANT…\n##  5     40.4     -3.64 Private        Escuela In… Presencial  <NA>      C/ de la…\n##  6     40.2     -3.11 Public         Escuela de… Presencial  <NA>      C/ Mayor…\n##  7     40.4     -3.70 Public         Escuela In… Presencial  Horario … JOSÉ ABA…\n##  8     40.3     -3.52 Private        Escuela In… Presencial  <NA>      C/ Feder…\n##  9     40.5     -3.37 Public         Escuela In… Presencial  <NA>      C/ José …\n## 10     40.6     -3.45 Public         Escuela de… Presencial  <NA>      C/ Subid…\n## # … with 17 more rows, and 4 more variables: school_one <chr>,\n## #   school_two <chr>, school_three <chr>, school_four <chr>"},{"path":"automating-web-scraping-scripts.html","id":"automating-web-scraping-scripts","chapter":"8 Automating Web Scraping Scripts","heading":"8 Automating Web Scraping Scripts","text":"two types webscraping: one-scrapings frequent scrapings. first one chapter TODO primer webscraping. create program scrape something . common approach ’ve done time throughout book. second one involves building scrapers know used frequently. Many examples come mind: scrapers extract news frequent basis, scrapers extract temperature data daily basis, scrapers collect prices groceries supermarket website . scrapers designed run frequent intervals get timely information.one-scrapers, material book chapter enough. However, frequent scrapings need new tools strategies. asked can automate script? automating mean, example, run script every Thursday 08:00 PM. chapter focuses scheduling programs run whenever want . might need collect data website changing constantly request data API frequent intervals (topic API’s second part book) two cases don’t want manually running house 3 morning run program. chapter make sure don’t .Scheduling scripts different operating systems. chapter focus solely scheduling scripts Linux MacOS. Windows users recommended search ‘Windows Task Scheduler’ online find can schedule R programs.","code":""},{"path":"automating-web-scraping-scripts.html","id":"the-scraping-program","chapter":"8 Automating Web Scraping Scripts","heading":"8.1 The Scraping Program","text":"first thing need scraping program. Let’s recycle one chapter TODO xpath loaded example newspaper “El País”. scraper count number articles sections available newspaper “El País”. R script parse “El País” website, extract number articles per section collect everything data frame. ’s look like:11 sections, one respective number articles. personal research project , ’re interested collecting counts every day three different times day. idea try map newspapers shift writing efforts different sections. hypothesis newspapers certain ideologies might give weight certain sections others give weight sections.scraper precisely ’s missing step: saving data. logic want achieve something like :first time scraper run, save csv file count sectionsIf CSV count section exists, open CSV file append newest data current time stampThis approach add rows new counts every time scraper run. plotting time stamp count sections ’ll able visualize counts change time. , add new section code save results CSV file time stamp current date. Let’s add section save data:summary, script read website “El País”, count number sections save results CSV file ~/newspaper/newspaper_section_counter.csv. directory still doesn’t exist, ’ll create first.","code":"\n# Load all our libraries\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(readr)\n\n# If this were being done on the real website of the newspaper, you'd want to\n# replace the line below with the real link of the website.\nnewspaper_link <- elpais_newspaper_ex()\nnewspaper <- read_html(newspaper_link)\n\nall_sections <-\n  newspaper %>%\n  # Find all <section> tags which have an <article> tag\n  # below each <section> tag. Keep only the <article>\n  # tags which an attribute @data-dtm-region.\n  xml_find_all(\"//section[.//article][@data-dtm-region]\")\n\nfinal_df <-\n  all_sections %>%\n  # Count the number of articles for each section\n  map(~ length(xml_find_all(.x, \".//article\"))) %>%\n  # Name all sections\n  set_names(all_sections %>% xml_attr(\"data-dtm-region\")) %>%\n  # Convert to data frame\n  enframe(name = \"sections\", value = \"num_articles\") %>%\n  unnest(num_articles)\n\nfinal_df## # A tibble: 11 × 2\n##    sections                                   num_articles\n##    <chr>                                             <int>\n##  1 portada_apertura                                      5\n##  2 portada_arrevistada                                   1\n##  3 portada_tematicos_science,-tech-&-health              5\n##  4 portada_tematicos_business-&-economy                  2\n##  5 portada_tematicos_undefined                           1\n##  6 portada_branded_                                      2\n##  7 portada_arrevistada_culture                           5\n##  8 portada_tematicos_work-&-lifestyle                    3\n##  9 portada_arrevistada                                   1\n## 10 portada_tematicos_celebrities,-movies-&-tv            4\n## 11 portada_tematicos_our-selection                       4\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(readr)\n\nnewspaper_link <- elpais_newspaper_ex()\n\nall_sections <-\n  newspaper_link %>%\n  read_html() %>%\n  xml_find_all(\"//section[.//article][@data-dtm-region]\")\n\nfinal_df <-\n  all_sections %>%\n  map(~ length(xml_find_all(.x, \".//article\"))) %>%\n  set_names(all_sections %>% xml_attr(\"data-dtm-region\")) %>%\n  enframe(name = \"sections\", value = \"num_articles\") %>%\n  unnest(num_articles)\n\n# Save the current date time as a column\nfinal_df$date_saved <- format(Sys.time(), \"%Y-%m-%d %H:%M\")\n\n# Where the CSV will be saved. Note that this directory\n# doesn't exist yet.\nfile_path <- \"~/newspaper/newspaper_section_counter.csv\"\n\n# *Try* reading the file. If the file doesn't exist, this will silently save an error\nres <- try(read_csv(file_path, show_col_types = FALSE), silent = TRUE)\n\n# If the file doesn't exist\nif (inherits(res, \"try-error\")) {\n  # Save the data frame we scraped above\n  print(\"File doesn't exist; Creating it\")\n  write_csv(final_df, file_path)\n} else {\n  # If the file was read successfully, append the\n  # new rows and save the file again\n  rbind(res, final_df) %>% write_csv(file_path)\n}"},{"path":"automating-web-scraping-scripts.html","id":"the-terminal","chapter":"8 Automating Web Scraping Scripts","heading":"8.2 The Terminal","text":"use cron, general executing R scripts, ’ll need get familiar terminal. Let’s open terminal. Linux, can pressing keys CTRL + ALT + t together. MacOS click Launchpad icon Dock, type Terminal search field click Terminal. operating systems see window like (exactly similar) one pop :terminal. allows things like create directories files just computer code. Let’s create directory ’ll save CSV file R script performs scraping. create directory, use command mkdir stands makedirectory. Let’s create mkdir ~/newspaper/:Great, directory created. Now copy R script wrote save ~/newspaper/. Save newspaper_scraper.R. R file within ~/newspaper/ called newspaper_scraper.R. can confirm typing ls ~/newspaper/ list files/directories inside ~/newspaper/. see output like one:script saved successfully inside directory. Let’s switch ‘directory’ ~/newspaper/ terminal. terminal can change directories cd command, stands changedirectory, followed path want switch . case, cd ~/newspaper/:can see third line image, now appears ~/newspaper blue, denoting directory right now. execute R script terminal can Rscript command followed file name. case Rscript newspaper_scraper.R. Let’s run :first lines show printing package loading finally see print statement added file doesn’t exit: File exist; Creating . opened CSV computer see sheet like one:Great, scraper works! half job done. Now need come way execute Rscript ~/newspaper/newspaper_scraper.R schedule.","code":"ls ~/newspaper/\n# newspaper_scraper.R"},{"path":"automating-web-scraping-scripts.html","id":"cron-your-scheduling-friend","chapter":"8 Automating Web Scraping Scripts","heading":"8.3 cron, your scheduling friend","text":"Luckily , program already exists. ’s called cron allows run script schedule. Ubuntu, can install cron :MacOS can install :cases, install successful, able confirm works crontab -l:output means scheduled scripts computer. schedule script cron need two things: command execute schedule. command execute already know, ’s Rscript ~/newspaper/newspaper_scraper.R. specifying schedules, cron particular syntax. work? Let’s take look:bottom image see 5 *. text tells * stands . order, represent minutes, hours, day month, month day week.* fact placeholder signal whenever * means command repeated instance place holder. Complicated? Look example:writing * * * * *, ’re scheduling program run every minute, every hour, every day month, every month every day week. Say changed schedule run every 30 minutes hour day month day week. sounds awfully complicated say. simpler way say script run every 30 minutes date parameters *, means every unit parameters.\nschedule look like? know first slot minutes can write 30 first slot:’re effectively scheduling something run minute 30 hour, day, month, day week (day week, last slot, clashes schedule third slot day month day matching either day month, day week, shall matched).Now know * represents date parameter can start develop interesting schedules. example, Wednesdays third day week (start counting Monday), can run schedule every 30 minutes Wednesdays:might want run scraper 05:30 Saturday Sunday:expression reads like : run 30th minute 5th hour every month Saturday Sunday (6th 7th day week). simple rules can take long way building scraper.Let’s say wanted run newspaper scraper every 4 hours, every day, look like? sounds bit different ’ve done . syntax ’ve discussed specifically writes day / hour / minute want scraper . way saying, regardless day / hour / minute, run scraper every X hours. cron additional tricks. wanted run scraper every every 4 hours write like :date parameter want make recurrent, add / frequency want. instead wanted run scraper every 4 hours, every 2 days, write something like :. ’s basics cron. simple rules allow go far scheduling scripts scrapers APIs. details enough get example running.Let’s schedule newspaper scraper run every minute, just make sure works. get messy ’ll append results CSV file continuously, filling CSV repeated data. However, give proof script running schedule. want run every minute, cron expression * * * * *, simplest expression.save cron expression type crontab -e terminal. first time using crontab see something like :allow pick editor want use editing cron schedule. Pick whichever options points nano, easiest one. choosing editor (prompted pick editor), continue open new file like one:file write schedule command want cron run. Either scroll mouse hit scroll key bottom right keyboard go last line editor. last line, need write cron schedule expression command want execute:finish writing , terminal look something like :exit cron interface, follow steps:Hit CTRL X (exiting cron interface)prompt save file. PressY save .Press enter save cron schedule file name ., back terminal cron job saved. Nothing special happening moment. Wait two three minutes open CSV file . find records duplicated different time stamp:cron schedule worked expected! see different time stamps date_saved column, reflecting scraper run every minute. close chapter, remember remove schedule Rscript command cron. Enter crontab -e, go last line, delete text exit cron interface instructions detailed .","code":"sudo apt-get update\nsudo apt-get install cronbrew install --cask cron* * * * *30 * * * *30 * * * 330 5 * * 6,71 */4 * * *1 */4 * * */2* * * * * Rscript ~/newspaper/newspaper_scraper.R"},{"path":"automating-web-scraping-scripts.html","id":"conclusion-2","chapter":"8 Automating Web Scraping Scripts","heading":"8.4 Conclusion","text":"framework building scraper, testing scheduling run frequent intervals powerful. commands can automate program (fact, R programming language program). However, approach limitations. computer needs turned time order cron schedule run. ’re school project ’s possible, might get around using computer. However, demanding scrapings (lots data, frequent intervals) ’s almost always better idea run scraper server.Launching server running scraper scope chapter keep mind building scrapers work. many tutorials internet.cron can also become complex schedule patterns difficult. ’s bunch resources can help internet. worked :Crontab GuruCron tutorial","code":""},{"path":"automating-web-scraping-scripts.html","id":"exercises-5","chapter":"8 Automating Web Scraping Scripts","heading":"8.5 Exercises","text":"cron expression run every 15 minutes Monday, Wednesday Friday February?cron expression run every 15 minutes Monday, Wednesday Friday February?R packaged called cronR allows setup cron schedules within R. Can replicate chapter using cronR package? package documentation can found .R packaged called cronR allows setup cron schedules within R. Can replicate chapter using cronR package? package documentation can found .Can write script empties trash folder personal computer every Monday 11AM? Write scraper well cron expression. Remember remove cron deploying avoid unexpected files deleted.Can write script empties trash folder personal computer every Monday 11AM? Write scraper well cron expression. Remember remove cron deploying avoid unexpected files deleted.","code":""},{"path":"scraping-javascript-based-website.html","id":"scraping-javascript-based-website","chapter":"9 Scraping JavaScript based website","heading":"9 Scraping JavaScript based website","text":"Even techniques ’ve covered book, websites can’t scraped traditional way. mean? websites content hidden behind JavaScript code can’t seen simply reading HTML code. website built interactivity, meaning parts website revealed interact . read HTML website, won’t able see data want. usual, let’s motivate chapter example.European Social Survey (ESS) academically driven cross-national survey conducted across Europe since establishment 2001. Every two years, face--face interviews conducted newly selected, cross-sectional samples. survey measures attitudes, beliefs behavior patterns diverse populations thirty nations. website ‘Data Portal’ allows pick variables asked questionnaire, pick country/years data want download custom data set . can find data portal . looks like:developer’s tools (CTRL-SHIT-C), can see HTML code behind website. Let’s say want extract variable names associated category questions. ‘Media use trust’ several question names labels belong category:However, unless click drop menu, questions won’t displayed code behind website. , right now spent 10 minutes searching source code developer’s tools right, won’t find names variables anywhere:variable names won’t found anywhere. might think, well ’s easy solve: ’ll click drop menu use new generated link (data displayed) read withn read_html. issue URL website whether click drop menu:URL, even data displayed. means whenever use read_html link, variable labels won’t displayed won’t able gather data. ’s people thought Selenium.","code":""},{"path":"scraping-javascript-based-website.html","id":"introduction-to-rselenium","chapter":"9 Scraping JavaScript based website","heading":"9.1 Introduction to RSelenium","text":"Selenium software program use ‘mimick’ human. Yes, ’s right. Selenium literally open browser click parts page human. ’ve clicked certain parts website, can use knowledge XPath, HTML xml2 extract data need.example ’ll use chapter blog statistician Andrew Gelman. According Rohan Alexander:Gelman’s blog—Statistical Modeling, Causal Inference, Social Science—launched 2004—go-place fun mix somewhat-nerdy statistics-focused content. first post promised ‘…report recent research ongoing half-baked ideas, including … Bayesian statistics, multilevel modeling, causal inference, political science.’ 17 years , site much kept promise.’ll use blog explore topics discussed world statistics last 17 years. website need scraped using Selenium can scraped just reading HTML code. However, websites need use Selenium can’t saved locally doomed change time, entails risk making chapter unusable future. reason going perform scraping using traditional example highlighting JavaScript based website.Let’s load packages ’ll use example:described , Selenium tries mimick human. literally means need open browser. RSelenium rsDriver function. function initializes browser opens us:last line starts withBEGIN ’ll see options browser. example, browserName browserVersion. important thing , aside output, see browser opened computer completely empty:RSelenium opened browser don’t need touch. Everything browser performed using object remDr, browser initiated. remDr property called client can navigate website. Let’s open Andrew Gelman’s blog browser:look browser see blog opened :effective way navigate RSelenium use XPath find parts website want perform action use methods click, send submit something website. example, suppose wanted perform natural language processing posts last 17 years blog posts. first ’d like click blog post name extract text.","code":"\nlibrary(RSelenium)\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\n\nblog_link <- gelman_blog_ex()\nremDr <- rsDriver(port = 4441L)## checking Selenium Server versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking chromedriver versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking geckodriver versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking phantomjs versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## [1] \"Connecting to remote server\"\n## $acceptInsecureCerts\n## [1] FALSE\n\n## $browserName\n## [1] \"firefox\"\n\n## $browserVersion\n## [1] \"106.0\"\n\n## $`moz:accessibilityChecks`\n## [1] FALSE\n\n## $`moz:buildID`\n## [1] \"20221010110315\"\n\n## $`moz:geckodriverVersion`\n## [1] \"0.32.0\"\n\n## $`moz:headless`\n## [1] FALSE\n\n## $`moz:platformVersion`\n## [1] \"5.4.0-39-generic\"\n\n## $`moz:processID`\n## [1] 271886\n\n## $`moz:profile`\n## [1] \"/tmp/rust_mozprofile5ANm8J\"\n\n## $`moz:shutdownTimeout`\n## [1] 60000\n\n## $`moz:useNonSpecCompliantPointerOrigin`\n## [1] FALSE\n\n## $`moz:webdriverClick`\n## [1] TRUE\n\n## $`moz:windowless`\n## [1] FALSE\n\n## $pageLoadStrategy\n## [1] \"normal\"\n\n## $platformName\n## [1] \"linux\"\n\n## $proxy\n## named list()\n\n## $setWindowRect\n## [1] TRUE\n\n## $strictFileInteractability\n## [1] FALSE\n\n## $timeouts\n## $timeouts$implicit\n## [1] 0\n\n## $timeouts$pageLoad\n## [1] 300000\n\n## $timeouts$script\n## [1] 30000\n\n\n## $unhandledPromptBehavior\n## [1] \"dismiss and notify\"\n\n## $webdriver.remote.sessionid\n## [1] \"a3585bf8-9c4a-462e-a74d-b9d6b35daff7\"\n\n## $id\n## [1] \"a3585bf8-9c4a-462e-a74d-b9d6b35daff7\"\nremDr$client$navigate(prep_browser(blog_link))"},{"path":"scraping-javascript-based-website.html","id":"navigating-a-website-in-rselenium","chapter":"9 Scraping JavaScript based website","heading":"9.2 Navigating a website in RSelenium","text":"Let’s focus first blog post. Let’s find XPath expression return position first blog post name. ’re looking <> tag equivalent ‘clicking’ blog post. know ’s contains text blog name:validate XPath nothing prevents us reading source code trying . point read read_html tried several XPath managed land one ’re interested :Let’s explain . look source image , ’ll see <> tag ’s h1 tag distinctive class entry-title. came following XPath:entire document (//)Find h1 tags class entry-titleThen find <> tags h1 tagWrap everything () collect resultKeep first results [1]now know XPath validated need locate RSelenium click . Everything RSelenium done using property client, many methods associated browser. main methods ’ll using findElement clickElement. findElement used position pointer browser exactly want clickElement click . Let’s use XPath move pointer blog post click :Now look browser:scroll ’ll see comments:clicked first entry browser entered post. Great! simple strategy findElement clickElement can reused needs. strategy simple:Find XPath want clickUse XPath findElementUse clickElement click itOne natural follow question actually extract HTML code extract stuff us analyze. , ’ve opened browser clicked something. actual fun extract data stuff . RSelenium property getPageSource(). property returns list need extract first slot:","code":"\nblog_link %>%\n  read_html() %>%\n  xml_find_all(\"(//h1[@class='entry-title']//a)[1]\")## {xml_nodeset (1)}\n## [1] <a href=\"https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-th ...\n# Since we'll use the client a lot let's save it on a separate object\ndriver <- remDr$client\ndriver$findElement(value = \"(//h1[@class='entry-title']//a)[1]\")$clickElement()\npage_source <- driver$getPageSource()[[1]]"},{"path":"scraping-javascript-based-website.html","id":"bringing-the-source-code","chapter":"9 Scraping JavaScript based website","heading":"9.3 Bringing the source code","text":"HTML code string ’re familiar ground. can use read_html read , find XPath categories text extract :post Bayesian Statistics Social Sciences. Let’s extract date wellLet’s also extract date order timestamp categories used time:Note use parse_date_time time format (% symbols) convert string date/time R. information can create clean data frame entry dates respective categories list column:Great, information clean ready recycle code function loop blog posts. , need one final thing: go back main page blog. Don’t forget browser still open whatever new moves make using browser starting left . Luckily, RSelenium makes simple: goBack attribute. Let’s go back:browser now back main page:","code":"\nhtml_code <-\n  page_source %>%\n  read_html()\n\ncategories_first_post <-\n  html_code %>%\n  xml_find_all(\"//footer[@class='entry-meta']//a[contains(@href, 'category')]\") %>%\n  xml_text()\n\ncategories_first_post## [1] \"Bayesian Statistics\" \"Economics\"           \"Political Science\"\n## [4] \"Sociology\"\nentry_date <-\n  html_code %>%\n  xml_find_all(\"//time[@class='entry-date']\") %>%\n  xml_text() %>%\n  parse_date_time(\"%B %d, %Y %I:%M %p\")\n\nentry_date\nfinal_res <- tibble(entry_date = entry_date, categories = list(categories_first_post))\nfinal_res## # A tibble: 1 × 2\n##   entry_date          categories\n##   <dttm>              <list>\n## 1 2022-10-15 09:08:00 <chr [4]>\ndriver$goBack()"},{"path":"scraping-javascript-based-website.html","id":"scaling-rselenium-to-many-websites","chapter":"9 Scraping JavaScript based website","heading":"9.4 Scaling RSelenium to many websites","text":"ready, can move code function loop blog posts page:code runs, open browser enjoy show. browser start working self ghost scrolling clicking blog post. finishes, ’ll list categories last 20 blog posts. Let’s combine get count used categories:","code":"\ncollect_categories <- function(page_source) {\n  # Read source code of blog psot\n  html_code <-\n    page_source %>%\n    read_html()\n\n  # Extract categories\n  categories_first_post <-\n    html_code %>%\n    xml_find_all(\"//footer[@class='entry-meta']//a[contains(@href, 'category')]\") %>%\n    xml_text()\n\n  # Extract entry date\n  entry_date <-\n    html_code %>%\n    xml_find_all(\"//time[@class='entry-date']\") %>%\n    xml_text() %>%\n    parse_date_time(\"%B %d, %Y %I:%M %p\")\n\n  # Collect everything in a data frame\n  final_res <- tibble(entry_date = entry_date, categories = list(categories_first_post))\n  final_res\n}\n\n# Get all number of blog posts on this page\nnum_blogs <-\n  blog_link %>%\n  read_html() %>%\n  xml_find_all(\"//h1[@class='entry-title']//a\") %>%\n  length()\n\nblog_posts <- list()\n\nfor (i in seq_len(num_blogs)) {\n  print(i)\n  # Go to the next blog post\n  xpath <- paste0(\"(//h1[@class='entry-title']//a)[\", i, \"]\")\n  Sys.sleep(2)\n  driver$findElement(value = xpath)$clickElement()\n\n  # Get the source code\n  page_source <- driver$getPageSource()[[1]]\n\n  # Grab all categories\n  blog_posts[[i]] <- collect_categories(page_source)\n\n  # Go back to the main page before next iteration\n  driver$goBack()\n}\ncombined_df <- bind_rows(blog_posts)\n\ncombined_df %>%\n  unnest(categories) %>%\n  count(categories) %>%\n  arrange(-n)## # A tibble: 16 × 2\n##    categories                   n\n##    <chr>                    <int>\n##  1 Bayesian Statistics          7\n##  2 Zombies                      7\n##  3 Miscellaneous Statistics     5\n##  4 Political Science            5\n##  5 Teaching                     5\n##  6 Sociology                    4\n##  7 Statistical computing        4\n##  8 Causal Inference             3\n##  9 Economics                    3\n## 10 Sports                       3\n## 11 Miscellaneous Science        2\n## 12 Stan                         2\n## 13 Art                          1\n## 14 Decision Theory              1\n## 15 Jobs                         1\n## 16 Public Health                1"},{"path":"scraping-javascript-based-website.html","id":"posting-a-comment","chapter":"9 Scraping JavaScript based website","heading":"9.5 Posting a comment","text":"Just way show can click button fill forms. done scrape posts locally scrapex (might possible).","code":""},{"path":"scraping-javascript-based-website.html","id":"exercises-6","chapter":"9 Scraping JavaScript based website","heading":"9.6 Exercises","text":"Building ‘extract categories posts’ example, extend extract blog posts last 20 pages blog posts. ’ll need click ‘Older posts’ see previous page blog posts:","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"ethical-issues-in-web-scraping","chapter":"10 Ethical issues in Web Scraping","heading":"10 Ethical issues in Web Scraping","text":"Although quick get running, web scraping delicate issue. ’s delicate involves grabbing information using purposes. respects might even contrary terms services website. also involves cluttering servers website potentially many requests, making functioning website less optimal. chapter ’ll describe affect website scrape , can avoid problems website owners figure indeed website allows scrape information.","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"make-your-scraper-sleep","chapter":"10 Ethical issues in Web Scraping","heading":"10.1 Make your scraper sleep","text":"Whenever run scraping script, program makes request website. request means ask servers behind website send information. learned scraping worked like , completely shocked. thought scraping like copying content website local computer. Nothing harmless , right?Well, surprise, making request many times every day without knowing . Scraping entering website browser. moment hit enter go website information actually displayed time takes server behind website return content. operation important website. involves requesting information, waiting server return rendering browser. human slow enough server handle requests needs imagine human tried enter website 5 times per second continuously 48 hours. amounts 864,000 requests. ’s lot.Big websites Google Amazon enough servers throttle handle millions requests per second content internet . reason, ’s important whenever make request website (calling read_html read_xml), add system sleep scraper. R can Sys.sleep(amount_of_seconds) amount_of_seconds amount seconds want sleep making request.scraping website , making single request, adding system sleep matter. want avoid making many requests shorts amount time. example, example chapter TODO Spanish school scraped information several different schools. school scraped, meant making request website. ’s perfect example want sleep making request. R code, skeleton code look something like system sleep:create single function works well scraping single school. launch scrape different schools, add Sys.sleep(5) scraping school. way, making new request, ’ll let servers rest avoid many requests short amount time.many seconds wait? Sometimes robotstxt file (see section ) tell many seconds per scrape wait. , ’s real estimate. Depending many requests make ’ll want lower number seconds 2 3. Multiplying total number websites ’ll scrape number seconds ’ll sleep give rough estimate much time program last.rule thumb, ’s always good idea limit scraping non-working hours evening. can help reduce chances collapsing website since fewer people visiting websites evening.","code":"\nlibrary(scrapex)\n\n# List of links to make a request\nschool_links <- spanish_schools_ex()\n\n# List where we will save the information for each link\nall_schools <- list()\n\nsingle_school_scraper <- function(single_link) {\n  # Before making a request, sleep 5 seconds\n  Sys.sleep(5)\n\n  # Perform some scraping\n}\n\n# Loop over each link to make a request\nfor (single_link in school_links) {\n  # Save results in a list\n  all_schools[[single_link]] <- single_school_scraper(single_link)\n}"},{"path":"ethical-issues-in-web-scraping.html","id":"terms-of-services","chapter":"10 Ethical issues in Web Scraping","heading":"10.2 Terms of services","text":"General Data Protection Regulation (GDPR) came effect Europe, websites needed make sure every user agreed terms services. terms services lengthy contain lot information website can data. However, also contains information can data. internet users, simply doesn’t matter. ’re building scraping program, however, important.Whenever intend scrape website, make sure read terms services. website clearly states web scraping allowed, must respect . example, Facebook clause specifically automated data collection:collect users’ content information, otherwise access Facebook, using automated means (harvesting bots, robots, spiders scrapers) without prior permission.website explicitly prohibits scraping, . Let make clear : terms services forbids scraping website, . can legal consequences.’s clear terms services, contact website receive written confirmation website.","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"copying-information","chapter":"10 Ethical issues in Web Scraping","heading":"10.3 Copying information","text":"Even website allows user scrape contents, might preferences sections website can scrape forbidden. ’s standard file called robots.txt nearly website internet tells want parts website can scraped. robots.txt just convenient form website tell URLs can scrape; enforce anything block way. cases, follow guidelines robots.txt.robots.txt file websites located main URL. example, robots.txt Facebook www.facebook.com/robots.txt. Similarly, robots.txt Google can found google.com/robots.txt looks like :documents URL Google website explicitly tells ones allowed disallowed. section ’ll use robotstxt R package makes easy tell whether website can scrapable. example, can figure can scrape landing page Wikipedia :TRUE statement tell us can . Let’s see can scrape Facebook homepage:Using paths_allowed can provide link website ’ll automatically extract robots.txt figure can scrape . scraping website, check whether URL want scrape allowed.Another thing aware robotstxt files often contain field Crawl-delay, suggesting time wait requests. keep mind Sys.sleep requests.","code":"\nlibrary(robotstxt)\npaths_allowed(\"https://wikipedia.org\")\npaths_allowed(\"https://facebook.com\")"},{"path":"ethical-issues-in-web-scraping.html","id":"identifying-yourself","chapter":"10 Ethical issues in Web Scraping","heading":"10.4 Identifying yourself","text":"Even website allows scrape URL ’re , need extra careful identify . means need give website clear identification making requests. way website can contact find ’s something wrong requests, even directly block . Remember times: ’re scraping data polite grabbing data. owner data considers don’t want give data, ’re right .identify need something called “User-Agent”. User-Agent contains information computer browser. can find user agent googling “user agent?”. Google directly tell :hand, just need tell scraper incorporate request. ? httr package. code :Notice added name / email user agent? often websites can contact case believe ’m breaking terms services want know purpose scraper. case, ’s just way polite. R user agent can set top script. need include anywhere else inside scraper inside loop scrapes many websites; user agent set globally reused requests.","code":"\nlibrary(httr)\n\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:105.0) Gecko/20100101 Firefox/105.0; Jorge Cimentada / cimentadaj@gmail.com\")\n)"},{"path":"introduction-to-rest-apis.html","id":"introduction-to-rest-apis","chapter":"11 Introduction to REST APIs","heading":"11 Introduction to REST APIs","text":"API\nFundamentals API communication","code":""},{"path":"introduction-to-json.html","id":"introduction-to-json","chapter":"12 Introduction to JSON","heading":"12 Introduction to JSON","text":"Examples JSON\nExamples nested JSON\n’re parsed RHow handle nested data frames Rhttps://www.gastonsanchez.com/intro2cwd/json.html","code":""},{"path":"a-primer-on-apis.html","id":"a-primer-on-apis","chapter":"13 A primer on APIs","heading":"13 A primer on APIs","text":"","code":""},{"path":"what-is-a-restful-api.html","id":"what-is-a-restful-api","chapter":"14 What is a RESTful API?","heading":"14 What is a RESTful API?","text":"https://www.gastonsanchez.com/intro2cwd/http.htmlhttps://www.gastonsanchez.com/intro2cwd/apis.html","code":""},{"path":"authentication-in-apis.html","id":"authentication-in-apis","chapter":"15 Authentication in APIs","heading":"15 Authentication in APIs","text":"","code":""},{"path":"case-study-grabbing-data-from-a-public-api.html","id":"case-study-grabbing-data-from-a-public-api","chapter":"16 Case Study: grabbing data from a public API","heading":"16 Case Study: grabbing data from a public API","text":"","code":""},{"path":"why-automation-is-important.html","id":"why-automation-is-important","chapter":"17 Why automation is important","heading":"17 Why automation is important","text":"Examples data lost (bycicle)\nLack time scrape times","code":""},{"path":"automating-data-collectiong-programs.html","id":"automating-data-collectiong-programs","chapter":"18 Automating data collectiong programs","heading":"18 Automating data collectiong programs","text":"","code":""},{"path":"insightful-and-robust-data-collection-progams.html","id":"insightful-and-robust-data-collection-progams","chapter":"19 Insightful and robust data collection progams","heading":"19 Insightful and robust data collection progams","text":"Adding logger\nChecking inputs\nSaving empty rows avoid breaking programs","code":""}]
