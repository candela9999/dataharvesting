```{r automating-bicing, echo = FALSE, eval = TRUE}
main_dir_api <- "./images/automating_bicing"

knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.path = paste0(main_dir_api, "/automatic_rmarkdown/"),
  fig.asp = 0.618
)

```

# Automating API programs: real-time bicycle data

Grabbing data from APIs can be achieved with all of the techniques we've covered in the book so far. However, as it is the case with web scraping, data is ephemeral. If you're interested in web scraping some temperature data for your city on a website, the temperature from 6 months ago for a given neighborhood might not be available any more. That's why it's very common for for web scraping or API scripts to be automated.

Let me tell you a brief story that motivates this chapter. Throughout my PhD studies I was living in Barcelona where I used to take a bike ride from my house to my university. As it is common in Barcelona, I used the public bicycle system called "Bicing". Every day, I used to take the bike from the upper part of the city, riding down through the big wide streets down to the campus that was next to the beach. Whenever the day was nearing the end, I took the bike up again to the upper part of the city. Yet every day when I tried to find a bike, there were no bikes available in the stations close to my university. This happened a lot as well in the mornings when I wanted to grab a bike to go to my university. I used to waste up to 15-20 minutes just waiting for bikes to arrive in one of the stations. With 3-4 stations near my university, I also had to juggle between which station I would wait for the bike. My 'estimation' was based on experience: on this day I've seen that bikes arrive much more often on this station than in the other one. However, that was something of a leap of faith: when I logged into the 'Bicing' mobile app, I saw that some bikes arrived earlier in other stations and I'd missed that opportunity.

After spending about 2-3 years doing this I started to think there must be a way to make my 'estimations' more accurate. For example, if I had data on the availability of bikes on each station, I could probably estimate some type of poisson process where I could measure and predict the rate at which bikes arrived during different parts of the day. All of that was fine but I didn't have data on that. **That's until I found it**. Bicing had a real time API that allow you to check the status of each station (number of bikes available) and the time stamp at the time I was requesting this data. That's it, that's the information I needed but I needed it pretty much every minute, to be able to get the entire history of the station during the day. This near second-by-second snapshot would allow me to record at which interval of the day a bike was retrieved and at which interval another bike was deposited on each station. How do I do that? With automation.

## The Bicing API

The `scrapex` package contains a replica of the bicing API that I used during my PhD studies. Let's load the packages we'll use in the chapter and launch the bicing API.

```{r}
library(scrapex)
library(httr2)
library(dplyr)
library(readr)

bicing <- api_bicing()
```
As usual, the first we want to do is access the documentation. Here's what we get:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "main_bicing_docs.png"))
```


The bicing API has only one endpoint. Let's open up the endpoint to see if there are any arguments that we need to pass to the endpoint:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "endpoint_bicing.png"))
```

The documentation of the endpoint states a clear description of what the endpoint returns:

> Access real time bicycle usage from a sample of 20 stations from Barcelona's public bicycle system

As this is an example API, I've only included 20 stations from the original results. The original results contain information for over 100 stations all over Barcelona. We can also see under the *parameters* heading that this endpoint does not need any parameters.

However, the most important thing from this endpoint can be interpreted from the description that we see above. *Access real time bicycle usage* is very transparent in highlighting that this data is *real time*. This means that data that we see in a request right now, might not be the same data that we see in 10 minutes. Since I created this sample API, I made sure that this is the case. This means that every time that we request some data from this endpoint, we'll see completely new data from the previous request. The nature of this API forces us to think along two lines: if we want to understand bicycle patterns, we need to automate this process and we need to save this data somewhere.

This is the point in my previous story where I had to learn more about robust programs, focusing on solid programming foundations and thinking about edge cases. Since this program will run every minute, if something fails (the bicing API gets a lot of requests, the response of the API changes, etc..) we want the API to persist, go around edge cases where it can and send an alarm if everything falls down. In this chapter we'll build a very similar example to what I had to go through in building my bicycle time-to-arrival time predictor.

Since we already know enough of the single endpoint of this API, let's try to build a request to check how the output is structured. From the image above we can see that the endpoint is `/api/v1/real_time_bicycles` so let's append it to the base of the API:

```{r}
rt_bicing <- paste0(bicing$api_web, "/api/v1/real_time_bicycles")
rt_bicing
```
There we go, let's make a request and check the body:

```{r}
resp_output <- 
  rt_bicing %>%
  request() %>% 
  req_perform()

resp_output
```

The status code of the request is `200` so everything went successful. We see that the content-type is in JSON format, so it's everything we're used to. Let's check the content of the body:

```{r}
sample_output <-
  resp_output %>%
  resp_body_json() %>%
  head(n = 2)

sample_output
```

I've only shown the first two slots of the resulting list (parsed from the JSON directly using `resp_body_json`). This output looks like each slot is the row of a data frame where each contains a column, where we have names like `slots`, `in_use`, `latitude/longitude` and `streetName`. There are different ways to parse this output but the first that comes to mind is to loop over each slot and simply try to convert it to a data frame. Since each slot has a named list inside, `data.frame` can convert that directly to a data frame. Finally, we can bind all those rows into a single data frame. Let's try it:

```{r}
sample_output %>%
  lapply(data.frame) %>%
  bind_rows()
```

Yep, works well. Let's go back to the request an extract all stations and combine them together:

```{r}
all_stations <-
  resp_output %>%
  resp_body_json()

all_stations_df <- 
  all_stations %>%
  lapply(data.frame) %>%
  bind_rows()

all_stations_df
```

Great, there we go. We managed to access the API, request the real time information of all bicycles and parse the output into a data frame. The most important columns here are `slots`, `current_time` and `in_use`. With these three you can calculate the intensity at which the station is used throughout the day. In addition, we have cool stuff like the `latitude`/`longitude` to locate the station in case we want to make, for example, maps of usage. Let's take a step back and move all of our code into a function that makes a request:

```{r}
real_time_bicing <- function(bicing_api) {
  rt_bicing <- paste0(bicing_api$api_web, "/api/v1/real_time_bicycles")
  
  resp_output <- 
    rt_bicing %>%
    request() %>% 
    req_perform()
  
  all_stations <-
  resp_output %>%
  resp_body_json()

  all_stations_df <- 
    all_stations %>%
    lapply(data.frame) %>%
    bind_rows()
  
  all_stations_df
}
```
Using this example function we can confirm that making two requests will return different data in terms of bicycle usage. For example:

```{r}
first <- real_time_bicing(bicing) %>% select(in_use) %>% rename(first_in_use = in_use)
second <- real_time_bicing(bicing) %>% select(in_use) %>% rename(second_in_use = in_use)
bind_cols(first, second)
```

We performed two requests and only selected the column `in_use` to check that these two columns are different. In effect, the usage of just a 1 second difference shows vastly different usage patterns for each station. This highlights that what we're really interested here is in adding the two ingredients for making this a robust program: save the data on the fly and automate the script.

## Saving data in API programs

The first step we'll take is saving the data. The strategy here can be very different depending on your needs. For example, you might want to access an online database such as MySQL and save the data there. That has a lot of benefits and also some drawbacks. In this example we'll go with the very basics and save the result in a local CSV file. This is a very valid strategy depending on your needs. The main drawback from this strategy is that we'll save the data *locally* meaning we have no backup in case you loose your computer or it breaks. An alternative is to host a CSV, for example, on a Google Drive and access it real time to save your results. We'll focus on a simpler example by loading/saving our CSV in our local computer.

For saving data we need to focus on several things:

* We must perform the request to the bicing API

* We must specify a local path where the CSV file will be hosted. If the directory of the CSV file has not been created, we should create it.

* If the CSV file does not exist, then we should create it from scratch

* If the CSV file *exists*, then we want to append the result

Here's a rough implementation with comments:

```{r}
save_bicing_data <- function(bicing_api) {
  # Perform a request to the bicing API and get the data frame back
  bicing_results <- real_time_bicing(bicing_api)
  
  # Specify the local path where the file will be saved
  local_csv <- "/home/jorge.cimentada/bicing/bicing_history.csv"
  # Extract the directory where the local file is saved
  main_dir <- dirname(local_csv)

  # If the directory does *not* exist, create it, recursively creating each folder  
  if (!dir.exists(main_dir)) {
    dir.create(main_dir, recursive = TRUE)
  }

  # If the file does not exist, save the current bicing response  
  if (!file.exists(local_csv)) {
    write_csv(bicing_results, local_csv)
  } else {
    # If the file does exist, the *append* the result to the already existing CSV file
    write_csv(bicing_results, local_csv, append = TRUE)
  }
}
```

One way to test whether this works is to run this twice and read the results back to a CSV to check that the data frame is correctly structured. Let's try it:

```{r, eval = FALSE}
save_bicing_data(bicing)
Sys.sleep(5)
save_bicing_data(bicing)

bicing_history <- read_csv("/home/jorge.cimentada/bicing/bicing_history.csv")
bicing_history %>%
  distinct(current_time)
```
```{r, echo = FALSE}
dates <- c(lubridate::ymd_hms("2022-12-20 23:51:56"), lubridate::ymd_hms("2022-12-20 23:52:04	"))

tibble(
  current_time = dates
)
```

```{r, echo = FALSE, include = FALSE}
file.remove("/home/jorge.cimentada/bicing/bicing_history.csv")
```

We called the API once, saved the results and the allowed the program to sleep for 5 seconds. We then called the API again and saved the results. After this, we read the `bicing_history.csv` file into an R data frame and showed the distinct time stamps that the file has. If we requested the data from the API twice and saved the results, then we should find two time stamps. That's precisely what we find in the results meaning that our results are correct. Great! We should also have two numbers in the column `in_use` for the number of bicycles under usage. Let's pick one station to check that it works:

```{r, eval = FALSE}
bicing_history %>%
  filter(streetName == "Ribes") %>% 
  select(current_time, streetName, in_use)
```
```{r, echo = FALSE}
tibble(
  current_time = dates,
  streetName = c("Ribes", "Ribes"), 
  in_use = c(13, 19) 
)
```

In the first time stamp the station had 13 bikes and in the second time stamp it had 19. Our aim is to automate this to run very frequently and thus we'll get hundreds of rows for each station. 

## Automating the program

Just as we did in chapter TODO chapter automating web scrapers, APIs need to be automated. The strategy is pretty much the same as in that chapter. We create a script that defines the functions we'll use for the program and run the functions at the end. In our case, that's running the function `save_bicing_data`. We then automate the process using `cron` by running the script in a schedule. Let's organize all of our code in a script. Here's the code that needs to be in a script:

```{r, eval = FALSE}
library(scrapex)
library(httr2)
library(dplyr)
library(readr)

bicing <- api_bicing()

real_time_bicing <- function(bicing_api) {
  rt_bicing <- paste0(bicing_api$api_web, "/api/v1/real_time_bicycles")
  
  resp_output <- 
    rt_bicing %>%
    request() %>% 
    req_perform()
  
  all_stations <-
  resp_output %>%
  resp_body_json()

  all_stations_df <- 
    all_stations %>%
    lapply(data.frame) %>%
    bind_rows()
  
  all_stations_df
}

save_bicing_data <- function(bicing_api) {
  # Perform a request to the bicing API and get the data frame back
  bicing_results <- real_time_bicing(bicing_api)
  
  # Specify the local path where the file will be saved
  local_csv <- "/home/jorge.cimentada/bicing/bicing_history.csv"
  # Extract the directory where the local file is saved
  main_dir <- dirname(local_csv)

  # If the directory does *not* exist, create it, recursively creating each folder  
  if (!dir.exists(main_dir)) {
    dir.create(main_dir, recursive = TRUE)
  }

  # If the file does not exist, save the current bicing response  
  if (!file.exists(local_csv)) {
    write_csv(bicing_results, local_csv)
  } else {
    # If the file does exist, the *append* the result to the already existing CSV file
    write_csv(bicing_results, local_csv, append = TRUE)
  }
}

save_bicing_data(bicing)
```

Let's save that script locally. I'm going to save it as `bicing_api.R` in `/home/jorge.cimentada/bicing/` but you need to save it locally on *your* computer somewhere else. Once you have that file saved we have to set up the `cron` job to schedule the script. Refer to chapter TODO chapter web scraping on how `cron` works and how the syntax for setting the schedule works. So at this point you should have the the file `bicing_api.R` saved locally wherever you want to store the bicing data. In my case, you can see it here:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "console_ls_bicing.png"))
```

Let's test that it works first by running `Rscript api_bicing.R`:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "results_api_bicing_console.png"))
```

If this works we should see that the file is saved with `ls`, let's check:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "ls_bicing_history.png"))
```

Moreover, we should be able to see that the CSV file has content inside:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "bicing_history_csv.png"))
```

Let's type `crontab -e` to open the `crontab` console:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "main_menu_crontab.png"))
```

Since we want to run this every, the `cron` expression we want is `* * * * * Rscript /home/jorge.cimentada/bicing/api_bicing.R`. This effectively reads as: for every minute of every day of every year, run the script `api_bicing.R`. This is how it should look like in `crontab`:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "rscript_cron.png"))
```

With that out of the way, remember that to exit the cron interface, follow these steps:

* Hit CTRL and X (this is for exiting the cron interface)
* It will prompt you to save the file. Press Y to save it.
* Press enter to save the cron schedule file with the same name it has.

Immediately `cron` will start the schedule. Wait two or three minutes and if you open the file `bicing_history.csv` you'll be able to see something like this:

```{r, echo = FALSE, eval = TRUE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir_api, "bicing_history_cron.png"))
```

The CSV file is being continually updated every minute. Just as we did with web scraping, this strategy of automation can take you a long way for collecting data. Automation is one of the main building blocks in data harvesting because more often than not you'll find that online data is quick to disappear so you'll need to automate a process to collect it for you.

## Exercises

* Can you disable the `cron` expression we just enable above?

* Can you create a `logs.txt` file inside the directory where the `cron` is running and save the time stamp, the number of rows and columns of the request response? Logs are extremely important in these programs because you can check them to see why a scheduled program might have failed. The print out of your logs should look something like this for each of the request that your program makes:

```
#############################################################################
Current timestamp of request 2022-12-21 01:41:25
Number of columns: 8 
Number rows: 20
#############################################################################
Current timestamp of request 2022-12-22 01:40:25
Number of columns: 8
Number rows: 20
```

Some hints:

* You'll need to create an empty logs file if it doesn't exist.
* You'll need to extract the time stamp and dimensions of the data frame
* You'll need to use one of the `write_*` functions to save to a `txt` file and remember to *append* the results to accumulate everything.

After adding these changes, relaunch your schedule and look at your logs file: it should populate every minute with the status of your program.


```{r, eval = FALSE, echo = FALSE}
library(scrapex)
library(httr2)
library(dplyr)
library(readr)

bicing <- api_bicing()

real_time_bicing <- function(bicing_api) {
  rt_bicing <- paste0(bicing_api$api_web, "/api/v1/real_time_bicycles")
  
  resp_output <- 
    rt_bicing %>%
    request() %>% 
    req_perform()
  
  all_stations <-
  resp_output %>%
  resp_body_json()

  all_stations_df <- 
    all_stations %>%
    lapply(data.frame) %>%
    bind_rows()
  
  all_stations_df
}

save_bicing_data <- function(bicing_api) {
  local_logs <- "/home/jorge.cimentada/bicing/logs.txt"
  
  if (!file.exists(local_logs)) {
    file.create(local_logs)
  }
  
  # Perform a request to the bicing API and get the data frame back
  bicing_results <- real_time_bicing(bicing_api)
  
  # Logs
  separator <- "#############################################################################\n"
  timestamp <- paste0("Current timestamp of request ", unique(bicing_results$current_time), "\n")
  df_dims <- paste0("Number of columns: ", ncol(bicing_results), " \nNumber rows: ", nrow(bicing_results), "\n")
  write_lines(c(separator, timestamp, df_dims), local_logs, append = TRUE)
  
  # Specify the local path where the file will be saved
  local_csv <- "/home/jorge.cimentada/bicing/bicing_history.csv"
  # Extract the directory where the local file is saved
  main_dir <- dirname(local_csv)

  # If the directory does *not* exist, create it, recursively creating each folder  
  if (!dir.exists(main_dir)) {
    dir.create(main_dir, recursive = TRUE)
  }

  # If the file does not exist, save the current bicing response  
  if (!file.exists(local_csv)) {
    write_csv(bicing_results, local_csv)
  } else {
    # If the file does exist, the *append* the result to the already existing CSV file
    write_csv(bicing_results, local_csv, append = TRUE)
  }
}

save_bicing_data(bicing)
```




