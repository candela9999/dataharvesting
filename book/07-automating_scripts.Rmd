```{r automating-setup, include=FALSE}
main_dir <- "./images/automating_scripts"
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.path = paste0(main_dir, "/automatic_rmarkdown/"),
  fig.asp = 0.618
)

```

# Automating Web Scraping Scripts

There are two types of webscraping: one-off scrapings or frequent scrapings. For the first one, all of the material of the book until this chapter should be enough. However, for the second we need new tools and strategies. Have you asked yourself how can you automate a script? By automating I mean, for example, run that script every Thursday at 08:00 PM. This chapter focuses on scheduling programs to run whenever you want. You might need this to collect data on a website that is changing constantly or to request data from an API on frequent intervals (the topic of API's is the second part of this book) but in any of those two cases you don't want to be manually running to your house at 3 in the morning to run the program. This chapter will make sure you don't have to do that.

::: {.rmdnote}
<br>
Scheduling scripts is very different between operating systems. This chapter will focus solely on scheduling scripts for Linux and MacOS.
:::

## The Scraping Program

First things first, we need a scraping program. Let's build a one recycled from previous chapters. Let's count the number of articles for each section of the newspaper "El País". The R script will parse the website, extract the number of articles and collect everything in a data frame. Here's how it would look like:

```{r}
library(scrapex)
library(xml2)
library(magrittr)
library(purrr)
library(tibble)
library(tidyr)
library(readr)

# If this were being done on the real website of the newspaper, you'd want to replace the line below with the real link of the website.
newspaper_link <- elpais_newspaper_ex()
newspaper <- read_html(newspaper_link)

all_sections <-
  newspaper %>%
  xml_find_all("//section[.//article][@data-dtm-region]")

final_df <-
  all_sections %>%
  map(~ length(xml_find_all(.x, ".//article"))) %>%
  set_names(all_sections %>% xml_attr("data-dtm-region")) %>%
  enframe(name = "sections", value = "num_articles") %>%
  unnest(num_articles)

final_df
```

We see there are 11 sections each one with their respective number of articles. For a personal research project of yours, you're interested in collecting these counts every day at three different times of the day. Your idea is to try to map how newspapers shift their efforts in different areas over time. To do that, we should also add a new section in the code to save our results on a CSV file with the time stamp of the current date. That way we can filter our results by date and collect historical date on this. Let's add a section to save our data:


```{r}
library(scrapex)
library(xml2)
library(magrittr)
library(purrr)
library(tibble)
library(tidyr)
library(readr)

newspaper_link <- elpais_newspaper_ex()

all_sections <-
  newspaper_link %>%
  read_html() %>%
  xml_find_all("//section[.//article][@data-dtm-region]")

final_df <-
  all_sections %>%
  map(~ length(xml_find_all(.x, ".//article"))) %>%
  set_names(all_sections %>% xml_attr("data-dtm-region")) %>%
  enframe(name = "sections", value = "num_articles") %>%
  unnest(num_articles)

# Save the date time as a column
final_df$date_saved <- format(Sys.time(), "%Y-%m-%d %H:%M")

file_path <- "~/newspaper/newspaper_section_counter.csv"

# *Try* reading the file. If the file doesn't exist, this will silently save an error
res <- try(read_csv(file_path, show_col_types = FALSE), silent = TRUE)

if (inherits(res, "try-error")) {
  # If the file doesn't exist, save the data frame as is
  print("File doesn't exist; Creating it")
  write_csv(final_df, file_path)
} else {
  # If the file was read, append the new rows and save the file again
  rbind(res, final_df) %>% write_csv(file_path)
}
```

This script will read the website of El País, count the number of sections and save the results as a CSV file at `~/newspaper/newspaper_section_counter.csv`. That directory still doesn't exist, so we'll create it first.

At this point, you need to open your terminal. In Linux, you can do it by pressing the keys CTRL + ALT + t together. For MacOS click the Launchpad icon in the Dock, type Terminal in the search field, then click Terminal. For both operating systems you should see a window like this (not exactly, but similar) one pop up:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "basic_terminal_ubuntu.png"))
```

This is the terminal. It allows you to run commands just as you would do in your computer but typing code. Let's create the directory where we'll save the CSV file and the R script that performs the scraping. To create the directory, we use the command `mkdir` which stands for `m`a`k`e`dir`ectory. Let's create it with `mkdir ~/newspaper/`:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "create_newspaper_dir.png"))
```

Great, that directory is created. Before we continue, you should copy the R script we wrote down above and save it in `~/newspaper/`. Save it as `newspaper_scraper.R`. So far you should have only an R file within `~/newspaper/` called `newspaper_scraper.R`. Let's switch our 'directory' to `~/newspaper/` in the terminal. In the terminal you can change directories with the `cd` command, which stands for `c`hange`d`irectory, followed by the path where you want to switch to. For our case, this would be `cd ~/newspaper/`:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "cd_newspaper.png"))
```

As you can see in the third line of the image, there now appears `~/newspaper` in blue, denoting that I am at the directory right now. To execute an R script from the terminal you can do it with the `Rscript` command followed by the file name. For our case it should be `Rscript newspaper_scraper.R`. Let's run it:


```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "running_scraper_once.png"))
```

The first few lines show the printing of package loading but we finally see the print statement we added when the file doesn't exit: `File doesn't exist; Creating it`. If you opened the CSV in your computer you should see the a sheet like this one:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "newspaper_scraping_excel.png"))
```

Great, our scraper works! We have about half the job done. Now we need to come up with a way to execute this `Rscript ~/newspaper/newspaper_scraper.R` on a schedule. 

## cron, your scheduling friend

Luckily for you, such a program already exists. It's called `cron` and it allows you to run your script on a very specific schedule. For Ubuntu, you can install `cron` with:

```{bash, eval = FALSE}
sudo apt-get update
sudo apt-get install cron
```

For MacOS you can install it with:

```{bash, eval = FALSE}
brew install --cask cron
```

In both cases, after the install is successful, you should be able to confirm that it works with `crontab -l`:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "crontab_list.png"))
```

This means you have no scheduled scripts in your computer. To schedule a script with `cron` you need two things: the command to execute and the schedule. The command we already know, it's `Rscript ~/newspaper/newspaper_scraper.R`. For specifying schedules, `cron` has a particular syntax. Let's take a look:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "crontab_syntax.png"))
```

This chapter on automating scripts with `cron` is aimed at being an introduction to get it up and running. As with XPath and regular expressions, `cron` is very flexible and can have a complicated syntax for achieving complex schedule. Here we'll explore the basics on how to create simple scheduled scrapers.

`cron` syntax specifies the each possible date parameter and gives you a placeholder `*` to signal that whenever there is a `*` it means it will be repeated at each instance of the place holder. Complicated? Look at this example:

```{bash, eval = FALSE}
* * * * *
```

From the plot above we know that each `*` corresponds to minutes, hours, day of month, month an day of week. So by writing `* * * * *`, we're scheduling the program to run at every minute, of every hour, of every day of the month, for every month. Say we changed the schedule to run every half an hour, how would it look like? We know the first slot is for minutes so we can write `30` in the first slot:

```{bash, eval = FALSE}
30 * * * *
```

We're effectively scheduling something to run at minute 30 of each hour, each day, each month. You might've noticed that the last slot is for day of week. That might clash with the third slot which is day of month. You can specify any of the two to make interesting schedules. Wednesdays are the third day of the week (if we start counting on Monday), so we can run a schedule every 30 minutes but *only* on Wednesdays:

```{bash, eval = FALSE}
30 * * * 3
```

Or you might want to run your scraper at 05:30 AM only on Saturday and Sunday:

```{bash, eval = FALSE}
30 5 * * 6,7
```

The expression reads like this: run on the 30th minute at 5 AM every month but on Saturday and Sunday (6th and 7th day of the week). Let's say we wanted to run our newspaper scraper every 4 hours, every day, how would it look like? That sounds a bit different to what we've done. What the syntax we've discussed, we have to specifically write down the day / hour / minute we want to the scraper to do. For that `cron` has additional tricks. If we wanted to run a scraper every every 4 hours we would write it like this:

```{bash, eval = FALSE}
* */4 * * *
```

You take the slot you want to me recurrent and add `/` by the frequency you want. If instead you wanted to run the scraper every 4 hours, every 2 days, you would write something like this:

```{bash, eval = FALSE}
* */4 * * */2
```

With the theory out of the way, let's get our example running. Let's schedule our newspaper scraper to run every minute, just to make sure it works. This will get messy because it'll append the same results in the CSV file continuously but it will give you proof that your script is running on a schedule. If we want this to run every minute, our cron expression should be this `* * * * *`, a the simplest expression. 

To save the cron expression type `crontab -e` in your terminal. If this is your first time using `crontab` you should see something like this:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "crontab_choose_editor.png"))
```

This will allow you to pick the editor you want to use for editing your cron schedule. Pick whichever of the options points to `nano`, the easiest one. That should open a new file like this one:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "crontab_schedule_file.png"))
```

This is the file where you write the schedule and command that you want `cron` to run. Either scroll down with your mouse or hit the scroll down key at the bottom right of your computer to go the last line of the editor. There we need to write our cron expression. Let's write the cron expression and our command:

```{bash, eval = FALSE}
* * * * * Rscript ~/newspaper/newspaper_scraper.R
```

Your terminal should look something like this:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "crontab_newspaper_scraper.png"))
```

To save this file, follow these steps:

* Hit `CTRL` and `X`
* It will prompt you to save the file by hitting `Y`
* Hit `enter` for the file name

After you do this, you should be back at the terminal and you cron job should be saved. Wait two or three minutes and open your excel file again. We should find the same records duplicated but with a different time stamp:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics(file.path(main_dir, "newspaper_results_crontab.png"))
```

Our crontab worked as expected as you can see from the time stamp column. There are three different dates, each 1 minute apart, meaning that our cron executed the command every minute. This framework of building a scraper, testing it and then scheduling it to run on frequent intervals is very powerful. With these commands you can automate any program (in fact, not only R but any programming language or program). However, this approach has a limitation. Your computer needs to be turned on all the time in order for your `cron` schedule to run. If you're doing a school project and it's possible, you might get around with only using your computer. However, for more demanding scrapings (lots of data, frequent intervals) it's almost always a better idea to run your scraper on a server. 

Launching a server and running a scraper is out of the scope of this chapter but keep that in mind when building scrapers for your work.

As mentioned throughout the chapter, `cron` can become complex if your schedule patterns are difficult. There's a bunch of resources that can help you on the internet. Here are some that worked for me:

* [Crontab Guru](https://crontab.guru/)
* [Cron tutorial](https://linuxhint.com/cron_jobs_complete_beginners_tutorial/)

## Exercises
